[
  {
    "corpusid": 207778550,
    "section": "18/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.375,
    "text": "USU can be determined from the difference between the population standard deviation of the experimental **data** and the average value of their reported uncertainties. This method might be appropriate if there is no clear evidence of data-set grouping based on experimental methodology.",
    "id": 1,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 1,
    "created_at": "2023-03-01T16:23:15.979236Z",
    "updated_at": "2023-03-01T16:23:15.979262Z",
    "lead_time": 5.719
  },
  {
    "corpusid": 2275365,
    "section": "3/18",
    "subsection": 2.0,
    "section_pos_in_pct": 0.167,
    "text": "It is worth mentioning that Smets made recently an attempt to justify usage of belief functions instead of Bayesian probabilities, to identify situations in which usage of belief functions is more reasonable than usage of probabilities [36]. However, the **data** modifying impact of application of Dempster-rule is not explicitly recognized there and numerical examples presented there deliberately avoid situations where more than one belief function may have more than one focal point.",
    "id": 2,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 2,
    "created_at": "2023-03-01T16:24:00.041392Z",
    "updated_at": "2023-03-01T16:24:00.041426Z",
    "lead_time": 15.455
  },
  {
    "corpusid": 2423505,
    "section": "10/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.435,
    "text": "Often, the simplifying assumptions of the previous section doesn't hold. This section discuss method that use prior probabilities estimated on the source **data** to regularize the model. We first cover priors in the bayesian sense, and then some examples of discriminative methods. Ei(",
    "id": 3,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 3,
    "created_at": "2023-03-01T16:27:34.919700Z",
    "updated_at": "2023-03-01T16:27:34.919752Z",
    "lead_time": 214.715
  },
  {
    "corpusid": 17671315,
    "section": "26/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.812,
    "text": "Figure 4 :\n4a) The graph of the function ε r (x) for which the **data** are computationally simulated via solution of the forward problem\n",
    "id": 4,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 4,
    "created_at": "2023-03-01T16:27:40.922916Z",
    "updated_at": "2023-03-01T16:27:40.922939Z",
    "lead_time": 5.807
  },
  {
    "corpusid": 208006088,
    "section": "4/26",
    "subsection": 1.0,
    "section_pos_in_pct": 0.154,
    "text": "Our unifying Mixing Model Hierarchy begins with Linear Mixing Models (LMMs), a simple class of MOGP models typically characterised by low-rank covariance structure. An LMM decomposes a signal f (t) comprising p outputs into a fixed basis h 1 , . . . , h m ∈ R p whose coefficients x 1 (t), . . . , x m (t) are time-varying and modelled independently with Gaussian processes:\nf (t) = h 1 x 1 (t) + . . . + h m x m (t) = h 1 · · · h m H x 1 (t) · · · x m (t) T x(t) = Hx(t).\nThe noisy signal y(t) is, then, generated by adding N (0, Λ)-distributed noise to f (t). Intuitively, this means that the p-dimensional **data** lives in a \"pancake\" around the m-dimensional column space of H, where typically m p.",
    "id": 5,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 5,
    "created_at": "2023-03-01T16:27:54.564673Z",
    "updated_at": "2023-03-01T16:27:54.564699Z",
    "lead_time": 13.457
  },
  {
    "corpusid": 207778550,
    "section": "32/48",
    "subsection": 3.0,
    "section_pos_in_pct": 0.667,
    "text": "It should be emphasized that USU refers to \"unrecognized\" sources of uncertainties as the term implies. That means that before considering the possibility of USU effects, all known sources of biases and uncertainties should be addressed, estimated, and corrected for to the best of our knowledge. A detailed analysis of uncertainty evaluation by the experimentalists may hopefully reduce the need for hidden errors or unrecognized sources of uncertainty (USU ). More specifically, clearly wrong **data** should be rejected as they have the potential to bias the resulting mean values as well as significantly affecting the evaluated uncertainties. Known biases should be corrected, e.g., by updating to the newest standard reference cross-section or half-life values. A detailed uncertainty quantification (UQ) exercise should be undertaken using the literature that is pertinent to the respective **data** set. Furthermore, it should be investigated whether known sources of uncertainties for this particular measurement type are missing and should be added. Templates of expected uncertainties [59][60][61][62][63] for specific measurement types can help identify and estimate these missing covariances. Once all these steps have been completed for all input **data** of an evaluation, and still an unexplained scatter in the **data** considering their standard deviations is observed, one should take recourse to identifying whether USU components are present and estimate them.",
    "id": 6,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 6,
    "created_at": "2023-03-01T16:28:12.239555Z",
    "updated_at": "2023-03-01T16:28:12.239595Z",
    "lead_time": 17.5
  },
  {
    "corpusid": 17837291,
    "section": "1/52",
    "subsection": 2.0,
    "section_pos_in_pct": 0.019,
    "text": "Step 3. Linking theorem. The construction above is functorial, so that if X comes equipped with a torus T action, then the entire construction becomes G = S 1 × T equivariant and not just S 1 equivariant. In particular, the Euler **data** identity is an identity of G-equivariant classes on W d . Our problem is to first compute the G-equivariant classes Q d on W d satisfying the Euler **data** identity, and with the property that A d ∼ α −2 . Note that the restrictions Q d | p to the T fixed points p in X 0 ⊂ W d are polynomial functions on the Lie algebra of G. Suppose that X is a balloon manifold. Then it can be shown that (with a nondegeneracy assumption on e G (X 0 /W d )) the classes Q d are uniquely determined by the values of the Q d | p , when α is some scalar multiple of a weight on the tangent space T p X. These values of Q d | p can be computed explicitly by exploiting the structure of a balloon manifold.",
    "id": 7,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 7,
    "created_at": "2023-03-01T16:28:19.931705Z",
    "updated_at": "2023-03-01T16:28:19.931729Z",
    "lead_time": 7.519
  },
  {
    "corpusid": 14912634,
    "section": "10/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.435,
    "text": "To discern the appropriate U for power curves, 10 min power **data** P norm are plotted as a function of (a) nacelle cup anemometer 80 m U, (b) nacelle cup anemometer 'true-flux' equivalent U, (c) SODAR 80 m U, and (d) SODAR 'true-flux' equivalent U for a typical summer day in figure 3. The uncertainty induced by a non-co-located SODAR wind speed in the power curves can be seen in figures 3(c) and (d). When compared with the manufacturer's power curve, the SODAR-based power curves have lower Pearson's coefficient (r) values (r = 0.88-0.89) than the nacelle-based (r = 0.94-0.95). Furthermore, a small improvement (in terms of a higher r value and lower standard deviation of residuals) is evident from using the nacelle-adjusted 'true-flux' equivalent wind speed instead of the nacelle hub-height U ( figure 3(b)). Though small, these differences suggest that the nacelle-adjusted 'true-flux' equivalent wind speed generates the most accurate power curves at this site.",
    "id": 8,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 8,
    "created_at": "2023-03-01T16:28:27.857535Z",
    "updated_at": "2023-03-01T16:28:27.857561Z",
    "lead_time": 7.75
  },
  {
    "corpusid": 52275756,
    "section": "9/51",
    "subsection": 1.0,
    "section_pos_in_pct": 0.176,
    "text": "The stepwise linear regression (SLR) model is often used to assess the linear relationship between multiple independent variables and it can be used for variable screening and avoided collinearity for soil nutrient prediction [16]. Stepwise regression can also be used for variable screening. SLR can remove the weakly significant variables while retaining those with high contribution rate. In this study, the SLR model was used to identify the optimal combination of input PCs with targets. The linear fitting relationships between hyperspectral auxiliary variables and soil nutrients were also extracted. The SLR model was performed in MATLAB 2013b software. The formula of SLR model (Equation (1)) is defined as follows [32]:\ny = b + a 1 x 1 + · · · + a k x k ,(1)\nwhere y is the estimation value of SLR for soil nutrients, b is a regression constant, a 1 , . . . , a k are regression coefficients and x 1 , . . . , x k are the input PCs converted from hyperspectral variables. The support vector machine (SVM) model is a supervised learning method that was used to solve the regression problem in this study. The SVM method can identify the separating optimal hyperplane in multi-dimensional spatial **data** and seeks to minimize the error of all training samples. The SVM model overcomes the limitation of neural networks, which tend to rely on local optimal solutions [33]. Therefore, the SVM model is well suited to soil nutrient prediction with multi-dimensional variables.",
    "id": 9,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 9,
    "created_at": "2023-03-01T16:28:48.839716Z",
    "updated_at": "2023-03-01T16:28:48.839748Z",
    "lead_time": 20.815
  },
  {
    "corpusid": 119253123,
    "section": "5/32",
    "subsection": 2.0,
    "section_pos_in_pct": 0.156,
    "text": "Corrected **data** are compared to three Monte Carlo models: Pythia 6.4 (tune Perugia-0), Pythia 8.1 (tune 1 [15]) and Phojet 1.12.",
    "id": 10,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 10,
    "created_at": "2023-03-01T16:28:55.332761Z",
    "updated_at": "2023-03-01T16:28:55.332799Z",
    "lead_time": 6.316
  },
  {
    "corpusid": 245986448,
    "section": "10/45",
    "subsection": 1.0,
    "section_pos_in_pct": 0.222,
    "text": "We now build a Gaussian process 29 model to statistically infer the density given one or two schlieren images. To ease our exposition, we enumerate the model training and the model prediction **data** below, where the abbreviation \"loc.\" denotes locations and \"out.\" denotes outputs.",
    "id": 11,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 11,
    "created_at": "2023-03-01T16:29:11.715162Z",
    "updated_at": "2023-03-01T16:29:11.715187Z",
    "lead_time": 16.2
  },
  {
    "corpusid": 17837291,
    "section": "34/52",
    "subsection": 3.0,
    "section_pos_in_pct": 0.654,
    "text": "If V is a multipicative type K-theory characteristic class, then we can develope a similar theory of Euler **data** and uniqueness. These result can also be extended to the nonconvex case without a hitch.",
    "id": 12,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 12,
    "created_at": "2023-03-01T16:29:15.763717Z",
    "updated_at": "2023-03-01T16:29:15.763741Z",
    "lead_time": 3.888
  },
  {
    "corpusid": 15394341,
    "section": "11/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.611,
    "text": "Figure S4 :\nS4Twenty-second new (uncataloged) earthquake waveforms detected by FAST, ordered by event time in 1 week of continuous **data** from CCOB.EHN (bandpass, 4 to 10 Hz); FAST found a total of 68 new events. (A) FAST detected 43 new events that autocorrelation also found. (B) FAST detected 25 new events that were missed by autocorrelation.\n",
    "id": 13,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 13,
    "created_at": "2023-03-01T16:29:24.309053Z",
    "updated_at": "2023-03-01T16:29:24.309083Z",
    "lead_time": 8.383
  },
  {
    "corpusid": 119303658,
    "section": "12/22",
    "subsection": 1.0,
    "section_pos_in_pct": 0.545,
    "text": "It is our new hypothesis in this paper that the two methods for fitting time-elongation **data** (HMF and FPF) are just limiting cases of ICMEs with low or high axis inclination to the ecliptic, using two \"extreme\" assumptions on the ICME front shape (circular vs.",
    "id": 14,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 14,
    "created_at": "2023-03-01T16:29:44.001173Z",
    "updated_at": "2023-03-01T16:29:44.001203Z",
    "lead_time": 19.529
  },
  {
    "corpusid": 87345260,
    "section": "4/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.308,
    "text": "In a jack-knife analysis, given a sample of observations and a parameter to evaluate, a subsample is made by eliminating a proportion of the original **data** and the parameter is calculated for the subsample. This procedure is repeated n times and summarized. Since the introduction of the jack-knife (Quenouille 1949 ), researchers have used it, to defi ne limits of confi dence in many sorts of analyses, from statistics (Efron 1979 ;Smith and van Belle 1984 ) and ecology (Crowley 1992 ) to phylogeny. It has been used not only as a measure of support (Lanyon 1987 ), but as a way to obtain the best solution for large **data** sets (Farris et al. 1996 ), to test competing hypotheses (Miller 2003 ), to generalize the performance of predictive models or for cross-validation to estimate the bias of a estimator. As the bootstrapping, it could be seen as \"a measure of robustness of the estimator with regard to small changes in the data\" (Holmes 2003 ).",
    "id": 15,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 15,
    "created_at": "2023-03-01T16:29:50.882005Z",
    "updated_at": "2023-03-01T16:29:50.882039Z",
    "lead_time": 6.711
  },
  {
    "corpusid": 119247546,
    "section": "19/46",
    "subsection": 1.0,
    "section_pos_in_pct": 0.413,
    "text": "In order to write the conformal curve equations (24a)-(24b) for the Reissner-Nordström metric, it is noticed that the metric l in the warped product line element (23) is given by\nl = D(r)dt ⊗ dt − D −1 (r)dr ⊗ dr. Equations (24a)-(24b) implyt + ∂rD(r) D(r)r t = 1 D(r) βr ,(32a)r − ∂rD(r) 2D(r)r 2 + D(r)∂rD(r) 2t 2 = D(r)βt ,(32b)\nwhere consistent with the notation of section 3.2 we have setr ≡ r(τ ),t ≡ t(τ ). Initial **data** for these equations is prescribed by observing the discussion of Section 4.1, and by requiringẋ to be given initially by the unit normal toS. It follows that\nt * = 0, r * > r + ,t * = 1 √ D * ,r * = 0, (b t ) * = 0, (b r ) * = − 2 r * ,(33)whereb t ≡ b , ∂ t ,b r ≡ b , ∂ r .\nNotice that r * =r * , t * =t * . As a consequence of the symmetry of the hypersurfaceS with respect to the bifurcation sphere at r * = r + , it is only necessary to consider the case r * > r + . The equations (32a)-(32b) can be decoupled by making use of thẽ g-normalisation condition\nD(r)t 2 − 1 D(r)r 2 = 1.(34)\nSolving the latter for t ≥ 0 and substituting into (32b), one obtains that\nr + 1 2 ∂rD(r) − β D(r) +r 2 = 0.(35)\nThis equation can be integrated once to yield\nD(r) +r 2 − βr = γ,\nwhere γ is a constant given in terms of the initial **data** by\nγ = − D * . It follows thatr = ± (γ + βr) 2 − D(r),(36)\nwith the sign depending on the value of r * .",
    "id": 16,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 16,
    "created_at": "2023-03-01T16:29:58.292356Z",
    "updated_at": "2023-03-01T16:29:58.292393Z",
    "lead_time": 7.238
  },
  {
    "corpusid": 251865248,
    "section": "14/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.56,
    "text": "The virtual massless particle tracing assumes that the flow fields around the actuators are 2D, and the particle velocity equals the flow velocity at the particle position. The flow fields used for particle tracing are obtained from the PIV experiments. To get the flow velocity at any location within the observation region, cubic interpolation is used to interpolate the 2D velocity-gridded **data** obtained from the PIV experiment. The center of mass of the particles is\ncalculated by X COM = ∑ i=1 M X i _ M .\nHere, M is the total number of the particles and X i is the coordinate of the ith particle.",
    "id": 17,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 17,
    "created_at": "2023-03-01T16:30:05.183258Z",
    "updated_at": "2023-03-01T16:30:05.183288Z",
    "lead_time": 6.736
  },
  {
    "corpusid": 2423505,
    "section": "21/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.913,
    "text": "10 ,FIG. 3 .\n103a posterior (MAP) estimation of model parameters is central in bayesian statistics. In this setting prior 28 Digital Signal Processing Vol. NosPictorial example of two steps in adapting a hypothesized speaker model. (a) The training vectors (x's) are probabilistically mapped into the UBM mixtures. (b) The adapted mixture parameters are derived using the statistics of the new **data** and the UBM mixture parameters. The adaptation is **data** dependent, so UBM mixture parameters are adapted by different amounts.\n",
    "id": 18,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 18,
    "created_at": "2023-03-01T16:30:10.894316Z",
    "updated_at": "2023-03-01T16:30:10.894343Z",
    "lead_time": 5.547
  },
  {
    "corpusid": 119479526,
    "section": "13/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.448,
    "text": "(13a) through Eq. (13f) as well as for the hyperparameters which rescale the errors for each experimental dataset. In Fig. 4, we plot the model predictions for each phase versus temperature for the results of two previous assessments and the datasets used. The first assessment is from Arblaster [17], hereafter Arb2014, whose results leverage their knowledge and expertise to judge the quality of various experimental measurements and come up with a final model. The second assessment was performed using thermodynamic information and HSC Chemistry, a software package which automatically combines existing thermodynamic assessments to find an optimal model, leveraging multiple experts whenever possible. The software uses **data** from literature, especially review articles. Whenever possible, we trace back the original article and use the   content correction). This explains why the two beta-phase models follow the same trend in enthalpy but are highly discrepant in specific heat. Furthermore, the linear model of Arb2014 for enthalpy forces the specific heat to be constant and to take a value which does not pass through the majority of the experimental specific heat measurements. The difference between our model and the HSC chemistry assessment is small, as the HSC Chemistry software appears to have incorporated specific heat measurements. Uncertainty intervals are broader for the beta-phase model due to the fewer number of **data** points and their significant spread.",
    "id": 19,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 19,
    "created_at": "2023-03-01T16:30:23.705293Z",
    "updated_at": "2023-03-01T16:30:23.705328Z",
    "lead_time": 12.653
  },
  {
    "corpusid": 19033986,
    "section": "3/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.107,
    "text": "Electrons generated by DM lose essentially all their energy via Inverse Compton scattering, e ± γ → e ± γ , on ambient light with average energy E γ ∼ eV. Such scatterings give rise to photons with larger energy E γ ∼ E γ (E e /m e ) 2 ∼ 10 GeV, which is in the energy range being probed by FERMI. As discussed below, this DM ICS γ flux is only marginally affected by astrophysical and DM distribution uncertainties. The reasons for this can be traced back to two observations: (i) Far away from the Galactic Center, the DM uncertainties are relatively mild. (ii) As we will see in Section 4, all DM models that fit the **data** predict roughly the same e ± spectrum, as it is now mostly fixed by the new measurements (given the new FERMI and HESS results). Thereby the DM ICS spectrum is well predicted. As already illustrated in fig. 1 it is not much below the first FERMI diffuse γ-ray data, released for energies ≤ 10 GeV in a specific angular region. Therefore, if the e ± excess is due to DM, FERMI is expected to observe an associated γ excess which is not sensitive to the specific DM model or DM density profile. Whether such an excess is seen or not, will decisively implicate on the DM (or any other mechanisms that produces e ± in a spherical region away from the galactic plane) interpretation of the measured excesses. Alternative scenarios involve e ± generated locally (e.g. by a powerful pulsar) or along the galactic plane (e.g. by supernovae).",
    "id": 20,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 20,
    "created_at": "2023-03-01T16:30:44.407740Z",
    "updated_at": "2023-03-01T16:30:44.407770Z",
    "lead_time": 20.533
  },
  {
    "corpusid": 202178736,
    "section": "6/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.375,
    "text": "The rigid rough square footing of size 100 X 100 mm was placed at the middle of the surface of the unsaturated sample. The footing was fixed at the chosen angle with the loading piston. The displacement transducer was connected with the piston for the measurement of the displacement [R-Displacement {Fig. 5(a)}] during loading. Two external, vertical and horizontal displacement transducer were also placed precisely at the middle of the footing. These transducers were placed perfectly in vertical and horizontal plane at the center of the footing as shown in Fig. 5(a). The footing settlements were measured with a resolution equal to 0.001 mm and the accuracy of the applied load [R-Force] was equal to 0.05 %. A National Instrument (NI) **data** logging system was used to collect all the experimental data. The noisy signal received from load and displacement transducers of the device was filtered using third order Zero-Phase Low Pass Butterworth filter in MATLAB. To obtain a smooth signal after filtering, the unwanted transients were eliminated by using Median Filtering technique available in MATLAB. The loading piston was connected with the footing at an angle of 15 degrees with vertical Fig. 5(a). This provides an angular loading to the footing of 15 degrees to impose a coupled cyclic excitation. A force controlled coupled harmonic excitation from Equation 1 is imposed to the footing at a constant amplitude of 0.25kN, with the frequency of 0.1Hz with respective static load as shown in Fig. 5(b).\n[ ] ( ) t A F Force R st cyclic ω cos * + = −(1)\nWhere, Fst = Static Load A = Amplitude = 0.25kN ω = Angular Frequency = 2*π*f f = frequency = 0.1Hz t = Total time of loading = 0 to 5 x 10 5 sec for 50000 cycles.",
    "id": 21,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 21,
    "created_at": "2023-03-01T16:31:00.818063Z",
    "updated_at": "2023-03-01T16:31:00.818094Z",
    "lead_time": 16.248
  },
  {
    "corpusid": 207778550,
    "section": "34/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.708,
    "text": "Maximum likelihood estimation is an established method in statistics that is used to infer unknown parameters of a probability distribution from data. Assume a probability distribution L(D | θ) that gives the likelihood for the realization of any potential measurement D under a specific choice of values for the distribution parameters θ. In maximum likelihood estimation, the values of θ are chosen to maximize the likelihood for the actually observed **data** D obs . This estimation technique is general and not bound to a specific choice of probability distribution.",
    "id": 22,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 22,
    "created_at": "2023-03-01T16:31:04.746219Z",
    "updated_at": "2023-03-01T16:31:04.746245Z",
    "lead_time": 3.77
  },
  {
    "corpusid": 53380409,
    "section": "40/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.336,
    "text": "\nnew heavy resonances decaying into different pairings of W, Z, or Higgs bosons, as well as directly into leptons, are presented using a **data** sample corresponding to 36.1 fb −1 of pp collisions at ffiffi ffi s p ¼ 13 TeV collected during 2015 and 2016 with the ATLAS detector at the CERN Large Hadron\n",
    "id": 23,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 23,
    "created_at": "2023-03-01T16:31:13.482022Z",
    "updated_at": "2023-03-01T16:31:13.482050Z",
    "lead_time": 8.577
  },
  {
    "corpusid": 38256305,
    "section": "37/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.755,
    "text": "Figure 10 .\n10Landsat 8 thermal bands (10, 7, 6, merged with band 8) RGB-combination enhancing NS-and NE-SW-oriented linear features (arrows) Principal component image processing of Landsat 8 **data** enhanced structural features, as shown in\n",
    "id": 24,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 24,
    "created_at": "2023-03-01T16:31:29.289359Z",
    "updated_at": "2023-03-01T16:31:29.289386Z",
    "lead_time": 15.644
  },
  {
    "corpusid": 248858280,
    "section": "8/24",
    "subsection": 1.0,
    "section_pos_in_pct": 0.333,
    "text": "The antioxidant activity of galangin was determined based on its superoxide radical scavenging ability that was measured using a protocol developed in our lab [22]. In this cyclic voltammetry method superoxide is developed in situ by bubbling O 2 into the electrovoltaic cell containing anhydrous DMSO solvent, see details in Experimental section. After the blank experiment, aliquots taken from a stock 0.02M galangin solution, were added to the voltaic cell. Fig 10A shows voltammograms of blank and all runs for both electrodes. The lower part of these voltammograms develops at the disk electrode, where O 2 incorporates an electron to generate superoxide O 2 • -. The opposite effect, oxidation of O 2 • -, takes place at the ring electrode, which has a fixed potential enough positive for this action, upper part. As scavenger aliquots are added, they consume part of the superoxide and the ring disc detects less superoxide, lowering the current. The collection efficiency is shown in Fig 10B, for all data, showing the decrease of superoxide. All scavengers studied by us so far behaved in a linear manner for the initial aliquots used. Beyond the initial aliquots, 2 possibilities were seen: (1) complete linear behavior, as in, chrysin [22], eriodictyol [22], BHT [31], emodin [32], celastrol [34] or (2) initial linear behavior followed by asymptotic performance, such as in quercetin [22], embelin [31] and clovamide [35]. In the present study, we also calculated a linear trend for all data, with regression factor of 0.9296, whereas the initial 3 **data** (using the blank plus 2 initial added aliquots) have a linear regression factor of 0.9635, slightly better, but still not ideal. Since we find that several superoxide anions are incorporated in one galangin unit, Fig 9C, galangin is a very efficient scavenger, and indeed this is demonstrated by the slope in Fig 10B, -19x10 4 . In ",
    "id": 25,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 25,
    "created_at": "2023-03-01T16:31:39.432472Z",
    "updated_at": "2023-03-01T16:31:39.432514Z",
    "lead_time": 9.991
  },
  {
    "corpusid": 54041858,
    "section": "8/45",
    "subsection": 2.0,
    "section_pos_in_pct": 0.178,
    "text": "From the determination of first peak times of arrival, the location was determined from:\nLocation = (x/2)(∆t/∆t x )(1)\nwhere x is the length between the outer sensors (25 mm). The location of each event after 130,000 s is plotted in Figure 7a versus the time of the test where the value 0 is the center of the gage section. Each **data** point represents a single event and the width of the **data** point is proportioned to the average AE energy of the given event. There are scattered and infrequent peak events for the over 70,000 s (~20 h) leading up to the heightened period of AE activity just prior to failure. The events at the end of the test are so dense that the period after 200,000 s (box in Figure 7a) are plotted in Figure 7b,c for the valley and peak events, respectively. Most of the peak and all of the valley events are concentrated in the 0 to +1.5 mm location from the center of the gage, which is where failure took place. Whatever occurred just prior to failure would not correspond to distributed matrix cracking along the gage length. Therefore, one would expect to observe localized damage near the fracture surface within an approximate 2 mm length or less of the composite, as shown below.",
    "id": 26,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 26,
    "created_at": "2023-03-01T16:31:52.942645Z",
    "updated_at": "2023-03-01T16:31:52.942695Z",
    "lead_time": 13.366
  },
  {
    "corpusid": 88514824,
    "section": "3/12",
    "subsection": 1.0,
    "section_pos_in_pct": 0.25,
    "text": "The likelihood function of (H, σ 2 ) conditional on observations of averaged energies from levels j 1 , . . . , j 2 is\nL(H, σ 2 |y j 1 , . . . , y j 2 ) = j 2 i=j 1 g(y i ).\nWe use beta distribution and non-informative prior 1/σ 2 as independent priors on H and σ 2 , respectively,\nπ(H, σ 2 ) = H α−1 (1 − H) β−1 B(α, β) × 1 σ 2 .\nThe hyperparameters in beta distribution, α and β are calibrated by considering the impact of effective sample size (ESS) and the mean of the beta distribution, α α+β , which is linked to the Hurst exponent of an input signal. The ESS for the beta(α, β) prior is approximated with α + β and is closely related to the performance of the Bayesian estimation. For example, when ESS is large, the posterior distribution is dominated by the prior [13]. Based on simulations, we selected the ESS to be approximately 50% the original **data** size, but the ESS can be calibrated based on the level of certainty about H. The larger the ESS is, the more confident we are about the mean of a prior, that is, about the \"true\" value of H.",
    "id": 27,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 27,
    "created_at": "2023-03-01T16:32:01.963594Z",
    "updated_at": "2023-03-01T16:32:01.963625Z",
    "lead_time": 8.859
  },
  {
    "corpusid": 251913584,
    "section": "8/31",
    "subsection": 1.0,
    "section_pos_in_pct": 0.258,
    "text": "The main limitation of logistic regression is that it can only be used to predict discrete functions. Thus, the dependent variable of Logistic Regression is restricted to the discrete set. This restriction is prohibitive to predicting continuous **data** (Al Shamali, 2015).",
    "id": 28,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 28,
    "created_at": "2023-03-01T16:32:06.187808Z",
    "updated_at": "2023-03-01T16:32:06.187836Z",
    "lead_time": 4.058
  },
  {
    "corpusid": 53380409,
    "section": "62/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.521,
    "text": "The search channels included here provide access to several coupling strengths of heavy resonances to SM particles as described by Eq. (1) in the context of the HVT model. Specifically, the **data** constrain the coupling strength to both the quarks and bosons in the VV and VH channels, whereas constraints are placed on both the quark and lepton couplings in the leptonic channels. These constraints are shown in Figs. 9-11, where the first and second include a shaded area denoting a region where the limits are not valid because resonances would have a width greater than 5% of their mass. This is a region where the resonance width would exceed the discriminating variable's resolution in the search, and the assumed narrow-width approximation breaks down. Figures 10 and 11 include constraints on heavy resonances with masses of 3, 4, and 5 TeV from precision electroweak (EW) measurements [66], which already exclude this aforementioned region for the relevant contours shown. The EW constraints are only overlaid on the final plots for each part of the combination.",
    "id": 29,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 29,
    "created_at": "2023-03-01T16:32:15.263602Z",
    "updated_at": "2023-03-01T16:32:15.263625Z",
    "lead_time": 8.917
  },
  {
    "corpusid": 19033986,
    "section": "22/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.786,
    "text": "Fig.\nFig. 5 shows the χ 2 as function of the DM mass for various DM annihilation (left) or DM decay (right) modes. We find that, independently of the non-observation of an excess in thē p PAMELA data, only some leptonic modes can reproduce all data. Here HESS observations play a key role demanding that the e ± excess terminates in a sharper way than what typical of non-leptonic channels, irrespectively of the DM density profile. DM heavier than 10 TeV that annihilates or decays into light quarks still provides a reasonable fit to PAMELA and FERMI data, if fitted conservatively. However it is disfavored by the HESS e + + e − data, and presumably the photon **data** as well ( in the annihilating case). The only spectral feature that can allow one to discriminate the various modes lies at the high end of the spectrum between 1 and 3 TeV, where we only have the electron HESS **data** which is less precise than the FERMI data. The leptonic channels can be ordered according to the sharpness of their end-point:\n",
    "id": 30,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 30,
    "created_at": "2023-03-01T16:32:22.258430Z",
    "updated_at": "2023-03-01T16:32:22.258463Z",
    "lead_time": 6.786
  },
  {
    "corpusid": 2423505,
    "section": "11/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.478,
    "text": "Bayesian Priors\nx 2 ) = 1 ni T t =1 Pr(i | xt )x 2 t .(10)\nThis is the same as the expectation step in the EM algorithm. Finally, these new sufficient statistics from the training **data** are used to update the old UBM sufficient statistics for mixture i to create the adapted parameters for mixture i (Fig. 3b) with the equations:\nwi = α w i ni /T + (1 − α w i )wi γ (11) µi = α m i Ei(x) + (1 − α m i )µi (12) σ 2 i = α v i Ei(x 2 ) + (1 − α v i )(σ 2 i + µ 2 i ) −μ 2 i .(13)\nThe adaptation coefficients controlling the balance between old and new estimates are {α w i , α m i , α v i } for the weights, means and variances, respectively. The scale factor, γ , is computed over all adapted mixture weights to ensure they sum to unity. Note that the sufficient statistics, not the derived parameters, such as the variance, are being adapted.",
    "id": 31,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 31,
    "created_at": "2023-03-01T16:32:27.973698Z",
    "updated_at": "2023-03-01T16:32:27.973725Z",
    "lead_time": 5.509
  },
  {
    "corpusid": 52995678,
    "section": "24/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.828,
    "text": "In this paper, we proposed a Lagrangian data-driven ROM for an efficient and relatively accurate numerical simulation of the FTLE field. We explicitly used Lagrangian **data** (FTLE field) in the inner product utilized to construct the ROM basis of the new Lagrangian ROM. We compared the Lagrangian ROM with the Eulerian ROM in the numerical simulation of the QGE.",
    "id": 32,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 32,
    "created_at": "2023-03-01T16:32:33.129311Z",
    "updated_at": "2023-03-01T16:32:33.129334Z",
    "lead_time": 4.938
  },
  {
    "corpusid": 119313773,
    "section": "3/21",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "The detectability of breakdown of linear response is linked to the amount of available data. As N → ∞, a breakdown will always become detectable at any specified significance level α. Conversely, if the mismatch Eq between the true response of the dynamical system and the linear response is too small and there is an insufficient amount of **data** available, the actual response will be swamped by the sampling noise, and one will not be able to detect the breakdown of linear response with a reasonable significance level.",
    "id": 33,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 33,
    "created_at": "2023-03-01T16:32:39.492363Z",
    "updated_at": "2023-03-01T16:32:39.492397Z",
    "lead_time": 6.15
  },
  {
    "corpusid": 201251303,
    "section": "4/10",
    "subsection": 1.0,
    "section_pos_in_pct": 0.4,
    "text": "We did 100 simulations with **data** size = 500 and = 6. We computed SAR probit parameter estimation for each method; MLE, linearized GMM, Bayes, and conditional approximate likelihood. We constructed a cumulative mean plot for every SAR probit parameter estimation with a variable value of spatial lag as the following:  Figure 1-3, we can see that parameter estimation from linearized GMM is the worst method because the estimation is so far away from the real value of SAR probit parameter (beta). We also can check this by a cumulative mean plot of w x y − ; z, = 0,1,2 in figure 4-6.    ",
    "id": 34,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 34,
    "created_at": "2023-03-01T16:32:53.217558Z",
    "updated_at": "2023-03-01T16:32:53.217585Z",
    "lead_time": 13.514
  },
  {
    "corpusid": 17837291,
    "section": "44/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.846,
    "text": "Theorem 6 . 4 .\n64The transformations µ f , ν g : B →B above each defines a mirror transformation. That is, if B is a Euler **data** then µ f (B) and ν g (B) are both Euler **data** linked to B.\n",
    "id": 35,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 35,
    "created_at": "2023-03-01T16:33:10.933607Z",
    "updated_at": "2023-03-01T16:33:10.933639Z",
    "lead_time": 17.515
  },
  {
    "corpusid": 24865762,
    "section": "1/34",
    "subsection": 4.0,
    "section_pos_in_pct": 0.029,
    "text": "In Sect. VII, we introduce the class of MSMs tailored on stochastic Stuart-Landau oscillators, aimed at the modeling of the aforementioned reduction coordinates corresponding here to the projection of the dataset onto the DAH modes; see Sectns. VII A and VII B. These elemental models-the multilayer Stuart-Landau models (MSLMs)-are stacked per frequency, only coupled at different frequencies by the same noise realization; see Sect. VII C. In Sect. VIII, we finally show the flexibility of the DAH-MSLM modeling approach, by its ability to provide skilled stochastic inverse models for **data** issued either from the nonlinear chaotic Lorenz 96 model, or a stochastic heat equation driven by a space-time white noise. Concluding remarks discuss then in Sect. IX directions for future research.",
    "id": 36,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 36,
    "created_at": "2023-03-01T16:33:19.317747Z",
    "updated_at": "2023-03-01T16:33:19.317773Z",
    "lead_time": 8.205
  },
  {
    "corpusid": 253708011,
    "section": "1/6",
    "subsection": 2.0,
    "section_pos_in_pct": 0.167,
    "text": "In phenomenological estimates (see Refs. [23,24] and references therein), the enhancement of the intrinsic nuclear Schiff moment, Ŝ , is predicted to scale with the quadrupole β 2 and the square of the octupole β 3 deformations, as well as inversely with the energy difference ∆E P D between the parity doublet Table 1 Data for selected odd-mass nuclei in the A ≈ 224 mass region exhibiting parity-doublet candidates. The table presents the isotope, its halflife T 1/2,gs , spin and parity of the ground state J π 0 , energy difference to the lowest-lying possible parity-doublet partner ∆E P D , and the lifetime T 1/2,ul of the upper level. Data are taken from the NNDC **data** base [27] and Ref. [28].",
    "id": 37,
    "sentiment": "data availability statement",
    "annotator": 1,
    "annotation_id": 37,
    "created_at": "2023-03-01T16:34:17.762991Z",
    "updated_at": "2023-03-01T16:34:17.763015Z",
    "lead_time": 58.236
  },
  {
    "corpusid": 207778550,
    "section": "31/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.646,
    "text": "Early neutron capture activation measurements by Stavisskii and Tolstikov [111] and Tolstikovet al. [112] with sealed 232 Th oxide samples are shown in Fig. 12. As later measurements were made of this cross section with metal samples, it became clear that cross sections derived from these earlier **data** were significantly larger than those obtained with metal samples. Not all measured experimental **data** sets are shown in Fig. 12. Although the authors tried to avoid water absorption in their samples, it was found later that the increase of the capture cross section could be explained qualitatively by the presence of water in those samples. Initially this difference might have been considered to be due to an USU effect. However, in this case, the physical origin of the discrepancy became known but no correction for the neutron multiple scattering due to hydrogen and subsequent capture in 232 Th, could be made. Therefore, a large uncertainty (not an USU component) should be assigned to these **data** if they are to be considered for evaluation purposes. An equivalent (and perhaps best) alternative is to discard these uncorrected **data** and not use them for **data** evaluation.",
    "id": 38,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 38,
    "created_at": "2023-03-01T16:34:31.466784Z",
    "updated_at": "2023-03-01T16:34:31.466812Z",
    "lead_time": 13.5
  },
  {
    "corpusid": 53380409,
    "section": "5/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.042,
    "text": "The event selection discussed in Sec. VI relies on the reconstruction of electrons, muons, jets, and missing transverse momentum (with magnitude E miss T ). Although the requirements vary for the different channels, the general algorithms are introduced below. The small differences between the efficiencies measured in **data** and MC simulation are corrected for by applying scale factors to the MC simulation so that it matches the data.",
    "id": 39,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 39,
    "created_at": "2023-03-01T16:34:40.420490Z",
    "updated_at": "2023-03-01T16:34:40.420515Z",
    "lead_time": 8.774
  },
  {
    "corpusid": 248366486,
    "section": "6/24",
    "subsection": 2.0,
    "section_pos_in_pct": 0.25,
    "text": "If the scaling behavior were of the standard FV type, the scaling function g(u) would be u-independent at small arguments u 1; on the contrary, our **data** agree with a scaling law of the form g(u) ∼ u −2α , with 2α ≈ 0.9, so that α loc ≈ 0.45 while α = 0.89, implying the occurrence of intrinsic anomalous scaling [34]. Analogous behavior is found for other parameter choices, see Table VI in Appendix A for specific exponent values.",
    "id": 40,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 40,
    "created_at": "2023-03-01T16:34:44.753210Z",
    "updated_at": "2023-03-01T16:34:44.753236Z",
    "lead_time": 4.163
  },
  {
    "corpusid": 5648146,
    "section": "15/18",
    "subsection": 3.0,
    "section_pos_in_pct": 0.833,
    "text": "The Lyapunov exponent spectrum of the COP signal has been applied to characterize the human standing activity as chaotic and as an aid to identify arrhythmias. The minimal distance to define these two points of the phase space is , the range of which has been given in Section 3.4. In order to obtain a uniform evaluation and to avoid the interference of the evaluation of different parameters, was set at 1000, which was within a reasonable range. Different working parameters of may lead to different conclusions about the same **data** [25]. If we change the value, the metric would be inaccurate or even fail to assess human standing ability. A sensitivity analysis of parameter will be carried out by using the Lyapunov exponent spectrum in our future work. ",
    "id": 41,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 41,
    "created_at": "2023-03-01T16:34:51.591628Z",
    "updated_at": "2023-03-01T16:34:51.591658Z",
    "lead_time": 6.651
  },
  {
    "corpusid": 249258070,
    "section": "11/119",
    "subsection": 2.0,
    "section_pos_in_pct": 0.092,
    "text": "Collectively, there were ten categories of police reactive and proactive activities coded from the data, and these categories make up the dependent variables for the current study. Police reactivity to CFS were coded into seven different categories using an approach consistent with previous research utilizing CFS **data** (see Wu & Lum, 2017). Violent crimes included CFS involving reported shootings, robbery, and several types of assaults. Property offenses was a category comprised of burglary, theft, and forgery type offenses. Disorder incidents consisted of an array of disturbances, including general, family, and noise CFS. Suspicious incidents included calls where an alarm was sounded (e.g., vehicle alarm set off) and reports involving a suspicious person or vehicle. The trafficrelated activities category comprised CFS for traffic accidents, driving while intoxicated (DWI), and other traffic-related issues (i.e., road rage). Service-related activities included calls to assist first-responders, including other officers, fire fighters, or emergency medical services. Lastly, the non-crime events category consisted primarily of assisting specialized units with transporting individuals and responding to silent 911 calls.",
    "id": 42,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 42,
    "created_at": "2023-03-01T16:34:59.226036Z",
    "updated_at": "2023-03-01T16:34:59.226070Z",
    "lead_time": 7.463
  },
  {
    "corpusid": 207778550,
    "section": "40/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.833,
    "text": "FIG. 7 .\n7(Color online) Likelihood of the USU variance σ 2 δ derived from the **data** listed in",
    "id": 43,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 43,
    "created_at": "2023-03-01T16:35:08.108701Z",
    "updated_at": "2023-03-01T16:35:08.108732Z",
    "lead_time": 8.708
  },
  {
    "corpusid": 2482529,
    "section": "13/37",
    "subsection": 1.0,
    "section_pos_in_pct": 0.351,
    "text": "We use a crossover rate of 0.6 and a mutation rate of 0.001 (these are the default settings in GENESIS). Each experiment is repeated with three different levels of noise in the training **data** (that is, three values for p), 0.1%, 0.5%, and 1% (see Table 1 above).",
    "id": 44,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 44,
    "created_at": "2023-03-01T16:35:11.833107Z",
    "updated_at": "2023-03-01T16:35:11.833134Z",
    "lead_time": 3.563
  },
  {
    "corpusid": 54041858,
    "section": "6/45",
    "subsection": 1.0,
    "section_pos_in_pct": 0.133,
    "text": "The set-up for AE acquisition was such that the parametric **data** (load and strain) were acquired every second in addition to the AE waveforms. Thus, the variation in stress for a given fatigue cycle was not possible for the 1 Hz data. However, for the 0.1 Hz and 0.01 Hz tests, enough load **data** were available to discern at which point of the cycle the events were occurring. The AE analysis for the 0.1 Hz test (113-3) are presented here in detail, which showed the enhanced AE activity near failure ( Figure 2b). The features found for the 0.1 Hz test were also observed for the 1 Hz tests (147-3 and 113-9) for the most part. Although both specimens were tested at 1 Hz, the same acquisition rate for load in the software, the peak and valley events could be discerned by the timing of the event. In other words, the peak and valley events occurred approximately 0.5 s from one another. Where there were differences is discussed below. The AE **data** were separated into events which occurred near the peak of the cycle and events which occurred near the valley of the cycle. Figure 3a shows the cumulative AE energy **data** for the overall test as well as the contributions of energy for the peak and valley. There were no valley events until the 201,990 s (56.1 h) mark of the test, 2.4 h prior to failure. After the occurrence of valley events, most of the AE energy was dominated by what occurred during the valley part of the stress-cycles with some appreciable AE occurring during the peak of the cycle as well. An example of when the valley and peak events occur in a stress cycle is shown in Figure 3b for the cycle centered at 210,001 s. Note that \"valley\" events occur during unloading as the valley is approached and \"peak\" events occur upon loading as the peak of the stress-cycle is approached. with some appreciable AE occurring during the peak of the cycle as well. An example of when the valley and peak events occur in a stress cycle is shown in Figure 3b for the cycle centered at 210,001 s. Note that \"valley\" events occur during unloading as the valley is approached and \"peak\" events occur upon loading as the peak of the stress-cycle is approached.",
    "id": 45,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 45,
    "created_at": "2023-03-01T16:35:26.163653Z",
    "updated_at": "2023-03-01T16:35:26.163677Z",
    "lead_time": 14.157
  },
  {
    "corpusid": 216162633,
    "section": "3/6",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "(ii) For fixed Ψ(u 0 ) and δ,\nP(u 0 , u 1 ) → |I δ | = β δ − α δ ,\nis strictly increasing, and we have the limit values lim P(u0,u1)→∞\nα δ = 0 = lim P(u0,u1)→∞ β δ − 1 2 Φ(u 0 , u 1 ) , lim P(u0,u1)→∞ ν * = ∞, lim P(u0,u1)→∞ λ * = r − 2 − δ/ √ c r − δ/ √ c .\nCorollary 1. Assume that hypotheses of Theorem 3.1 are met. For every number K > 0, we can choose initial **data** with P(u 0 , u 1 ) large enough, so that K ∈ I δ , and then the corresponding solution with E 0 = K exists only up to a finite time.",
    "id": 46,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 46,
    "created_at": "2023-03-01T16:35:34.953257Z",
    "updated_at": "2023-03-01T16:35:34.953284Z",
    "lead_time": 8.558
  },
  {
    "corpusid": 207778550,
    "section": "47/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.979,
    "text": "TABLE VIII .\nVIIIResults of the variance analysis proposed by Badikov et al.[35] applied to cross-section ratio **data** specified in the first column. The estimation of a systematic effect for three energy groups is given together with the corresponding covariance matrix. The covariance matrix is represented by the square root of the diagonal terms and the correlation matrix.Energy group ",
    "id": 47,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 47,
    "created_at": "2023-03-01T16:35:45.923326Z",
    "updated_at": "2023-03-01T16:35:45.923380Z",
    "lead_time": 10.783
  },
  {
    "corpusid": 119479526,
    "section": "7/29",
    "subsection": 2.0,
    "section_pos_in_pct": 0.241,
    "text": "Firstly, we generate 100 synthetic **data** points each for specific heat and enthalpy. The specific heats and enthalpies are equally spaced in the 1K − 75K and 300K − 1800K temperature ranges, respectively.",
    "id": 48,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 48,
    "created_at": "2023-03-01T16:35:59.249830Z",
    "updated_at": "2023-03-01T16:35:59.249857Z",
    "lead_time": 13.143
  },
  {
    "corpusid": 251711372,
    "section": "21/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.176,
    "text": "The remainder of this paper is organized as follows. Section 2 reviews the relevant literature. Section 3 provides the testable research hypotheses. Section 4 presents the **data** and the event study methodology. In Section 5, we present and discuss our main findings. Section 6 concludes the paper.",
    "id": 49,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 49,
    "created_at": "2023-03-01T16:36:04.265535Z",
    "updated_at": "2023-03-01T16:36:04.265566Z",
    "lead_time": 4.845
  },
  {
    "corpusid": 118139389,
    "section": "7/13",
    "subsection": 5.0,
    "section_pos_in_pct": 0.538,
    "text": "Finally we consider the evolution of the black hole mass density in Figure 7. The grey band represents the observed z = 0 density from Shankar et al. (2009) and BS09 slightly overestimates this value. The model of PNK11 produces a density that is a factor of three smaller than BS09 at z = 0 and also outside the observed value. Overall, PNK11 has a smooth evolution of the density with redshift. BS09 has a less smooth distribution and grows in three stages.  Tremaine et al. (2002). This is different to the scatter on the **data** which is larger. BS09 is well fit by the observations, but PNK11 tends to produce larger velocity dispersions for high mass black holes. BS09 PNK11 Figure 5. The M BH − M * relation for BS09 (left) and PNK11 (right). The red line and shading represents the observed best fit and the uncertainty on this fit from Marleau et al. (2012). This is different to the scatter on the **data** which is larger. BS09 is well fit by the observations, but tends to produce higher stellar mass galaxies for low mass black holes. PNK11 tends to produce larger stellar mass galaxies for high mass black holes. has a lower black hole accretion rate to BS09, which weakens the feedback leading to high mass galaxies becoming too massive.",
    "id": 50,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 50,
    "created_at": "2023-03-01T16:36:15.467815Z",
    "updated_at": "2023-03-01T16:36:15.467841Z",
    "lead_time": 11.018
  },
  {
    "corpusid": 251719245,
    "section": "4/21",
    "subsection": 2.0,
    "section_pos_in_pct": 0.19,
    "text": "We created several graph representations by varying the input parameter . In total, we obtained 5 graph representations of the same **data** set by creating a corresponding graph for = {0.70, 0.75, . . . , 0.95}. We summarized the properties of the graphs in Table 1 Table 1: Summary of network properties for graphs obtained using value-based constrution",
    "id": 51,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 51,
    "created_at": "2023-03-01T16:36:22.296659Z",
    "updated_at": "2023-03-01T16:36:22.296685Z",
    "lead_time": 6.635
  },
  {
    "corpusid": 207778550,
    "section": "2/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.042,
    "text": "A non-informative prior P (θ) = 1 is often used in adjustment procedures where all **data** are used simultaneously. Inclusion of prior information is an important aspect of the procedures commonly used in nuclear **data** evaluation, where datasets are added to a previous adjustment procedure. In such a case the posterior of the previous dataset evaluation can be used as a prior. Then, only the new dataset, in combination with this prior, is employed in forming the new posterior function that defines the revised evaluation..",
    "id": 52,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 52,
    "created_at": "2023-03-01T16:36:27.687572Z",
    "updated_at": "2023-03-01T16:36:27.687595Z",
    "lead_time": 5.188
  },
  {
    "corpusid": 248366486,
    "section": "19/24",
    "subsection": 1.0,
    "section_pos_in_pct": 0.792,
    "text": "FIG. 13 :\n13(Color online) (a) Structure factor of the front, S(k, t), as a function of wave-vector modulus k for increasing times as indicated by the color scale, from numerical simulations of Eq. (16) using parameters as in Fig. 12. (b) Collapse of the **data** of panel (a) following Eq. (12). The u-independent behavior at large u = kt 1/z indicates standard FV scaling with α = α loc 1. All units are arbitrary. FIG. 14: (Color online) (a) PDF of front fluctuations as obtained from numerical simulations of Eq. (16) using parameters as in Fig. 12, for t = 10 −2 (green points) and for t = 2621.44 = 2 18 · 10 −2 (red points). The dashed line shows a Gaussian distribution while the solid line corresponds to the exact TW-GOE form. Inset: time evolution of the skewness and excess kurtosis corresponding to the same set of simulations as the main panel. The TW-GOE values are the abscissas of the corresponding horizontal lines and are shown for reference. (b) Data collapse according to Eq. (15) for the same simulations as in panel (a). For reference, the exact Airy1 covariance is shown as a dashed line. All units are arbitrary.",
    "id": 53,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 53,
    "created_at": "2023-03-01T16:36:36.705041Z",
    "updated_at": "2023-03-01T16:36:36.705074Z",
    "lead_time": 8.794
  },
  {
    "corpusid": 119634392,
    "section": "1/9",
    "subsection": 4.0,
    "section_pos_in_pct": 0.111,
    "text": "We remark that in the first theorem, γ 2 can be arbitrarily large, but s 1 > −1, while in the second theorem γ 2 < 2n/p, but for sufficiently large γ 1 and sufficiently small γ 2 , s 1 > γ 2 − n/p − 1 can be less than −1. Thus the non-standard product estimate allows us to obtain existence results for initial **data** with lower regularity, but requires γ 2 to be small and requires the use of Besov spaces.",
    "id": 54,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 54,
    "created_at": "2023-03-01T16:36:51.325823Z",
    "updated_at": "2023-03-01T16:36:51.325856Z",
    "lead_time": 14.398
  },
  {
    "corpusid": 118072666,
    "section": "4/13",
    "subsection": 4.0,
    "section_pos_in_pct": 0.308,
    "text": "Finally, we look at the fraction of candidates, after background subtraction, outside the signal mass region. We count the number of candidates in the range 1.036 < m KK < 1.05 GeV/c 2 and calculate the ratio of this number to the number of candidates in the signal region. This ratio is found to be 2.4% in **data** and 2.6% in Monte Carlo. This yields a difference of 0.2% on the number of φ candidates between **data** and Monte Carlo.",
    "id": 55,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 55,
    "created_at": "2023-03-01T16:36:57.698440Z",
    "updated_at": "2023-03-01T16:36:57.698472Z",
    "lead_time": 6.19
  },
  {
    "corpusid": 85554746,
    "section": "18/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.947,
    "text": "Figure 5 .\n5Observing stations of selected networks for (a) GHG and aerosols and (b) weather observation against the major biomes (Olson et al 2001) of the African continent. The source for the station **data** for most of the networks is the WMO's OSCAR tool.\n",
    "id": 56,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 56,
    "created_at": "2023-03-01T16:37:02.481081Z",
    "updated_at": "2023-03-01T16:37:02.481108Z",
    "lead_time": 4.616
  },
  {
    "corpusid": 238834501,
    "section": "3/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.158,
    "text": "In terms of the **data** matrix X with rows corresponding to days and columns to times, the 'average' type regression estimate for d • is given by where β is an estimate for the (column) vector of regression coefficients β and Ŷ is the regression estimate. This expression corresponds to an unweighted average over N (selected) days if β contains N elements with value 1/N and otherwise zeros. Viewed as a regression, an intercept is included by adding a column of ones to X T , and the estimate β d • t • for β is obtained by minimising a loss function. We denote with the subscripts the fact that β is estimated by using the observation at d • but only the times t • (excluding the eclipse) as a target. The 'smoothing' approach consists in using the columns of X corresponding to t • as the regression's targets and all remaining columns X dt • as predictors:",
    "id": 57,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 57,
    "created_at": "2023-03-01T16:37:09.627712Z",
    "updated_at": "2023-03-01T16:37:09.627740Z",
    "lead_time": 6.967
  },
  {
    "corpusid": 209991952,
    "section": "9/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.391,
    "text": "The **data** weighting method described above neglects the differences in momentum scale and resolution between standard and charge-flip electrons. This approximation was validated by recomputing the expected SR yields after reducing the p T of the electron with largest |η| by 5 GeV -a value bounding from above the invariant mass resolution of same-sign ee pairs near the Z boson mass -in all weighted **data** events, which was found to have a negligible impact on the results. For the Rpc3LSS1b SR, the method is adapted by simply selecting **data** events with three or more leptons, which are weighted by the probability of one or more electron charges to be mismeasured such that the resulting event contains three same-sign leptons.",
    "id": 58,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 58,
    "created_at": "2023-03-01T16:37:18.279878Z",
    "updated_at": "2023-03-01T16:37:18.279911Z",
    "lead_time": 8.479
  },
  {
    "corpusid": 52141668,
    "section": "6/16",
    "subsection": 2.0,
    "section_pos_in_pct": 0.375,
    "text": "We also try to identify the most dominant LIWC word classes of the **data** by calculating the coverage of each word class for both lie and truth data. After that, we calculate the ratio between the two coverage scores to get dominance of each word class (Mihalcea & Strapparava, 2009). The calculation is performed on every **data** in the IDC corpus. As can be seen in Table 4, the result shows   (Pennebaker et al., 2007). Word classes with scores higher than 1 mean the classes are dominant in lie **data** and less than 1 mean the otherwise.",
    "id": 59,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 59,
    "created_at": "2023-03-01T16:37:25.282874Z",
    "updated_at": "2023-03-01T16:37:25.282906Z",
    "lead_time": 6.832
  },
  {
    "corpusid": 15294885,
    "section": "6/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.24,
    "text": "Given a pair (B, 1/T ) of a bandwidth and a reference rate, the excess bandwidth β is defined as\nβ 2BT − 1 and the Nyquist interval F is defined as F f : − 1 2T ≤ f < 1 2T .\nGiven a pair (B, 1/T ) and a deterministic function p(t) having the continuous-time Fourier transform\n(CTFT) P (ξ) ∞ −∞ p(t)e −j2πξt dt, the VFT p(f ) of p(t) is defined as a vector-valued function of f ∈ F that is equivalent to P (ξ). In particular, the kth entry of p(f ) is given by [p(f )] k P f + k−L−1 T for k = 1, 2, · · · , 2L + 1, where L ⌈β/2⌉.\nGiven a pair (B, 1/T ) and an SOCS random process N(t) with cycle period T having the auto-\ncorrelation function r N (t, s), the matrix-valued PSD R N (f ) of N(t) is defined as a matrix-valued function of f ∈ F , whose (k, l)th entry is given by [R N (f )] k,l R (k−l) N (f +(l − L − 1)/T ) for k, l = 1, 2, · · · , 2L+ 1, where R (k) N (ξ) is the CTFT of r (k)\nN (τ ) that is obtained by applying the Fourier series expansion to\nr N (t, t − τ ), i.e., r N (t, s) = ∞ k=−∞ r (k) N (t − s)e j2πkt/T .\nIn using the above definitions, it is assumed that the parameter B is chosen as bandwidth in complex baseband over which the Rx can observe and process a signal and that the parameter 1/T is chosen as the symbol transmission rate of the Tx. It is also assumed that the frequency band over which the Tx can emit non-zero power is identical to the frequency band of the Rx. For a general case where these two frequency bands are different, the notion of virtual legacy Rx's and the orthogonal constraint at the virtual legacy Rx's can be employed as is done in [15] for the transmission of a proper-complex **data** sequence.",
    "id": 60,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 60,
    "created_at": "2023-03-01T16:37:30.141214Z",
    "updated_at": "2023-03-01T16:37:30.141243Z",
    "lead_time": 4.634
  },
  {
    "corpusid": 207778550,
    "section": "24/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.5,
    "text": "For each **data** set i and energy group m a systematic errorη i,m is estimated by:\nη i,m = 1 n i ni j=1 (y ij,m − f (E j,m , θ)),(22)\nwhere y ij,m are the **data** belonging to the **data** set i with energies E j belonging to the energy group m. These values are used to provide estimates of the systematic error η m for each energy group m:\nη m = 1 K K i=1η i,m ,(23)\ntogether with its covariance matrix C η . The diagonal and off-diagonal terms of this matrix, which are denoted by C η,mm and C η,m1m2 , respectively, are given by:\nC η,mm = 1 K − 1 K i=1 (η i,m −η m ) 2(24)\nand\nC η,m1m2 = 1 K K i=1η i,m1ηi,m2 −η m1ηm2 ,(25)\nwith a constraint that:\nC η,m1m2 ≤ min [C η,m1m1 , C η,m2m2 ].(26)\nThe variance of the distribution of the random error for a given **data** set i and energy group m is estimated from the formulâ\nσ 2 i,m = 1 n i − 1 ni j=1 (y ij,m −f (E j,m , θ)−η i,m ) 2 . (27)\nThese values form a covariance matrix D with only diagonal terms reflecting the contribution of random effects due to, e.g., counting statistics.",
    "id": 61,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 61,
    "created_at": "2023-03-01T16:37:41.236025Z",
    "updated_at": "2023-03-01T16:37:41.236054Z",
    "lead_time": 10.883
  },
  {
    "corpusid": 6811986,
    "section": "15/20",
    "subsection": 2.0,
    "section_pos_in_pct": 0.75,
    "text": "Proposition 30. There are CQs Q and sets of IDs C such that NQI(Q, C, S, V) can not be described by a first-order query over V. More generally, there are CQs Q and sets of IDs C such that NQI(Q, C, S, C, V) is PTime-hard in **data** complexity (that is, as V varies over instances).",
    "id": 62,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 62,
    "created_at": "2023-03-01T16:37:47.581875Z",
    "updated_at": "2023-03-01T16:37:47.581905Z",
    "lead_time": 6.163
  },
  {
    "corpusid": 251953240,
    "section": "16/41",
    "subsection": 1.0,
    "section_pos_in_pct": 0.39,
    "text": "In order to enhance the performance of PDLF-Net, we augment all the weakly visible EMs and their GT images. Then joint pairwise features are extracted from each image and concatenated to different blocks. Each block is trained and tested independently for each dataset class.   Comparing the observation performance from figure 11, the PDLF-Net shows better segmentation results. For instance, in **data** class DC3 and DC4 (2 nd and 3 rd rows from the top) SegNet in (c) has not been able to show the foreground while there is a good segmented output of the same image by PDLF-Net in (e) and (f). In DC8 (last row), SegNet over-segments the image while good visual results are observed by PDLF-Net when concatenation of pairwise feature is at block 2, 3 and 5. Generally, the visual results show great improvement of segmentation results when using PDLF-Net.",
    "id": 63,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 63,
    "created_at": "2023-03-01T16:37:52.785761Z",
    "updated_at": "2023-03-01T16:37:52.785791Z",
    "lead_time": 5.019
  },
  {
    "corpusid": 207778550,
    "section": "3/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.062,
    "text": "But the most important issue to stress is that experimental datasets considered for nuclear **data** evaluation are often not statistically ideal. The process of UQ is usually not straightforward. Also, not all uncertainties (known or unknown) are reported. In addition u may vary over the years, between laboratories, or with respect to the used experimental methodologies. However, it has been demonstrated that in specially designed interlaboratory comparisons, often for single physical quantities, such conditions can be optimized and mastered [13][14][15][16][17].",
    "id": 64,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 64,
    "created_at": "2023-03-01T16:38:02.355248Z",
    "updated_at": "2023-03-01T16:38:02.355272Z",
    "lead_time": 9.405
  },
  {
    "corpusid": 278265,
    "section": "14/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.56,
    "text": "(Indeed from j∈Z (−X) j 2 + j∈Z X j 2 = 2 j∈Z X 4j 2 and the Jacobi identity 13.6(b) we obtain\nΨ(X) −2 Ψ(X 2 ) + Ψ(−X) −2 Ψ(X 2 ) = 2Ψ(−X 4 ) −2 Ψ(X 8 ).\nWe then replace X by wX.) Thus we have\nc = 2Ψ q (X 4 )Ψ((wX) 2 )Ψ(−X 4 ) −2 Ψ(X 8 ) + 2Ψ q (X 4 )Ψ((wX) 2 ) 2 Ψ(−wX) −1 Ψ(wX) −1 .\nThe product of the last two factors above can be rewritten using 3.16(f) with X replaced by wX. We obtain\nc = 2Ψ q (X 4 )Ψ((wX) 2 )Ψ(−X 4 ) −2 Ψ(X 8 ) + 2Ψ q (X 4 )Ψ((wX) 2 ) −1 Ψ(X 4 ).\nCombining the results above we obtain\nn∈2N α n X n = a + b ′ + b ′′ + c = 3Ψ q (X 4 )Ψ((wX) 2 )Ψ(−X 4 ) −2 Ψ(X 8 ) + 3Ψ q (X 4 )Ψ((wX) 2 ) −1 Ψ(X 4 ) + 2Ψ q (X 4 )Ψ(X 2 ) −1 Ψ(X 4 ). (c)\n4. Counting **data** in the dual group\n4.1. Let s ∈ Ξ be such that ∆ := ∆ s ∈ A N 0 . We show: (a) β ψ s = ψ −s ∈ Sβ ∆ . It is enough to show that if U ∈ U • then [[⊕ λ∈U V −s λ ]] = N/2 + [[⊕ λ∈U V s λ ]]. This follows from 2.1(a) since ⊕ λ∈U V −s λ = ⊕ λ∈−U V s λ = ⊕ λ∈U −1 V s λ is a subspace of V complementary to ⊕ λ∈U V s λ . 4.2. Let A (resp. A N ) be the set of unordered pairs (∆, ∆ ′ ) where ∆, ∆ ′ are elements of A (resp. A N ) such that ∆ ′ = β ∆. For k ∈ {0\n, 1, 2} let A N k be the set of all (∆, ∆ ′ ) ∈ A N such that exactly k of the numbers n ∆ 1 , n ∆ −1 are nonzero. Let A N, = (resp. A N,= ) be the set of all (∆, ∆ ′ ) ∈ A N such that ∆ = ∆ ′ (resp. ∆ = ∆ ′ ).",
    "id": 65,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 65,
    "created_at": "2023-03-01T16:38:06.390956Z",
    "updated_at": "2023-03-01T16:38:06.390991Z",
    "lead_time": 3.855
  },
  {
    "corpusid": 28774068,
    "section": "11/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.478,
    "text": "To determine the effect of ground vortices on the performance of the propeller, the **data** at two height ratios are compared, namely, h=R ¼ 3:0 and h=R ¼ 1:46. The height ratio of h=R ¼ 3:0 is the maximum height ratio could be achieved in the setup. It is supposed that the strength of ground vortices generated at the height ratio of h=R ¼ 3:0 is much smaller than that at h=R ¼ 1:46. The height ratio of h=R ¼ 1:46 is the position closest to the ground during our test, and it induces ground vortices which have the strongest impact on the propeller inflow. The difference of the propeller performance between h=R ¼ 1:46 and h=R ¼ 3:0 is negligible, as shown in Fig. 17. This means that the time-averaged performance of the propeller is independent of the ground vortices. First, the effects of the vortices entering the propeller in the propeller axial direction are cancelled out by each other. This hypothesis is confirmed by the tangential velocity distribution, as shown in the top right of Fig. 12. Second, although the effect of vortices entering the propeller in the radial direction induces an axial velocity decrease in the propeller inflow (as shown in the top left of Fig. 12), this influenced region is small compared with the whole disk region of the propeller and its effect is negligible as well. As the majority of research on turbofans is conducted on suction tubes, the impact of ground vortices on the loadings of a turbofan is not available. Our tests on a propeller give such **data** for the first time.",
    "id": 66,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 66,
    "created_at": "2023-03-01T16:38:13.993469Z",
    "updated_at": "2023-03-01T16:38:13.993504Z",
    "lead_time": 7.433
  },
  {
    "corpusid": 5181686,
    "section": "2/10",
    "subsection": 1.0,
    "section_pos_in_pct": 0.2,
    "text": "The estimates of the logarithms of the probability of the **data** under the models and assumptions regarding independence of allele frequencies are shown in Table 1. Under the admixture model, the smallest probability is associated with a prior K of 1 and little of the posterior probability is associated with higher K values. The distribution of members of the sample to inferred clusters is consistent with this observation. The proportion of individuals assigned to each cluster is approximately the same with little variation between ethnic groups ( Table 2). This symmetry is strongly suggestive of the absence of population structure in the AADM study sample. This is so because real population structure is associated with individuals being strongly assigned to one inferred cluster or another with the proportions assigned to each ethnic group showing asymmetry. The posterior probability under the no-admixture model also favours a K of 1. Examination of the distribution of individuals sampled to inferred clusters also shows the same strong symmetry. These consistent displays of symmetry suggest that a K of 1 is the most parsimonious model. The same conclusion was reached by examining the membership coefficients (Q). Irrespective of the value of K between the range of 2 and 6, Q is similar across the whole sample as illustrated by the bar plots in Figure 2.",
    "id": 67,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 67,
    "created_at": "2023-03-01T16:38:22.522335Z",
    "updated_at": "2023-03-01T16:38:22.522368Z",
    "lead_time": 8.357
  },
  {
    "corpusid": 14601365,
    "section": "3/8",
    "subsection": 1.0,
    "section_pos_in_pct": 0.375,
    "text": "For our second set of experiments, we use the three-class annotation scheme. We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be **data** sparsity since existing work on citation sentiment analysis uses more **data** (Athar, 2011).  ",
    "id": 68,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 68,
    "created_at": "2023-03-01T16:38:29.208901Z",
    "updated_at": "2023-03-01T16:38:29.208923Z",
    "lead_time": 6.516
  },
  {
    "corpusid": 119351763,
    "section": "10/10",
    "subsection": 2.0,
    "section_pos_in_pct": 1.0,
    "text": "Approximate **data** for the anhysteretic magnetization curve H m (m) = µ −1 (dW m /dm) are derived from the **data** for specific states and compared with the predictions of the analytic formulae in Fig. 5. The agreement is good, particularly near m = 0 and 1. The fields predicted by the two asymptotic formulae differ quite sharply at m = 1/2, where they indicate a step discontinuity in field that matches a similar step in H m (m), the **data** from the series of specific states.",
    "id": 69,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 69,
    "created_at": "2023-03-01T16:38:34.608553Z",
    "updated_at": "2023-03-01T16:38:34.608576Z",
    "lead_time": 5.19
  },
  {
    "corpusid": 207778550,
    "section": "10/48",
    "subsection": 5.0,
    "section_pos_in_pct": 0.208,
    "text": "With respect to their influence, USU can be either correlated or uncorrelated, depending on the circumstances. Fully correlated USU will determine the lowest possible accuracy to which it is possible to determine a certain physical quantity by a particular experimental technique due to unknown bias-inducing effects. Uncorrelated USU may contribute in a quasi-random way to unexplained scatter in measured **data** when it is assumed by the experimenter that the measurement procedures are well understood and under control. This may occur because of unrecognized instabilities in the experimental apparatus. Uncorrelated USU will certainly limit achievable precision, and possibly the attainable accuracy as well.",
    "id": 70,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 70,
    "created_at": "2023-03-01T16:38:41.743455Z",
    "updated_at": "2023-03-01T16:38:56.166669Z",
    "lead_time": 10.185
  },
  {
    "corpusid": 53869932,
    "section": "2/3",
    "subsection": 1.0,
    "section_pos_in_pct": 0.667,
    "text": "In Fig. 2    The inverse temperature legend in (b) also applies to (a) and its inset, which shows a close-up of the transition region. Arrows drawn in (b) clarify the direction of the magnetization curve for increasing and decreasing magnetic field. Note that not all **data** points are represented by symbols (though the connecting lines pass through them).",
    "id": 71,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 71,
    "created_at": "2023-03-01T16:39:12.992463Z",
    "updated_at": "2023-03-01T16:39:12.992496Z",
    "lead_time": 4.254
  },
  {
    "corpusid": 119479526,
    "section": "22/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.759,
    "text": "Figure 9 :\n9(a) Sequences of 400 walkers from the kombine sampler over 150 iterations. Two randomly selected sequences are highlighted in black. All samples to the left of the black dashed line are discarded. (b) The marginal Likelihoods and 2-sigma standard deviations are plotted for both samplers and all models.Finally, we examine the Posterior distribution and uncertainty intervals.Figure 10summarizes the posterior distributions obtained via the kombine (left) and MultiNest (right) samplers. We plot histograms and KDE distributions for each univariate parameter distribution. We provide multivariate summaries for each pair of coefficients via scatter plots of 2000 randomly selected samples. True parameter values are indicated by black markers. We first note that the samplers produce similar posterior parameter distributions. Furthermore, regions of high density in the Posterior closely align with the true parameter values. Figure 11 displays the model prediction and 95 th percentile uncertainty intervals obtained via the kombine (left) and MultiNest (right) samplers versus the synthetic **data** points. The uncertainty intervals are obtained by randomly sampling the Posterior distributions of the model parameters and computing the 2.5 th and 97.5 th percentile levels of the resulting model predictions. The uncertainty intervals show the expected spread of model predictions from the distribution of θ 0 and θ 1 . The models fit the synthetic **data** well, and both the model predictions and uncertainty intervals are nearly indistinguishable between the two samplers. The results of\n",
    "id": 72,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 72,
    "created_at": "2023-03-01T16:39:20.306737Z",
    "updated_at": "2023-03-01T16:39:20.306762Z",
    "lead_time": 7.153
  },
  {
    "corpusid": 208006088,
    "section": "9/26",
    "subsection": 3.0,
    "section_pos_in_pct": 0.346,
    "text": "Rainforest tree point process modelling. We consider a subset of the extensive rainforest **data** set credited to Hubbell et al. (2005)  In the survey area, the locations of all Trichilia tuberculata (a tree species of the Mahogany family) have been measured (see Fig. 7 in App. J). We tackle this spatial point pattern with a log-Gaussian Cox process model, which is an inhomogeneous Poisson process model for count data. The unknown intensity function λ(x) is modelled with a Gaussian process such that f (x) = log λ(x). We model locally constant intensity in subregions by discretising the region into np bins (Møller et al., 1998). This leads to having a Poisson observation model in each bin (see App. J). This model reaches posterior consistency in the limit of bin width going to zero (Tokdar and Ghosh, 2007). The accuracy thus improves with tighter binning. We use a separable Matérn-5/2 GP prior over f (x 1 , x 2 ), and discretise the area into a n × p = 200×100 grid with np = 20000 grid bins, and treat the first dimension as time.",
    "id": 73,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 73,
    "created_at": "2023-03-01T16:39:32.112790Z",
    "updated_at": "2023-03-01T16:39:32.112818Z",
    "lead_time": 11.63
  },
  {
    "corpusid": 207778550,
    "section": "24/48",
    "subsection": 3.0,
    "section_pos_in_pct": 0.5,
    "text": "This method was applied using the 11 fission cross section ratio **data** sets specified in Table III Table VI reports for each **data** set i the average relative uncertainties derived from the ratio of the uncertainty and the best estimate for each of the three energy groups. The average values derived from the reported total uncertainties, and the uncertainties due to systematic and random effects are compared with the values derived from the statistical analysis method proposed by Badikov et al. [35]. The results in Table VI confirm the conclusions based on Eqs. (19) and (20) that the uncertainties due to systematic effects reported by Toveson et al. [85], Behrens et al. [87], Cierjacks et al. [89] and Lisowski et al. [92] might be underestimated. In addition, there is an indication that the uncertainties due to random effects reported for the n TOF **data** (2) [86] and the **data** of Cierjacks et al. [89] are overestimated, while those of Difilippo et al. [88] and Tovesson et al. [85] are underestimated.",
    "id": 74,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 74,
    "created_at": "2023-03-01T16:39:41.014570Z",
    "updated_at": "2023-03-01T16:39:52.806132Z",
    "lead_time": 17.326999999999998
  },
  {
    "corpusid": 55381955,
    "section": "1/12",
    "subsection": 1.0,
    "section_pos_in_pct": 0.083,
    "text": "The origin of the knee in the all-particle spectrum of CRs is inextricably connected with the issue of the end of the Galactic CR spectrum and the transition to extragalactic CRs: in Ref. [6], the authors try to model the transition region by summing the Galactic SNR contribution and the flux of nuclei of extragalactic origin, required to fit the Auger **data** [4]. The authors reach the conclusion that in order to satisfy the observational requirement of a predominantly light composition in the EeV region [4,2,3,60], an extra component of extragalactic protons is required. Such an extra component appears to be in good agreement with the proton spectrum as measured by KASCADE-Grande [10]. However, both the proton and iron spectra measured by KASCADE-Grande in the energy region 10 16 −10 18 eV suggest that either the injection spectra are not cut off exponentially at the maximum energy (namely the number of particles decreases more slowly than an exponential at energies E ≫ EM ), or there is some, as yet unknown, class of sources with maximum energy much in excess of the knee energy.",
    "id": 75,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 75,
    "created_at": "2023-03-01T16:40:07.613140Z",
    "updated_at": "2023-03-01T16:40:07.613169Z",
    "lead_time": 10.63
  },
  {
    "corpusid": 216162633,
    "section": "2/6",
    "subsection": 3.0,
    "section_pos_in_pct": 0.333,
    "text": "Theorem 2.2 ( [7,14]). For every solution (u,u) of (NLW), in the sense of Definition 2.1, with initial **data** (u 0 , u 1 ) ∈ H = V A × W P ≡ H 1 0 (Ω) × L 2 (Ω), only one of the following holds.",
    "id": 76,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 76,
    "created_at": "2023-03-01T16:40:09.852372Z",
    "updated_at": "2023-03-01T16:40:09.852411Z",
    "lead_time": 2.053
  },
  {
    "corpusid": 17895596,
    "section": "5/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.455,
    "text": "The analysis of low-multipoles from SKA continuum surveys can benefit from the methods developed for the study of the CMB. Missing sky area is always a problem for low-mode measurements. For CMB studies, many methods were proposed to deal with a mask of missing **data** for both power spectrum estimation and phase recovery. For the power spectrum, one of the most used methods is MASTER (Hivon et al. 2002). It consists in first building a matrix which captures the coupling between the modes induced by the mask, and then inverting this matrix. In Figure 4 we plot the angular power spectrum of SKA galaxies for low-mulitpoles with error bars corresponding to a SKA1 continuum survey. For phase recovery, or more generally for large scale map reconstruction, many methods have been proposed based on Wiener filtering, l 2 or l 1 norm regularization, constraint realizations or diffusion [see Starck et al. (2013) and references therein]. Based on these new methodological idea, Planck **data** were analyzed with a mask removing 27% of the sky (Planck collaboration 2014c; Rassat et al. 2014). For a given observed sky area, the shape of the mask will also be important. The importance of random sampling is also described in Paykari et al. (2013), and many small missing parts, randomly distributed, will always be much better for large scale studies than a compact big missing part.",
    "id": 77,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 77,
    "created_at": "2023-03-01T16:40:14.294773Z",
    "updated_at": "2023-03-01T16:40:14.294803Z",
    "lead_time": 4.253
  },
  {
    "corpusid": 91186200,
    "section": "12/14",
    "subsection": 1.0,
    "section_pos_in_pct": 0.857,
    "text": "Figure 3 .\n3Results from Carr-Purcell-Meibom-Gill (CPMG) measurements of PDMS melts under shear. The insets indicate the shear rates applied in the Couette cell. On the left, the result for the sample of a molecular weight of 410 g/mole, and, on the right, the result for the sample of a molecular weight of 63,000 g/mol, are depicted. Below each figure the residuals from a mono exponential (left) or biexponential (right) fit to the experimental **data** acquired under the highest shear rate are depicted as a measure of the quality of the fit.\n",
    "id": 78,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 78,
    "created_at": "2023-03-01T16:40:19.256311Z",
    "updated_at": "2023-03-01T16:40:19.256344Z",
    "lead_time": 4.78
  },
  {
    "corpusid": 119080499,
    "section": "19/30",
    "subsection": 1.0,
    "section_pos_in_pct": 0.633,
    "text": "The optical absorptivity spectra for samples of LaH x and NdH x were also measured by Peterman et al. [36] at 4.2K at near-normal incidence. From the absorption data, they deduced the real and imaginary part of the dielectric constant, and hence the optical conductivity by using Kramers-Kronig analysis. They found a relatively broad feature for LaH 2.87 . However, their samples were polycrystals and the use of Kramers-Kronig analysis might enlarge the **data** uncertainties. On the other hand, a more recent and accurate measurement reported by Griessen et al. [39] sheds new light on the optical conductivity curve. The optical transmission spectra of the insulating phase YH 3−δ were measured as functions of the photon frequencyhω and of hydrogen vacancy concentration δ. The effect of the vacancy appears to reduce the overall transmission spectra quite evenly between hω = 0.5 eV and 2 eV. In our theory, the δ-dependent conductivity σ(ω) athω < 2.8 eV mainly determined by the optical transition from the vacancy state to the conduction bounds. The transition energy from the valence bands to the vacancy state is larger because of the larger energy difference between the two states and because of the Coulomb repulsion of the doubly occupied electron states on the same vacancy site. Since the vacancy state is highly localized, the transition matrix largely depends on the density of states of the conduction bands, which is expected to lack pronounced feature.",
    "id": 79,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 79,
    "created_at": "2023-03-01T16:40:24.851868Z",
    "updated_at": "2023-03-01T16:40:24.852019Z",
    "lead_time": 5.418
  },
  {
    "corpusid": 17671315,
    "section": "20/32",
    "subsection": 3.0,
    "section_pos_in_pct": 0.625,
    "text": "Step 3. Let Y = X be any other target listed in Table 1. When applying GLK to the **data** F Y (t), use the same calibration factor CF (X) . Here CF (wood stake) = 10 −5 , R GLK = 3.8; b) the function R GLK (plastic cylinder) for the case when the plastic cylinder was the calibration target. Here CF (plastic cylinder) = 1.8 · 10 −5 , R GLK (plastic cylinder) = 0.28; c) the function R GLK (bush) for the case when bush was the calibration target. Here CF (bush) = 0.6 · 10 −5 , R GLK (bush) = 6.4.",
    "id": 80,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 80,
    "created_at": "2023-03-01T16:40:28.445677Z",
    "updated_at": "2023-03-01T16:40:28.445704Z",
    "lead_time": 3.417
  },
  {
    "corpusid": 119479526,
    "section": "26/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.897,
    "text": "Figure 13 :\n13errors on each dataset by the best fit hyperparameter values of 0.9, 0.4 and 0.1 for hyperparameters α A , α B and α C , respectively. Figure 14 displays the histograms and KDE distributions for each parameter and scatter plots for each pair of parameters. Each true parameter value (marked by the black dashed line) falls within the high probability regions of the associated Posterior distribution. Considering the log marginal The model prediction, uncertainty intervals and synthetic **data** points are plotted versus x for the posterior distributions (a) employing the reported errors, and (b) weighing the errors using hyperparameters. Note that for a visual effect we have rescaled the errorbars in (b) by the best fit hyperparameters.\n",
    "id": 81,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 81,
    "created_at": "2023-03-01T16:40:33.323001Z",
    "updated_at": "2023-03-01T16:40:33.323028Z",
    "lead_time": 4.704
  },
  {
    "corpusid": 19033986,
    "section": "1/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.036,
    "text": "Recently the PAMELA experiment observed an unexpected rise with energy of the e + /(e + +e − ) fraction in cosmic rays, suggesting the existence of a new component. The sharp rise suggests that the new component may be visible in the e − + e + spectrum. As worked out in [1] and further stressed in [2], FERMI provides the first precise measurement of the e + + e − spectrum. The main purpose of this article is to analyze the implications of the first FERMI e + + e − **data** [3], in conjunction with the new HESS measurements [4].",
    "id": 82,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 82,
    "created_at": "2023-03-01T16:40:37.615051Z",
    "updated_at": "2023-03-01T16:40:37.615105Z",
    "lead_time": 4.116
  },
  {
    "corpusid": 19033986,
    "section": "7/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.25,
    "text": "We now demonstrate the viable models that can fit all the available **data** for charged cosmic rays including the new FERMI and HESS data. For our analysis we fit to the PAMELA e + /(e + +e − ) **data** and the e + +e − **data** from FERMI and HESS. We also include the preliminary un-normalized PAMELA e − spectrum available at [17], although it has a minor impact. We then describe the predictions and bounds for ICS as discussed in Section 3, and include bounds from several other observations previously studied in [11,38,37,12]. These include the bounds from the HESS photon measurements in the Galactic Center and Galactic Ridge and from up-going muons measured at SuperKamiokande.",
    "id": 83,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 83,
    "created_at": "2023-03-01T16:40:48.930228Z",
    "updated_at": "2023-03-01T16:40:48.930276Z",
    "lead_time": 11.091
  },
  {
    "corpusid": 11224784,
    "section": "6/14",
    "subsection": 1.0,
    "section_pos_in_pct": 0.429,
    "text": "Although the abundances of heavy s-process elements could not be estimated, the upper limits of the abundances of Ce, Nd do demonstrate the lighter s-process elements are more enhanced even in EHe stars. It is generally acknowledged that 13 C(α, n) 16 O is the main source of neutrons to run the s-processing in the He-burning shells of intermediate-mass AGB stars. Sufficient amounts of 13 C are to be generated by slow mixing of protons into the 12 C rich intershell regions to generate neutrons. The neutron irradiation occurs in radiative conditions. The heavier the neutron flux the greater is the abundance of heavies relative to light s-process elements. Busso et al. (2001)  the distribution of the ratio of heavy s-process celements (hs) to the light s-process (ls) elements with respect to metallicity to characterize various parameters of neutron exposures during the third dredgeup phase in AGB stars eg. mass of 13 C pocket in the inter shell regions. Reddy et al. (2002) showed that the variation of the [hs/ls] with respect to metallicity in post -AGB stars (that went through third dredgeup) is characterized by a model ST/1.5 of Busso et al. (2001). 4 ) shows that the enhancements are positive and both show a similar range in their abundances. We compared the run of the ratio of [ls/hs] for RCBs and EHes with respect to the metallicity parameter [M] . The estimates for EHes are based on the upper limits for the heavy s-process elements and includes **data** from our ongoing analysis of the HST UV spectra. Estimates of U Aqr and the born again giant, Sakurai's object (during May -Oct 1996) are also included for comparison. Both the groups RCBs and EHes blend together emphasizing the similarity in their ls/hs ratios. that shown by post-AGB stars. It also, probably, suggests the s-processing in RCBs and EHes is not a result of the third dredgeup and could have happened when the stars passed through a second AGB phase (presumably).",
    "id": 84,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 84,
    "created_at": "2023-03-01T16:42:35.676907Z",
    "updated_at": "2023-03-01T16:42:35.676940Z",
    "lead_time": 106.513
  },
  {
    "corpusid": 118912762,
    "section": "3/9",
    "subsection": 2.0,
    "section_pos_in_pct": 0.333,
    "text": "As can be seen from Table 2, for t 3 (f N α ) = t 3 (f K α ) very stringent constraints can be derived for F ij L,R , (i = j) which strongly limits the mixing of ordinary and exotic fermions. However, it is possible to evade these bounds by considering the fine-tuned cases in which the mixing matrices F † α F α are diagonal. These correspond to those directions in parameter space in which each known ordinary fermion mixes with its own exotic fermion. If the strong assumption F ij L,R = 0 for i = j is made, one can write\nF † α F α ij = s i α 2 δ i j α = L, R, (2.7) in which s i α 2 ≡ sin 2 θ i α , where θ i L(R)\nis the mixing angle of the i th L-handed (Rhanded) light fermion. In this case the ordinary-exotic mixing is parametrized by one angle per each L-or R-handed charged fermion. The same angles enter also the expression for the CC measurables. We refer to Refs. [4,5] for the extension of the formalism to the CC sector. Even if the limits on FCNC processes are not effective to constrain these particular fine-tuned mixings, still all the s i α 2 can be constrained by looking at the high precision **data** involving CC and flavor-diagonal NC. The corresponding limits are given in Table 3.",
    "id": 85,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 85,
    "created_at": "2023-03-01T16:42:39.177359Z",
    "updated_at": "2023-03-01T16:42:39.177383Z",
    "lead_time": 3.276
  },
  {
    "corpusid": 119698924,
    "section": "1/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "When considering equation (1.1) in a bounded subset Ω ⊂ R N , in view of H1, since ρ is allowed either to vanish or to diverge at S, it is natural to consider the following initial value problem associated with (1.1):\n(1.5) ρ∂ t u = ∆[G(u)] in Ω × (0, T ], u = u 0 in Ω × {0},\nwhere no boundary conditions are specified at S. We require ρ, G and f to satisfy hypotheses H1-2-3; furthermore, for the initial datum u 0 we assume that\nH3. u 0 ∈ L ∞ (Ω) ∩ C(Ω).\nConcerning the existence and uniqueness of the solutions to (1.5), the case G(u) = u has been largely investigated, using both analytical and stochastic methods (see, e.g., [23,28,29,32]). Also analogous elliptic or elliptic-parabolic equations have attracted much attention in the literature (see, e.g., [6,7,8,9,10,11,26,27]); in particular, the question of prescribing continuous **data** at S has been addressed (see, e.g., [23,27,28,29]).",
    "id": 86,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 86,
    "created_at": "2023-03-01T16:42:44.959118Z",
    "updated_at": "2023-03-01T16:42:44.959143Z",
    "lead_time": 5.582
  },
  {
    "corpusid": 240554273,
    "section": "12/22",
    "subsection": 1.0,
    "section_pos_in_pct": 0.545,
    "text": "One of the most famous ways to assess the sensitivity of the model is the global influence. Starting from case deletion (see, cook, 1977). The casedeletion is a common approach to study the effected of dropping the (i-th) observation from the data. First we obtain the estimates of parameters and get the log-like of these estimates. Then we estimate the parameters after cancelling one observation from the dataset. The aim is comparing between the log-likelihood in two cases. This measure can be written as: Cook measure = 2(Log-like before eliminating obs. -Log-like after eliminating obs.) To explain the effect of cancelling one observation on the estimation of parameters, we generate the same **data** with 99 n  observations. The  696586 We have noted that the cook is positive negative value, this means that the log-likelihood in small values (negative) is less than it is large values. This is happened because the values of estimates become less.",
    "id": 87,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 87,
    "created_at": "2023-03-01T16:42:50.047525Z",
    "updated_at": "2023-03-01T16:42:50.047550Z",
    "lead_time": 4.857
  },
  {
    "corpusid": 6811986,
    "section": "17/20",
    "subsection": 1.0,
    "section_pos_in_pct": 0.85,
    "text": "A summary of results on negative implication is below. We notice that the decidable cases are orthogonal to those for positive implications. Note also that unlike in the positive cases, we have tractable cases for **data** complexity.  ",
    "id": 88,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 88,
    "created_at": "2023-03-01T16:42:53.231623Z",
    "updated_at": "2023-03-01T16:42:53.231646Z",
    "lead_time": 2.955
  },
  {
    "corpusid": 17895596,
    "section": "6/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.545,
    "text": "With the assistance of Lyman alpha data, one can model the luminosity function and evolution. With the SKA morphology **data** we expect to be able to identify different type of sources. This will allow us to study cross correlations between star forming galaxies and AGNs. We could also cross correlate with the CMB and different types of radio sources, which have different redshift distributions.",
    "id": 89,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 89,
    "created_at": "2023-03-01T16:42:56.224739Z",
    "updated_at": "2023-03-01T16:42:56.224764Z",
    "lead_time": 2.757
  },
  {
    "corpusid": 235829209,
    "section": "30/47",
    "subsection": 2.0,
    "section_pos_in_pct": 0.638,
    "text": "There are also some other advanced machine-learning techniques available that can better evaluate accuracy of an ANN model. For example, using the Kfold cross validation method [31] ensures that every observation from the original dataset has the chance of appearing in training and test sets, especially when limited input **data** is available. In our examples for lattice optimization, rather than using the time-consuming cross-validation that K-fold requires, sufficient training **data** were generated with a particle tracking simulation code, because using only one-turn tracking for such rings requires much less of a demand on computational resources.",
    "id": 90,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 90,
    "created_at": "2023-03-01T16:43:02.254755Z",
    "updated_at": "2023-03-01T16:43:02.254778Z",
    "lead_time": 5.82
  },
  {
    "corpusid": 17837291,
    "section": "1/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.019,
    "text": "Step 2. Gluing identity. Consider the vector bundle U d := π * V d → M d (X), restricted to the fixed point components F r . A point in (C, f ) in F r is a pair (C 1 , f 1 , x 1 )×(C 2 , f 2 , x 2 ) of 1-pointed stable maps glued together at the marked points, ie. f 1 (x 1 ) = f 2 (x 2 ). From this, we get an exact sequence of bundles on F r :\n0 → i * r U d → U ′ r ⊕ U ′ d−r → e * V → 0.\nHere i * r U d is the restriction to F r of the bundle U d → M d (X). And U ′ r is the pullback of the bundle U r → M 0,1 (d, X) induced by V , and similarly for U ′ d−r . Taking the multiplicative characteristic class b, we get the identity on F r :\ne * b(V )b(i * r U d ) = b(U ′ r )b(U ′ d−r ).\nThis is what we call the gluing identity. This may be translated to a similar quadratic identity, via Step 1, for Q d in the equivariant cohomology groups H * S 1 (W d ). The new identity is called the Euler **data** identity.",
    "id": 91,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 91,
    "created_at": "2023-03-01T16:43:07.641862Z",
    "updated_at": "2023-03-01T16:43:07.641886Z",
    "lead_time": 5.152
  },
  {
    "corpusid": 207778550,
    "section": "8/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.167,
    "text": "It can happen that no matter how hard evaluators attempt to apply rigorous evaluation procedures faithfully, and to track down, adjust, and include all the different sources of uncertainty known to them (including their correlations) for the various experiments considered, the resulting evaluated uncertainties may still appear to be inconsistent with the input data, and thus will be perceived as unacceptable by **data** users. Most often, these evaluated uncertainties are perceived as being too small, as mentioned above.",
    "id": 92,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 92,
    "created_at": "2023-03-01T16:43:12.034549Z",
    "updated_at": "2023-03-01T16:43:12.034572Z",
    "lead_time": 4.153
  },
  {
    "corpusid": 52995678,
    "section": "29/29",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "\n, but also Lagrangian **data** (FTLE field). Thus, the new ROMs are Lagrangian data-driven ROMs.In Sec-\ntion 4.1, we construct the new Lagrangian ROMs. In Section 4.2, we briefly \ncontrast traditional Eulerian ROMs (built from Eulerian data) and the new \nLagrangian ROMs (which are built from both Eulerian and Lagrangian data). ",
    "id": 93,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 93,
    "created_at": "2023-03-01T16:43:18.815332Z",
    "updated_at": "2023-03-01T16:43:18.815357Z",
    "lead_time": 6.551
  },
  {
    "corpusid": 119313773,
    "section": "10/21",
    "subsection": 2.0,
    "section_pos_in_pct": 0.476,
    "text": "• The amount of **data** N required to detect a breakdown of linear response for a given perturbation size can be very large. For the logistic map with a given perturbation size of the order of ε = O(10 −6 ) one needs at least N = 600, 000 for a smooth observable A(x) = x. Hence, an apparent linear response seen in a given time series might be spuriously caused by an insufficient quantity of data.",
    "id": 94,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 94,
    "created_at": "2023-03-01T16:43:22.895531Z",
    "updated_at": "2023-03-01T16:43:22.895568Z",
    "lead_time": 3.837
  },
  {
    "corpusid": 207778550,
    "section": "46/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.958,
    "text": "TABLE VI .\nVIThe average relative uncertainties for each **data** set i resulting from the variance analysis proposed by Badikov et al.[35] (< σi >, < ση,i >, < σ ,i >) are compared with the ones derived from the reported uncertainties (< ui >, < uη,i > , < u ,i >). The total uncertainties are indicated without a sub-index. For the uncertainties due to systematic and random effects the sub-indices η and , respectively, are used. They are derived by taking the average of the relative values for the three energy groups multiplied by 100. The last three columns give the ratio between the values from the variance analysis and the reported ones for each component and for the total uncertainty. Uncertainty ratios that differ significantly from one indicate overestimated (> 1 in red) or underestimated (< 1 in blue) reported uncertainties.Dataset \nReported values \nVariance analysis \nRatio \n< σi > < ση,i > < σ ,i > < ui > < uη,i > < u ,i > total η \nTovesson \n0.91 \n0.84 \n0.34 \n2.38 \n2.31 \n0.55 \n2.63 2.76 1.63 \nn TOF (1) \n2.40 \n1.73 \n1.77 \n2.60 \n2.08 \n1.56 \n1.08 1.20 0.94 \nn TOF (2) \n3.65 \n2.49 \n2.67 \n2.66 \n2.34 \n1.27 \n0.73 0.94 0.48 \nn TOF (3) \n3.65 \n3.35 \n1.44 \n2.43 \n2.12 \n1.18 \n0.67 0.63 0.82 \nn TOF (4) \n4.39 \n3.74 \n2.30 \n3.04 \n2.29 \n2.00 \n0.69 0.61 0.87 \nBehrens \n1.46 \n0.81 \n1.21 \n2.57 \n2.26 \n1.22 \n1.76 2.79 1.01 \nDifilippo \n2.48 \n2.39 \n0.66 \n2.77 \n2.20 \n1.68 \n1.12 0.92 2.54 \nCierjacks \n3.01 \n1.09 \n2.81 \n2.77 \n2.36 \n1.45 \n0.92 2.16 0.52 \nCoates \n3.44 \n2.92 \n1.82 \n3.27 \n2.31 \n2.32 \n0.95 0.79 1.27 \nShcherbakov 2.53 \n2.45 \n0.64 \n2.47 \n2.31 \n0.88 \n0.98 0.94 1.37 \nLisowski \n1.54 \n0.94 \n1.22 \n2.59 \n2.28 \n1.24 \n1.68 2.42 1.01 ",
    "id": 95,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 95,
    "created_at": "2023-03-01T16:43:32.643636Z",
    "updated_at": "2023-03-01T16:43:32.643668Z",
    "lead_time": 9.537
  },
  {
    "corpusid": 251711372,
    "section": "1/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.008,
    "text": "The remainder of this paper is organized as follows. Section 2 reviews the relevant literature. Section 3 provides the testable research hypotheses. Section 4 presents the **data** and the event study methodology. In Section 5, we present and discuss our main findings. Section 6 concludes the paper.",
    "id": 96,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 96,
    "created_at": "2023-03-01T16:43:38.958709Z",
    "updated_at": "2023-03-01T16:43:38.958736Z",
    "lead_time": 6.121
  },
  {
    "corpusid": 209947757,
    "section": "9/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "In every test final modal models have been determined out of several excitation runs according to meaningful quality measures. Obtained results from multiple free-free tests allow for conclusions concerning effects of the finishing and the repeatability of the manufacturing process in terms of the modal parameters. For this blade series the impact of the finish on mode shape correlation is stronger than differences resulting from the production. Regarding frequency and damping, changes caused by the finishing and deviations between the blades within the same manufacturing steps are of a similar magnitude. For blade #1 a non-linear damping behaviour in the 1 st flap mode has been identified. Though, a comprehensive flutter analysis demands in preparation the examination on non-linear behaviour in other modes of interest. Due to the high sensor density and the advantages of both test scenarios, the acquired modal **data** represents an optimal basis for updating finite element models towards structural dynamic properties.",
    "id": 97,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 97,
    "created_at": "2023-03-01T16:43:44.057176Z",
    "updated_at": "2023-03-01T16:43:44.057207Z",
    "lead_time": 4.91
  },
  {
    "corpusid": 119217429,
    "section": "2/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.154,
    "text": "G0.25 was observed with ALMA (Hills et al. 2010) during Early Science Cycle 0 with extended configuration, using the Band 3 receivers. The array was in a configuration with projected baselines length between ∼ 36.1 to ∼ 452.8 m, sensitive to maximum angular scales of 9. ′′ 1 and providing a synthesized beam of 2. ′′ 30 × 1. ′′ 53. The field of view was ∼ 65. ′′ 5. A total of six **data** sets were collected, using 24 antennas of 12 m diameter and accounting for 4 hours of total integration time on target. Weather conditions were good and stable, the system temperature varied from 50 to 60 K.",
    "id": 98,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 98,
    "created_at": "2023-03-01T16:43:53.107650Z",
    "updated_at": "2023-03-01T16:43:53.107677Z",
    "lead_time": 8.862
  },
  {
    "corpusid": 16212589,
    "section": "1/6",
    "subsection": 1.0,
    "section_pos_in_pct": 0.167,
    "text": "We have applied modem techniques to extend the range of Po measurement below ~10 -7. The use of a skeletal muscle Na § channel clone deficient in fast inactivation permitted measurement of steady currents in the hyperpolarized range of potentials, and single-channel recording permitted detection of very brief open events even when very rare. Mean-Variance analysis (Patlak, 1993) was used, in part, to process hundreds of megabytes of **data** in order to facilitate analysis but maintain demonstrable quality. Our results show a limiting slope for Na § channels that is twice as steep as previously suspected, with a minimum transfer of 12 charges needed to open these Na + channels. These results have major implications for our understanding of Na + channel gating, for the estimates of the number of gates actively involved, and for the interpretation of mutagenesis experiments.",
    "id": 99,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 99,
    "created_at": "2023-03-01T16:44:02.669547Z",
    "updated_at": "2023-03-01T16:44:02.669579Z",
    "lead_time": 9.291
  },
  {
    "corpusid": 119419652,
    "section": "4/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.222,
    "text": "The CMS collaboration has performed a set of differential W +jets cross-section measurements with 2.2 fb −1 of Run 2 **data** -the first such measurements to be performed with a √ s = 13 TeV dataset [9]. The analysis strategy targeted events with a single, isolated, high-p T muon and considered jet multiplicities up to N jets = 6. Differential results unfolded to stable particle level were presented as a function of a number of kinematic observables including: the transverse momentum and absolute rapidity of the i th leading jet; the azimuthal separation between the i th leading jet and the selected reconstructed muon 2 ; the jet multiplicity; the scalar p T sum of reconstructed jets (H T ); and the separation in η-φ space between the reconstructed muon and its closest reconstructed jet. The final results as a function of H T for the N jets ≥ 1 case can be seen in Figure 2a. The measurements from **data** were compared to the predictions from a variety of simulated datasets. In all cases the results for the angular observables were found to be well described. Disagreements were observed between the **data** and a leading-order (LO) MG_aMC@NLO prediction, which underestimates the measured **data** at low H T , as well as at low to moderate jet p T .",
    "id": 100,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 100,
    "created_at": "2023-03-01T16:44:10.679775Z",
    "updated_at": "2023-03-01T16:44:10.679811Z",
    "lead_time": 7.786
  },
  {
    "corpusid": 15394341,
    "section": "5/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.278,
    "text": "Autocorrelation partitions the continuous **data** into N short overlapping time windows, and computes N(N-1)/2 normalized CC values between all possible window pairs using Eq. 1. Using a time window length of M = 200 samples (10 s), and τw = 2 samples (0.1 s) as the lag between adjacent time windows (Table S1), we found that N = 6,047,901, which is about 10 times greater than N fp = 604,781. This factor of 10 difference is attributed to the use of a 1 s lag between FAST fingerprints, compared to a 0.1 s lag for autocorrelation.",
    "id": 101,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 101,
    "created_at": "2023-03-01T18:39:24.019115Z",
    "updated_at": "2023-03-01T18:39:24.019142Z",
    "lead_time": 6913.148
  },
  {
    "corpusid": 5648146,
    "section": "10/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.556,
    "text": "The determinism test, which is based on a correct reconstruction of the attractor, enables us to measure average directional vectors in the coarse-grained embedding space [19]. The embedding space should be coarse-grained into equally sized boxes. Each pass of the trajectory through the th box can be regarded as a unit vector , and their directions are approximated by the points where the particular pass enters and leaves the box. Their average directional vector of the unit vector through the th box is defined by\n= 1 ∑ =1 ,(7)\nwhere is the total of all passes in box . Therefore, an approximate direction for the vector field can be obtained by all occupied boxes. If the **data** set sources from a deterministic dynamic process as well as a fine enough coarse-grained partitioning, the vector inside one box would nearly not cross. Since each crossing will decrease the size of the average vector , the average length of all directional vectors will be 1 for a deterministic process, while for a random process it will be 0. In order to verify whether the studied sway is from a stationarity process, stationarity test is evaluated [16,20]. To perform this method, the time series (20000 points) is divided into ℎ (ℎ = 100) short nonoverlapping segments; therefore, a cross-prediction error ( ) statistic is calculated for ℎ 2 possible combinations. We obtain a very sensitive statistic capable of detecting minute changes in dynamics and thus a very powerful probe for stationarity. If of each combination is not much larger than the average, the examined time series can be considered to be from a stationary system.",
    "id": 102,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 102,
    "created_at": "2023-03-01T18:39:42.332902Z",
    "updated_at": "2023-03-01T18:39:42.332937Z",
    "lead_time": 18.088
  },
  {
    "corpusid": 118420717,
    "section": "3/10",
    "subsection": 2.0,
    "section_pos_in_pct": 0.3,
    "text": "• From the simulation the quantities α, A(t), B(t) and Γ(t) are calculated using the filtered elements. Numerical forward differentiation with four **data** points was used to computeṗ andq.",
    "id": 103,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 103,
    "created_at": "2023-03-01T18:39:52.510130Z",
    "updated_at": "2023-03-01T18:39:52.510156Z",
    "lead_time": 9.991
  },
  {
    "corpusid": 53380409,
    "section": "101/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.849,
    "text": "The event selection discussed in Sec. VI relies on the reconstruction of electrons, muons, jets, and missing transverse momentum (with magnitude E miss T ). Although the requirements vary for the different channels, the general algorithms are introduced below. The small differences between the efficiencies measured in **data** and MC simulation are corrected for by applying scale factors to the MC simulation so that it matches the data.",
    "id": 104,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 104,
    "created_at": "2023-03-01T18:41:00.833948Z",
    "updated_at": "2023-03-01T18:41:00.833974Z",
    "lead_time": 68.141
  },
  {
    "corpusid": 126349038,
    "section": "21/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.75,
    "text": "Fig. 9\n9Axial variation of flow variables in SDTV (left column) and BKTV (right column) cores obtained using S1 through S4, and mean code with P E jj bars is compared with experimental **data** with U D bars for b 5 20 deg. The SDTV onset at x 5 0.077 and BKTV onset at x 5 0.4 are shown in radial location (R) subplot.\n",
    "id": 105,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 105,
    "created_at": "2023-03-01T18:41:04.128414Z",
    "updated_at": "2023-03-01T18:41:04.128456Z",
    "lead_time": 3.119
  },
  {
    "corpusid": 16515985,
    "section": "15/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.833,
    "text": "FIG. 7 .\n7The G ′ (ω) **data** ofFig. 5for various shear times collapsed onto the zero shear treatment (t s = 0) curve.\n",
    "id": 106,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 106,
    "created_at": "2023-03-01T18:41:06.843853Z",
    "updated_at": "2023-03-01T18:41:06.843881Z",
    "lead_time": 2.536
  },
  {
    "corpusid": 251500724,
    "section": "13/26",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "Figure 2 .\n2The kC temperature as a function of time during the printing stages: (a) cooling down from the loading temperature to the extruding temperature (the experiment at 40 °C is shown); (b) cooling down of the generated droplet (see text) after extrusion, during the self-supporting stage. The droplet was printed at 40 °C on the printer plate kept at 25 °C. Lines correspond to the best fit of the **data** to Equation(1).\n",
    "id": 107,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 107,
    "created_at": "2023-03-01T18:41:09.725657Z",
    "updated_at": "2023-03-01T18:41:09.725683Z",
    "lead_time": 2.698
  },
  {
    "corpusid": 17671315,
    "section": "24/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.75,
    "text": "Figure 2 :\n2The schematic diagram of the **data** collection by the Forward Looking Radar model (4)-(8) of section 3 R is the function of the spatial variable x, i.e. R = R (x) := ε r (x) , x ∈ (0, 1) . Hence, after computing R (x) we took the value R to compute the dielectric constant of the target ε r (target), where\n",
    "id": 108,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 108,
    "created_at": "2023-03-01T18:41:12.592979Z",
    "updated_at": "2023-03-01T18:41:12.593005Z",
    "lead_time": 2.692
  },
  {
    "corpusid": 118522044,
    "section": "37/41",
    "subsection": 1.0,
    "section_pos_in_pct": 0.902,
    "text": "Figure 7 .\n7Low ω behavior of the real and imaginary part of the xx conductivity for decreasing values the temperature. Frequency is measured in terms of T c . The yellow plot corresponds to T = 0.99 T c , then we have T = 0.94 T c , T = 0.92 T c and eventually the black line representing the case with T = 0.9 T c . We remind the reader that the **data** were obtained considering c = 1/10 and µ φ /µ η = 1.\n",
    "id": 109,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 109,
    "created_at": "2023-03-01T18:41:18.718663Z",
    "updated_at": "2023-03-01T18:41:18.718690Z",
    "lead_time": 5.964
  },
  {
    "corpusid": 24865762,
    "section": "22/34",
    "subsection": 1.0,
    "section_pos_in_pct": 0.647,
    "text": "We chose N x = 2 7 , L = 2π, D = 2 × 10 −1 and δt = 10 −3 for our simulations. The initial **data** is taken to be\nu 0 (x) = 10 −1 2 L cos 2πx L .(124)\nWithout any surprise, the integration reveals striking differences in term of patterns exhibited by the field u(t, x) compared to those obtained from the nonlinear example of Section VIII A; compare panel (b) with panel (a) in Fig. 7.",
    "id": 110,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 110,
    "created_at": "2023-03-01T18:41:21.778473Z",
    "updated_at": "2023-03-01T18:41:21.778499Z",
    "lead_time": 2.883
  },
  {
    "corpusid": 53380409,
    "section": "33/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.277,
    "text": "The small-R jet uncertainties are relevant for most of the channels in the combination, including those with leptonic final states that contain at least one neutrino, due to the impact of those uncertainties on the E miss T measurement. The uncertainties in the jet energy scale and resolution are derived by comparing the response between the **data** and the simulation in various kinematic regions and event topologies. Additional contributions to this uncertainty come from the dependence on the pileup activity and on the flavor composition of the jets as well as the punchthrough of the energy from the calorimeter into the muon spectrometer. An uncertainty in the efficiency for jets to satisfy the JVT requirements is assessed. The small-R jet uncertainties are summarized in Table VI. For large-R jets, the uncertainties in the energy, mass, and D 2 scales are estimated by comparing the ratio of calorimeter-based to track-based jet p T measurements in dijet events between the **data** and the simulation. The uncertainties in the jet mass resolution and jet energy resolution as well as D 2 are assessed by applying additional smearing of the jet observables according to the uncertainty in their resolution measurements. A summary of the large-R jet systematic uncertainties is provided in Table VII. The flavor-tagging uncertainty is evaluated by varying the data-to-MC corrections in various kinematic regions, based on the measured tagging efficiency and mistag rates. These variations are applied separately to b-hadron jets, c-hadron jets, and light (quark or gluon) jets, leading to three uncorrelated systematic uncertainties. An additional uncertainty is included due to the extrapolation for the jets with p T beyond the kinematic reach of the **data** calibration. The flavor-tagging uncertainties are summarized in Table VIII.",
    "id": 111,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 111,
    "created_at": "2023-03-01T18:41:29.108969Z",
    "updated_at": "2023-03-01T18:41:29.108995Z",
    "lead_time": 7.164
  },
  {
    "corpusid": 235606332,
    "section": "1/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.056,
    "text": "Theory Basis\nLargest System (electrons) err ρ KZG [29] iFCI finite element * C 6 H 4 (40) 3.4 × 10 −5 − 8.2 × 10 −5 modified-RKS [48] CASSCF gaussian/cc-pCV5Z ** HCN (14) 5 × 10 −4 SGB [50] MRCI gaussian/cc-pCVTZ ** F 2 (18) 3.8 × 10 −4 − 6.7 × 10 −3 DCEP-MRA [49] HF MRA * C 5 H 5 N 5 (70) 2.8 × 10 −6 − 4.2 × 10 −6 †\n* complete basis ** incomplete basis † the calculation for the target density (ρ **data** ) and the inverse DFT problem are done in the same basis, which allows for better accuracies in the density (see Supplementary Table 1 in [29])",
    "id": 112,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 112,
    "created_at": "2023-03-01T18:41:33.970797Z",
    "updated_at": "2023-03-01T18:41:33.970827Z",
    "lead_time": 4.69
  },
  {
    "corpusid": 119253123,
    "section": "12/32",
    "subsection": 2.0,
    "section_pos_in_pct": 0.375,
    "text": "We correct for leading-track misidentification with a data-driven procedure. Starting from the measured distributions, for each event the track loss due to inefficiency is applied a second time to the **data** (having been applied the first time naturally by the detector) by rejecting tracks randomly. If the leading track is considered reconstructed it is used as before to define the different regions. Otherwise the sub-leading track is used. Since the tracking inefficiency is quite small (about 20%) applying it on the reconstructed **data** a second time does not alter the results significantly. To verify this statement we compared our results with a two step procedure. In this case the inefficiency is applied two times on measured data, half of its value at a time. The correction factor obtained in this way is compatible with the one step procedure. Furthermore, the data-driven procedure has been tested on simulated **data** where the true leading particle is known. We observed a discrepancy between the two methods, especially at low leading-track p T values, which is taken into account in the systematic error. The maximum leading-track misidentification correction is 8% on the final distributions.",
    "id": 113,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 113,
    "created_at": "2023-03-01T18:42:10.314825Z",
    "updated_at": "2023-03-01T18:42:10.314855Z",
    "lead_time": 36.174
  },
  {
    "corpusid": 19033986,
    "section": "14/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "We explored how DM can interpret the excesses in e ± cosmic rays measured by FERMI, PAMELA and HESS. The results of the HESS and FERMI **data** indicate two specific things for any DM interpretation. First, the spectrum becomes steeper at around 1 TeV implying the scale of DM needs to be around a TeV. Second, the FERMI **data** is smooth from 100 GeV to around a TeV implying that a light DM explanation of PAMELA is inconsistent with the FERMI data.",
    "id": 114,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 114,
    "created_at": "2023-03-01T18:42:17.046902Z",
    "updated_at": "2023-03-01T18:42:17.046941Z",
    "lead_time": 6.564
  },
  {
    "corpusid": 245864793,
    "section": "6/42",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "According to the input **data** corresponding to DGM, the matrix of nominal geometry M (0) vn for the 6R CardioVR-ReTone robot is presented in Table 2.   Table 1 and Figure 2, Table 3 highlights the geometrical particularities for the 6R CardioVR-ReTone robot.   \n1 R 0 l 0 l 1 0 1 0 2 R 0 −l 2 l 3 0 0 1 3 R l 4 −l 5 0 0 0 1 4 R 0 l 6 0 0 0 1 5 R l 7 0 −li ∆ i = 1 k i ≡ {x i , y i , z i } 1 y k i ≡ y i = 0 1 0 T 2, 3, 4 z k i ≡ z i = 0 0 1 T 5, 6 x k i ≡ x i = 1 0 0 T For i = 7,\nwhich, in keeping with (8), according to (9), lead to: ",
    "id": 115,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 115,
    "created_at": "2023-03-01T18:42:21.792069Z",
    "updated_at": "2023-03-01T18:42:21.792098Z",
    "lead_time": 4.574
  },
  {
    "corpusid": 207778550,
    "section": "32/48",
    "subsection": 6.0,
    "section_pos_in_pct": 0.667,
    "text": "Also, examples of resolved USU cases were shown in order to highlight how they were turned into a known sources of uncertainty in the past. For instance, three examples were given ( 235 U(n,f) cross sections as a function of time, particle leakage effects in ionization chambers, and 232 Th(n,γ) measurements adversely impacted by water in the sample) where the need to introduce an USU component could be avoided by understanding the physics effect causing the scatter in the data. In these cases, it was recommended to reject **data** that cannot be corrected for this effect rather than increasing uncertainties of the **data** with an introduced uncertainty contribution attributed to USU. This recommendation is based on the fact that the evaluated mean values might be biased by clearly wrong **data** from a physics point of view if the uncertainties are just increased rather than the mean values corrected. These examples also showcase that USU effects can be reduced or even eliminated by studying the physics cause for the discrepancy in the **data** by dedicated experiments that are designed to investigate the possible effects.",
    "id": 116,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 116,
    "created_at": "2023-03-01T18:42:26.376123Z",
    "updated_at": "2023-03-01T18:42:26.376151Z",
    "lead_time": 4.41
  },
  {
    "corpusid": 1520137,
    "section": "5/35",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "The **data** can be fitted by the Bayesian model for the four yes rate curves (two configurations 3 two contextual contrasts) using only four parameters: k, r n , and the prior probabilities P(yes) colinear and P(yes) orthogonal , with each **data** point typically about one error bar size away from the model fit. As expected, P(yes) colinear . P(yes) orthogonal ( Figure 4E). However, both P(yes) colinear and P(yes) orthogonal are quite high. This we believe is the net result of combining two factors, one is the observers' internal prior to reach roughly equal numbers of ''yes'' and ''no'' responses, and the other is the contextual dependent priors from the statistical knowledge of the natural visual environment. Indeed, the average yes rate (over all trials and observers) is 57% 6 2%. The difference between the fitted P(yes) colinear and P(yes) orthogonal reflects the difference between the natural priors that has survived observers' internal prior imposed by the unnatural laboratory experiment.",
    "id": 117,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 117,
    "created_at": "2023-03-01T18:42:30.225798Z",
    "updated_at": "2023-03-01T18:42:30.225824Z",
    "lead_time": 3.681
  },
  {
    "corpusid": 118072666,
    "section": "8/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.615,
    "text": "As ε 2T is obtained from Monte Carlo, differences between **data** and Monte Carlo give rise to systematic uncertainties. Several components contribute to this systematic uncertainty:  • There is a systematic uncertainty related to the extrapolation of the 15-bin yield to the full result, due to our limited knowledge of the φ spectrum in B decays. Since only about 60% of B meson decays are well understood (see Section II), mismodeling of the remaining 40% can affect the result.",
    "id": 118,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 118,
    "created_at": "2023-03-01T18:42:35.742901Z",
    "updated_at": "2023-03-01T18:42:35.742930Z",
    "lead_time": 5.345
  },
  {
    "corpusid": 233705419,
    "section": "21/53",
    "subsection": 2.0,
    "section_pos_in_pct": 0.396,
    "text": "The overview of the model shown in Figure 6 illustrates the **data** set used, the target, and influencing parameters, as well as the quality of the model. ",
    "id": 119,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 119,
    "created_at": "2023-03-01T18:42:37.916348Z",
    "updated_at": "2023-03-01T18:42:37.916383Z",
    "lead_time": 1.954
  },
  {
    "corpusid": 17895596,
    "section": "3/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.273,
    "text": "SKA continuum surveys at low frequencies (< 1 GHz) should be ideal to probe the cosmic radio dipole already in the Early Science phase for two reasons. First, it is not necessary to cover the full area of the 3π surveys, since a sparse sampling spread out over all of the accessible sky should be sufficient for a first estimate. And second, a focus on low frequencies and bright sources will pick primarily AGNs which have a much higher mean redshift than the SFG. Figure 1 illustrates the accuracy that we can hope to achieve for a measurement of the radio dipole based on a linear estimator (Crawford 2009;Rubart & Schwarz 2013). Our estimates are based on differential number counts from surveys in small and deep fields and simulations (Wilman et al. 2008). Our expectations for all-sky continuum surveys are summarized in Table 1. We find that the cosmic radio dipole can be measured at high statistical significance, even taking realistic **data** cuts into account (e.g. masking the galaxy and very bright extragalactic sources, or morphology, spectral index or flux cuts).",
    "id": 120,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 120,
    "created_at": "2023-03-01T18:42:41.496901Z",
    "updated_at": "2023-03-01T18:42:41.496927Z",
    "lead_time": 3.388
  },
  {
    "corpusid": 119419652,
    "section": "11/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.611,
    "text": "Recent developments in analytical methods have introduced jet substructure observables accurate beyond leading-logarithm accuracy. One recent ATLAS publication based on 32.9 fb −1 of √ s = 13 TeV **data** presents a first measurement involving the jet soft-drop mass -one such substructure variable -using dijet events from p-p collisions [38]. The analysis features a reclustering of the constituents of anti−k T jets based on a designated soft-drop mass procedure. This procedure is based on an iterative condition which is governed primarily by the choice of two parameters: z cut and β , which relate to p T and angular thresholds in the context of gluon radiation. The final results correspond to normalized differential cross-section measurements, presented as a function of the logarithm of the dimensionless quantity ρ 2 , where ρ is the ratio of the invariant mass of the jet following the soft-drop procedure and the transverse momentum of the ungroomed jet (ρ = m soft drop /p ungroomed T ). The inclusion of p ungroomed T in the denominator counters an undesireable p T dependence. The distribution shown in Figure 7b, corresponding to the particular choice of β = 0, can be contrasted with other such distributions (for β = 1, 2) in the original publication. An unfolding procedure is used to correct for detector-level effects. The evolution of the distributions with β and z cut highlights particular regions of the quantity log 10 ρ 2 with larger levels of disagreement between **data** and predictions. Such disagreements between the measured **data** and either precise QCD calculations or leading-logorithm Monte Carlo simulations will prove beneficial in improving our understanding of the collinear QCD regime.",
    "id": 121,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 121,
    "created_at": "2023-03-01T18:42:45.962445Z",
    "updated_at": "2023-03-01T18:42:45.962470Z",
    "lead_time": 4.283
  },
  {
    "corpusid": 232380196,
    "section": "23/28",
    "subsection": 2.0,
    "section_pos_in_pct": 0.821,
    "text": "Implementation details for iNaturalist 2018 For iNaturalist 2018, following most of the existing work, we use ResNet-50 [10] as backbone network. The **data** augmentation is similar to that used in long-tailed CIFAR datasets except that random cropping with size 224 × 224 is used. To fit two NVIDIA 2080Ti GPUs, we use a batch size of 100 for both SC and PSC based hybrid networks. The networks are trained for 100 epochs using SGD with momentum 0.9 and weight decay 1 × 10 −4 . The initial learning rate is 0.05, which is decayed by a factor of 10 at epoch 60 and epoch 80. Motivated by the fact that iNaturalist has a large number of classes which can make classifier learning more difficult, we assign higher weighting to the classifier learning branch by using a linearly decayed weighting factor α, i.e., α = 1 − T /T max . The temperature τ is set to be 0.1 for both SC and PSC loss functions. For SC loss function, the number of positive samples for each anchor is fixed to 2.",
    "id": 122,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 122,
    "created_at": "2023-03-01T18:42:48.484849Z",
    "updated_at": "2023-03-01T18:42:48.484907Z",
    "lead_time": 2.303
  },
  {
    "corpusid": 19033986,
    "section": "13/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.464,
    "text": "Similarly, the lack of an edge in the e + + e − spectrum implies that DM that dominantly annihilates or decays into 2e is now firmly excluded, and one can constrain the BR of the subdominant e ± primary channel. Assuming that DM annihilates into leptons with lepton flavor components BR e , BR µ , BR τ that sum to unity, BR e + BR µ + BR τ = 1 one can constrain the allowed regions in light of the PAMELA, FERMI and HESS **data** together with the other constraints. To clarify the situation we show the constraints on the branching fractions for three distinct cases:",
    "id": 123,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 123,
    "created_at": "2023-03-01T18:42:51.444739Z",
    "updated_at": "2023-03-01T18:42:51.444764Z",
    "lead_time": 2.738
  },
  {
    "corpusid": 17671315,
    "section": "3/32",
    "subsection": 2.0,
    "section_pos_in_pct": 0.094,
    "text": "In section 2 we describe the experimental **data** we work with as well as the **data** preprocessing procedure. In section 3 we briefly describe the 1-d version of AGCM. GLK is described in section 4. In section 5 we compare performances of these two methods on our experimental data. We discuss results in section 6.",
    "id": 124,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 124,
    "created_at": "2023-03-01T18:42:54.321889Z",
    "updated_at": "2023-03-01T18:42:54.321918Z",
    "lead_time": 2.687
  },
  {
    "corpusid": 207778550,
    "section": "31/48",
    "subsection": 4.0,
    "section_pos_in_pct": 0.646,
    "text": "In a series of irradiations at Karlsruhe Intitute of Technology (KIT), neutron capture reactions for a 25-keV Maxwell-Boltzmann neutron energy distribution were studied, mainly for reactions that are relevant for s-process nucleosynthesis. AMS **data** obtained from such identical activations are used here, but they were converted into 30-keV MACS values for a direct comparison with TOF data. TOF measurement techniques have been well-established for several decades. In TOF, usually the prompt signature from the de-excitation of the reaction product is measured and used to generate a cross-section value. This approach involves different and also more sophisticated **data** processing procedures compared to AMS. The TOF **data** plotted here are based on measurements at ORELA ( 35 Cl, 40 Ca and 58 Ni) and n TOF/CERN ( 54 Fe and 58 Ni). In the activation reaction studies for AMS, the irradiation setup was designed such that the integrated neutron energy distribution resembled closely a Maxwellian distribution. Thus the measured cross section approximated an energy-averaged value. In TOF the energy-dependent cross sections are folded with the respective neutron flux energy-distribution for the MACS. There are no correlations between these two techniques. We find a systematic deviation between the two methods as seen in Table X. AMS **data** are systematically lower by about 11% compared to TOF **data** for these four reactions. In general, AMS measurements are normalized to reference materials which are independent from each other, and thus there is no correlation in AMS between different reactions. AMS and TOF **data** were both acquired at two different laboratories (Univ. of Vienna and TU Munich, and ORELA and n TOF, respectively). Currently, it is not known what the cause for this systematic deviation could be. The uncertainties of these ratios are generally lower than the observed deviation, leading to a potential estimate of an USU value. For instance, a numerical value for an USU contribution could be assumed to be the minimum deviation that make all measured **data** statistically consistent. If so, this could be estimated to be 5-7%.",
    "id": 125,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 125,
    "created_at": "2023-03-01T18:43:01.614363Z",
    "updated_at": "2023-03-01T18:43:01.614388Z",
    "lead_time": 7.104
  },
  {
    "corpusid": 16212589,
    "section": "3/6",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "Qualitatively, one can see a clear increase in the frequency of such currents from -90 to -85 inV. Just counting events gives a nearly 10-fold increment over this range of potential. A further increment is seen between -85 and -80 mV, but the difference is not as large. When these **data** were quantified with either half amplitude threshold analysis or mean-variance histograms (shown for these **data** in Fig.  2 B), channel open probabilities and open times could be determined. Fig. 3 A shows open probability and open time for these wild-type channels. Consistent with the qualitative estimates described above, the Po curve shows a steep increase in currents between -90 and -85 mV, but then becomes nearly level at more positive potentials. Open times were ~0.1 ms over the whole voltage range, with a gradual increase (roughly e-fold/25 mV) in the range from -90 to -70 inV.",
    "id": 126,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 126,
    "created_at": "2023-03-01T18:43:04.349130Z",
    "updated_at": "2023-03-01T18:43:04.349155Z",
    "lead_time": 2.517
  },
  {
    "corpusid": 250549291,
    "section": "13/55",
    "subsection": 1.0,
    "section_pos_in_pct": 0.236,
    "text": "The grey model, which is constructed from the grey system theory, focuses on the insufficient information available, or the uncertainty of information that is [29]. It has been extensively utilized in the industries of finance, economics and the quantification of construction waste generation [30]. A major advantage of this method is that it can help predict problems with less **data** [29], which is extremely suitable for the current study. As the core of grey theory, the grey model (1,1) is a first-order linear differential equation of a single variable selected to forecast the generation amount of construction waste in Shanghai City in the next 10 years.",
    "id": 127,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 127,
    "created_at": "2023-03-01T18:43:42.494298Z",
    "updated_at": "2023-03-01T18:43:42.494323Z",
    "lead_time": 37.949
  },
  {
    "corpusid": 207778550,
    "section": "13/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.271,
    "text": "If the population standard deviation of the input **data** points (without regard to assigned uncertainties) is noticeably different from the average of the magnitudes of uncertainties assigned to these data, then either the assigned uncertainties are over-estimated or under-estimated. In the latter case, the difference may be attributable to USU. This clue is related to Clue 1.",
    "id": 128,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 128,
    "created_at": "2023-03-01T18:43:45.387349Z",
    "updated_at": "2023-03-01T18:43:45.387377Z",
    "lead_time": 2.68
  },
  {
    "corpusid": 246863515,
    "section": "1/44",
    "subsection": 3.0,
    "section_pos_in_pct": 0.023,
    "text": "A few previous works also consider **data** combination for long-term causal inference, but they impose strong restrictions on the unmeasured confounders. Athey et al. [2019] rely on a surrogate criterion first proposed by Prentice [1989]. This surrogate criterion precludes any unobserved variable that can simultaneously affect the short-term and long-term outcomes (see Section 3.2). Athey et al. [2020] assume a latent confoundedness condition, requiring unobserved confounders to have no direct effects on the long-term outcome but only indirect effects through the short-term outcomes. Neither of these works can handle persistent confounders. By permitting the existence of persistent confounders, our setup is substantially more general than these existing literature.",
    "id": 129,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 129,
    "created_at": "2023-03-01T18:43:50.786483Z",
    "updated_at": "2023-03-01T18:43:50.786515Z",
    "lead_time": 5.204
  },
  {
    "corpusid": 235727288,
    "section": "17/46",
    "subsection": 2.0,
    "section_pos_in_pct": 0.37,
    "text": "In Fig. 7, we report the average BDMC gap on 2500 simulated **data** examples, and observe that q-paths with q = 0.994 or q = 0.996 consistently outperform the geometric path as we vary the number of intermediate distributions K.",
    "id": 130,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 130,
    "created_at": "2023-03-01T18:43:54.267640Z",
    "updated_at": "2023-03-01T18:43:54.267668Z",
    "lead_time": 3.299
  },
  {
    "corpusid": 238834501,
    "section": "1/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.053,
    "text": "Terminology. All discussions below apply to timeseries of **data** from a single sensor, in particular screen level air temperature. We think of such a timeseries as a **data** matrix X dt indexed by a calendar date d (rows; 'date axis') and a time t (columns; 'time axis'). The time period affected by the eclipse will be given the symbol t • ; this includes the full duration of the eclipse (C1-C4), plus an additional time span beyond C4 to account for some dynamical lag in the system (see Table 1). Conversely, all times not affected by the eclipse are symbolised by t • . Analogously, the day of the eclipse will be denoted d • and all other days by d • . When we talk about an average over a number of days, we mean that the averaging is performed over the date dimension and the result is a onedimensional array indexed by times (a daily cycle). All **data** used in this study comes from the meteorological surface station network operated by the Centro de Estudios Avanzados en Zonas Áridas (CEAZA) in La Serena, Chile, from here on called the CEAZAMet station network.",
    "id": 131,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 131,
    "created_at": "2023-03-01T18:43:57.579858Z",
    "updated_at": "2023-03-01T18:43:57.579887Z",
    "lead_time": 3.125
  },
  {
    "corpusid": 117119864,
    "section": "40/83",
    "subsection": 1.0,
    "section_pos_in_pct": 0.482,
    "text": "Application of the Luttinger theorem introduced in section 3.1 implies that, regardless of the route to a FL, if their magnetic moments are quenched then the f-electrons must be counted in the Fermi surface volume [73]. FS studies [74] using magneto-oscillatory techniques such as the de Haas van Alphen (dHvA) effect have verified that this is so. The most straightforward theory [75] for the FS utilizes the LDA band structure, including the f-electrons. A more sophisticated approach includes the f-electrons but renormalizes the ir scattering phase shifts at the FS to be Kondo-like. This procedure gives heavy masses and often makes only a minor perturbation of the original LDA FS [76]. A crucially important theoretical conjecture by Fulde and Zwicknagl [77,78] is that above TK where the f-electron moment is no longer quenched, the f-electrons should now be excluded from the FS volume. This idea is plausible but there is no proof having the rigor of the Luttinger theorem and in any case it would not hold experimentally if in fact Kondo physics were irrelevant in the heavy Fermion materials, as some workers propose. This section presents ARPES **data** verifying the Fulde/Zwicknagl conjecture.",
    "id": 132,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 132,
    "created_at": "2023-03-01T18:44:01.978826Z",
    "updated_at": "2023-03-01T18:44:01.978854Z",
    "lead_time": 4.217
  },
  {
    "corpusid": 85554746,
    "section": "17/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.895,
    "text": "Figure 4 (\n4b) depicts the land-based stations of the WMO's Global Observing System, which is derived from the National Meteorological and Hydrological Services of WMO Members, as well as the SASSCAL Weather Net network in Southern Africa. Overall, although long time series of meteorological **data** are crucial in the context of climate change research, meteorological observations also show a strong bias towards Southern Africa. With regards to oceanic observations, figure 4(c) illustrates that, compared to the Mediterranean Sea, the oceans surrounding the African continent are only sparsely monitored by the global oceanic observation networks identified in this study.\n",
    "id": 133,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 133,
    "created_at": "2023-03-01T18:44:07.227136Z",
    "updated_at": "2023-03-01T18:44:07.227165Z",
    "lead_time": 5.067
  },
  {
    "corpusid": 119253123,
    "section": "19/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.594,
    "text": "We have characterized the Underlying Event in pp collisions at √ s = 0.9 and 7 TeV by measuring the number density, the summed p T distribution and the azimuthal correlation of charged particles with respect to the leading particle. The analysis is based on about 6 · 10 6 minimum bias events at √ s = 0.9 TeV and 25 · 10 6 events at √ s = 7 TeV collected during the **data** taking periods from April to July 2010. Measured **data** have been corrected for detector related effects; in particular we applied a data-driven correction to account for the misidentification of the leading track. The fully corrected final distributions are compared with Pythia 6.4, Pythia 8 and Phojet, showing that pre-LHC tunes have difficulties describing the data. These results are an important ingredient in the required retuning of those generators.",
    "id": 134,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 134,
    "created_at": "2023-03-01T18:44:11.918512Z",
    "updated_at": "2023-03-01T18:44:11.918539Z",
    "lead_time": 4.512
  },
  {
    "corpusid": 2482529,
    "section": "11/37",
    "subsection": 2.0,
    "section_pos_in_pct": 0.297,
    "text": "If an individual relies entirely on instinct and its instinct is correct , then the probability that it will correctly classify all 32 input vectors in the testing phase is : (19) genotype\nd 1 … d 32 s 1 … s 32 , , , , , = bias direction d 1 … d 32 , , = bias strength s 1 … s 32 , , = d i 0 1 , { } ∈ 0 s i 1 ≤ ≤ s i s i d i 1 s i - α i guess g 1 … g 32 , , g = = g i 0 1 , { } ∈ P g i d i = d i α i ≠ ( ) s i = P g i α i = d i α i ≠ ( ) 1 s -i = P g i d i α i = = d i α i = ( ) 1 = s i 0 = α i s i 1 = d i α i ∀ ( ) s i 1 = [ ] i ∀ ( ) d i t i = [ ] 1 p - ( ) 32 i ∀ ( ) s i 1 = [ ] i ∀ ( ) d i t i = [ ] ∧ [ ] P i ∀ ( ) g i β i = [ ] ( ) 1 p - ( ) 32 = [ ] →\nIf an individual relies entirely on learning , then the probability that it will correctly classify all 32 testing vectors is : (20) For convenience, we want our fitness score to range from 0 (low fitness) to 1 (high fitness). To make the problem challenging, we require the individual to correctly guess the class of all 32 testing examples. (This is analogous to Hinton and Nowlan's (1987) requirement that the individual close all 20 switches to get a fitness score above the minimum.) We assign a fitness score of 0 when the guess does not perfectly match the testing **data** and a score of when the match is perfect.",
    "id": 135,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 135,
    "created_at": "2023-03-01T18:44:17.991965Z",
    "updated_at": "2023-03-01T18:44:17.991995Z",
    "lead_time": 5.888
  },
  {
    "corpusid": 40772223,
    "section": "7/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.636,
    "text": "The result for the beam momentum for a nominal settings of 450 GeV/c is given in Table 2. The beam momentum is 449.16 ± 0.14 GeV/c, which is −0.19 ± 0.03% lower than the nominal momentum. The accuracy on the beam momentum of 0.03% is dominated by the systematic effects. The intrinsic accuracy is 0.01−0.02%. A more detailed account of the **data** analysis and of the results is given in Reference [5].",
    "id": 136,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 136,
    "created_at": "2023-03-01T18:44:22.494079Z",
    "updated_at": "2023-03-01T18:44:22.494109Z",
    "lead_time": 4.329
  },
  {
    "corpusid": 17671315,
    "section": "2/32",
    "subsection": 2.0,
    "section_pos_in_pct": 0.062,
    "text": "AGCM was developed for Coefficient Inverse Problems (CIPs) for the hyperbolic equation c (x) u tt = ∆u, x ∈ R n , n = 1, 2, 3 with single measurement data, see, e.g. papers [7,8,9,20,24,25,23] about this method; a summary of results is given in the book [6]. \"Single measurement\" means that the **data** are generated either by a single point source or by a single direction of the incident plane wave. Thus, single measurement means the minimal amount of available information. In particular, the performance of AGCM was verified on 3-d experimental **data** in [8,6,20]. All known numerical methods for n−d, n > 1 CIPs with single measurement **data** require a priori knowledge of a point in a small neighborhood of the exact coefficient. Unlike this, the key advantage of AGCM is that it delivers good approximations for exact solutions of CIPs without any advanced knowledge about small neighborhoods of those solutions. It is well known that this goal is an enormously challenging one.",
    "id": 137,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 137,
    "created_at": "2023-03-01T18:44:26.426895Z",
    "updated_at": "2023-03-01T18:44:26.426925Z",
    "lead_time": 3.756
  },
  {
    "corpusid": 119598666,
    "section": "6/9",
    "subsection": 1.0,
    "section_pos_in_pct": 0.667,
    "text": "The **data** to produce initial conditions a 0 = 1/6, b 0 = 1, x 0 = 1/256 and limiting value lim n→∞ a n = 2 3π .\na 0 − 1 2π ≈ 9.08 × 10 −2 ,a\nAnother option is to use the series [8, Table 6, N = 7] .",
    "id": 138,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 138,
    "created_at": "2023-03-01T18:44:28.555975Z",
    "updated_at": "2023-03-01T18:44:28.556004Z",
    "lead_time": 1.945
  },
  {
    "corpusid": 247627894,
    "section": "14/15",
    "subsection": 1.0,
    "section_pos_in_pct": 0.933,
    "text": "In this example we study the classic \"on-off\" problem [14] of simultaneous measurements of two Poisson processes: p(x 1 , x 2 |µ, ν) = Pois(x 1 |µs + νb)Pois(\nx 2 |ντ b),(10)\nwhere the hyperparameters s, b indicate the nominally expected signal and background counts. The hyperparameter τ describes the relationship in measurement time between the two processes. The signal strength µ is the parameter of interest while the scaling factor for the background ν is a nuisance parameter. This model deviates from a pure Gaussian setup and introduces a more complicated relationship between **data** and parameters. The results are shown in Figure 2. With hyperparameters set at s = 15, b = 70, τ = 1 the model is comfortably in the asymptotic regime but with τ ∼ 1 significant deviation the profiled values of the nuisance parameters is expected as one moves away of the maximum-likelihood estimate. As in the Gaussian case, the neural network directly approximates the profile likelihood ratio to very high degree of precision.",
    "id": 139,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 139,
    "created_at": "2023-03-01T18:44:32.207119Z",
    "updated_at": "2023-03-01T18:44:32.207151Z",
    "lead_time": 3.478
  },
  {
    "corpusid": 237649489,
    "section": "5/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.102,
    "text": "The CA-Markov model is composed of CA model, Markov chain and multi-criteria evaluation (MCE) [29]. The CA model is a discrete, finite state composition of the meta-cell model. It can simulate complex dynamic systems with spatial-temporal characteristics according to certain local rules [43]. Markov chains create the transfer matrix and probability between land use types for multiple time periods in the past through spatial comparison analysis, which are the basic **data** for predicting future land use patterns. MCE refers to the selection of expansion factors to construct a land use transition suitability image collection. CA-Markov model can effectively predict future land use dynamics [44]. The prediction process equation of the CA-Markov model is shown below [45]:\nCtj + 1 = F[Ctj, N](5)\nwhere C(t j ) and C(t j+1 ) are the states of the cell at time t j and t j+1 , respectively; F is the transition rule; N is the domain of the cellular. In this study, elevation, slope, earthquake, landslide, highway, main road, railroad and population density were selected as the driving factors affecting urban expansion. Among them, elevation, slope, earthquake and landslide are negative indicators, and the rest are positive indicators. The suitability evaluation maps for earthquakes, landslides, highways, main roads and railroads were calculated by the kernel density tool, and finally, the normalized driver maps were obtained ( Figure 5). The Kappa coefficient was applied to check the accuracy of the CA-Markov model simulation results [46]. The formula for calculating the Kappa coefficient is shown below:\nKappa = p a − p c 1 − p c (6) p a = s n (7) p c = a 1 * b 1 + a 2 * b 2 n * n(8)\nwhere n is the total number of cell sizes in the raster; a 1 is the number of cell sizes in the real raster of the urban land; a 2 is the number of cell sizes in the non-urban land; b 1 is the number of cell sizes in the simulated raster of the urban land; b 2 is the number of cell sizes in the non-urban land; s is the number of cell sizes in the real raster and the simulated raster that correspond to each other. ",
    "id": 140,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 140,
    "created_at": "2023-03-01T18:44:40.734373Z",
    "updated_at": "2023-03-01T18:44:40.734404Z",
    "lead_time": 8.349
  },
  {
    "corpusid": 207778550,
    "section": "32/48",
    "subsection": 7.0,
    "section_pos_in_pct": 0.667,
    "text": "One example of USU effects, formulated by considering measurements of neutron-induced fission cross-section ratios of 238 U/ 235 U, highlighted the point that different USU estimation approaches result in slightly different but yet consistent evaluated mean values and uncertainties. Under/overestimation of reported uncertainties have been found. This example illustrated the subjective nature of estimating the effect of USU. However, this subjectivity does not apply only to estimating USU effects but also holds true for evaluating mean values and covariances in general. After all, these derived values depend on the evaluation techniques utilized, input **data** selected and subjective choices made on the parameters of models, corrections of experimental **data** and on methods for estimating covariances of both, model and experiment. Hence, our evaluated results are always subjective-with and without consideration of USU. However, if one neglects adding obviously necessary contributions from USU to uncertainties of input data, the evaluated mean values might be more biased and the evaluated uncertainties will be underestimated, in turn, adversely impacting application calculations.",
    "id": 141,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 141,
    "created_at": "2023-03-01T18:44:52.255046Z",
    "updated_at": "2023-03-01T18:44:52.255075Z",
    "lead_time": 11.335
  },
  {
    "corpusid": 52995678,
    "section": "14/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.483,
    "text": "In reduced order modeling, the basis is generally built from Eulerian data, e.g., velocity, vorticity, or streamfunction. Thus, ROMs generally are Eulerian ROMs. Of course, these Eulerian ROMs can efficiently approximate the corresponding Eulerian fields. However, Lagrangian fields (e.g., the FTLE field) might be better approximated by Lagrangian ROMs, i.e., ROMs whose bases are built from both Eulerian and Lagrangian data. The new α-ROM and λ-ROM are Lagrangian ROMs, since the FTLE weighted inner products (23) and (25) use both Eulerian **data** (i.e., the vorticity ω) and Lagrangian **data** (i.e., the FTLE λ). The input Eulerian **data** (i.e., the vorticity field) help the resulting FTLE-ROMs yield an accurate approximation of the output Eulerian **data** (i.e., the vorticity, streamfunction, and velocity fields computed by the FTLE-ROM). On the other hand, the input Lagrangian **data** (i.e., the FTLE field) \"nudge\" the FTLE-ROMs toward an accurate approximation of the output Lagrangian **data** (i.e., the FTLE field obtained from the FTLE-ROM velocity field). Thus, we expect the new α-ROM and λ-ROM to yield more accurate FTLE approximations than the standard G-ROM (17). In Section 5, we investigate the new α-ROM and λ-ROM, as well as the standard G-ROM, in the FTLE computation of a test problem that uses the QGE as mathematical model.",
    "id": 142,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 142,
    "created_at": "2023-03-01T18:45:07.458424Z",
    "updated_at": "2023-03-01T18:45:07.458457Z",
    "lead_time": 15.039
  },
  {
    "corpusid": 251953240,
    "section": "39/41",
    "subsection": 1.0,
    "section_pos_in_pct": 0.951,
    "text": "Table 5\n5shows the results of the most performing network concatenation configurations. From table 5, there is an overall improvement of segmentation performance contributed by all blocks. Comparing with the original SegNet, the PDLF-Net shows improvement by an increase of 2.1% acc, 4.11% IoU, 2.90% dice, 2.96% sens, 3.15% prec and 2.96% spec on weakly visible **data** class 1 (DC1). 1.51% acc, 4.28% IoU, 3.33% dice, 3.08% sens, 0.29% prec and 3.20% spec on DC2.\n",
    "id": 143,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 143,
    "created_at": "2023-03-01T18:45:10.779272Z",
    "updated_at": "2023-03-01T18:45:10.779311Z",
    "lead_time": 3.13
  },
  {
    "corpusid": 207778550,
    "section": "21/48",
    "subsection": 3.0,
    "section_pos_in_pct": 0.438,
    "text": "Both 235 U(n,f) and 238 U(n,f) cross sections are Neutron Standards in a broad energy region that includes neutron energies from 7.5 MeV up to 15 MeV. One of the most-often measured quantities is the ratio of the neutroninduced fission cross sections of 238 U to 235 U. Such ratio **data** in the region of interest, which will be denoted by R 8/5 , are weakly energy-dependent quantities. Their uncertainties, including possible components due to USU, are also expected to depend on energy. In this section the presence of USU for fission cross-section ratio **data** is investigated for neutron energies from 7.5 MeV up to 15 MeV. The **data** were taken from the Neutron Standards database, i.e., from the GMAP input database, covering the energy range of interest. The eleven **data** sets 5 that were considered are listed in Table XI of Appendix VI A, and are shown in Fig. 4. Note that for the exercise presented in this work all **data** sets were assumed to result from absolute ratio measurements.",
    "id": 144,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 144,
    "created_at": "2023-03-01T18:45:20.804492Z",
    "updated_at": "2023-03-01T18:45:20.804518Z",
    "lead_time": 9.847
  },
  {
    "corpusid": 17895596,
    "section": "4/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.364,
    "text": "SKA will provide a deep and wide large-scale structure dataset that will enable separating the effects of the early and late universe on the observed CMB anisotropy. For example, the SKA **data** could be used to reconstruct the late-time contribution to the CMB anisotropy via the integrated Sachs-Wolfe effect, and thus provide information about the temporal evolution of the CMB anomalies.",
    "id": 145,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 145,
    "created_at": "2023-03-01T18:45:26.453608Z",
    "updated_at": "2023-03-01T18:45:26.453642Z",
    "lead_time": 5.469
  },
  {
    "corpusid": 117119864,
    "section": "79/83",
    "subsection": 2.0,
    "section_pos_in_pct": 0.952,
    "text": "\nFermi surface map for T = 120K >> TK = 20K of CeRu2Si2 compared to ARPES map of LaRu2Si2, showing same size of large hole surface contour for both compounds. Absence of reduced hole surface size due to Ce 4f electron inclusion, as predicted by LDA and observed at very low T in dHvA experiments, is evidence of Ce 4f electron exclusion from Fermi surface at T >> TK, as proposed in Refs.[77,78].[from Ref. 18]. 5. Summary of below (102 eV) and above (108 eV and 112 eV) resonance ARPES **data** in panels (a)-(c) for heavy Fermion material URu2Si2, showing confinement of 5f weight (panel (b)) to interior of Ru d-band hole pocket (panel (c)), as in simple 2-band ansatz for the Anderson lattice model where a Kondo-renormalized f-state at εf′ hybridizes to the d-band, as shown in panels (d) and (e). [from Ref. 18] 6. Bulk sensitive PES spectra taken at photon energy 500 eV, showing V 3d valence band in all three phases of MI transition material (V0.982Cr0.018)2O3. Paramagnetic metal (PM) phase weight near EF could be quasi-particle weight transferred from Hubbard peaks of the antiferromagnetic insulating (AFI) or paramagnetic insulating (PI) phases, as in the dynamic mean field theory [91, 106] of the MI transition. [unpublished **data** from collaboration of Ref. 102]. 7. (a) schematic band structure of quasi-1-d metal Li0.9Mo6O17; (b) ARPES spectra showing all four bands A -D; (c) overlaid plot of ARPES spectra for comparison of band C lineshape to spectral theory [Ref. 126] of Tomonaga-Luttinger model in panel (d), showing lack of Fermi liquid quasi-particle due to Luttinger-liquid-like electron fractionalization, presently the only such ARPES example. [from Refs. 135 and 150].\n",
    "id": 146,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 146,
    "created_at": "2023-03-01T18:45:35.452271Z",
    "updated_at": "2023-03-01T18:45:35.452298Z",
    "lead_time": 8.82
  },
  {
    "corpusid": 233705419,
    "section": "27/53",
    "subsection": 1.0,
    "section_pos_in_pct": 0.509,
    "text": "With the EDI hive IoT framework, an efficient test design and evaluation of the **data** guaranteed that all influencing parameters and the interactions between them were presented and evaluated. In this case, the AI-based hybrid model was used to examine the reduction potential of oscillating the coal mass flow to reduce NO x emissions. The campaign consisted of a screening trial, where the **data** were used to identify relevant influencing parameters. Based on these results, a second trial was initiated to further investigate the optimal test settings and the reproducibility of the test setup. The formalized expert knowledge, measurement data, and the decisions for all iterations were captured sustainably through the AI-based hybrid model procedure, which is supported by the EDI hive. With the statistical planning of the trial and the use of statistical analysis algorithms, it was possible to identify significant influences, such as the best oscillation frequency. In this case, it was found to be 1.8 Hz for coal oscillation to reduce the NO x concentration to 380 mg/Nm 3 .",
    "id": 147,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 147,
    "created_at": "2023-03-01T18:45:44.223383Z",
    "updated_at": "2023-03-01T18:45:44.223416Z",
    "lead_time": 8.593
  },
  {
    "corpusid": 119247546,
    "section": "37/46",
    "subsection": 2.0,
    "section_pos_in_pct": 0.804,
    "text": "Using the equation for and the initial **data** (0) = 1 one sees that (0) > 0 so that has a local minimum atτ = 0. Thus, at least for positive values ofτ close to 0 one has that must be increasing. Furthermore, asχ > 0 for r > r , one finds that β 2 ≤ . This last differential inequality can be integrated to yield ≥ cosh(βτ ). One concludes that is increasing for all τ ≥ 0. A similar argument with the function η ≡ − ς satisfying the equation\nη − (β 2 +χ)η = 1, η(0) = 0, η (0) = 0,\nshows that ≥ ς for allτ ≥ 0.",
    "id": 148,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 148,
    "created_at": "2023-03-01T18:45:47.856633Z",
    "updated_at": "2023-03-01T18:45:53.248680Z",
    "lead_time": 6.719
  },
  {
    "corpusid": 19033986,
    "section": "23/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.821,
    "text": "Figures 6 ,\n67 and 10 show the best-fit regions in the (M, σv ) plane for DM annihilations and in the (M, τ ) plane for DM decays. Each panel assumes a specific mode and a specific DM profile. In each panel the red regions are favored by the global fit of FERMI, HESS and PAMELA **data** at 3 and 5σ (2 dof).\n",
    "id": 149,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 149,
    "created_at": "2023-03-01T18:46:14.348530Z",
    "updated_at": "2023-03-01T18:46:14.348562Z",
    "lead_time": 11.626
  },
  {
    "corpusid": 246165803,
    "section": "23/45",
    "subsection": 2.0,
    "section_pos_in_pct": 0.511,
    "text": "The other measurement was the F1 score, which measures complete accuracy using both precision and recall. The score reaches its best value at one and its worst at 0. The precision refers to the number of true positives per true positive and false positive, while the recall measures the number of true positives per true positives and false negatives.\nPrecision = TruePositives TruePositives + FalsePositives Recall = TruePositives TruePositives + FalseNegatives F1score = 2 × Precision × Recall Precision + Recall\nThe results on the test **data** were coherent with the results of the training. For example, the KNN algorithm scored 97.80%, while the ANN scored 93.33%. To visualize the presented data, we made 3D scatter plots with the labeled test data. Figure 12 shows both scatter plots, with the mean power of the frequencies for the three axes of the sensor data. The label is the same as the confusion matrices, with four different colors. It can be seen in the 2D plot of a particular case in Figure 9. Both results have a complete similarity, and the types of anomalies that differ the most within these visualizations are speed bumps and good road data. The dataset was randomized from the rule of 85% for training and 15% test; thus, each algorithm received different test datapoints. That is why the points are different in each scatter plot, but we can correctly observe the color classification of the output of each algorithm. ",
    "id": 150,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 150,
    "created_at": "2023-03-01T18:46:22.961857Z",
    "updated_at": "2023-03-01T18:46:22.961889Z",
    "lead_time": 8.385
  },
  {
    "corpusid": 52995678,
    "section": "10/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.345,
    "text": "In this section, we propose new ROMs for the FTLE computation. In order to construct the ROM modes, the new ROMs use not only Eulerian **data** (vorticity field) as the standard G-ROM (17) ",
    "id": 151,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 151,
    "created_at": "2023-03-01T18:46:25.780990Z",
    "updated_at": "2023-03-01T18:46:25.781024Z",
    "lead_time": 2.619
  },
  {
    "corpusid": 251719245,
    "section": "6/21",
    "subsection": 1.0,
    "section_pos_in_pct": 0.286,
    "text": "The standard approach for partitioning a **data** set is through clustering. The works of Ruan et al. [11] have proven that for some **data** sets, community detection in graphs provides more accurate partitions. In line with this, we use 4 different community detection algorithms to predict the grouping of genes based on the 5 functional groups. The first three algorithms use the concept of modularity while the last algorithm uses edge-betweenness. We'll discuss these two network metrics in the succeeding subsections. We subjected the largest component of each graph obtained by using value-based and rank-based construction with varying input parameters. Here, we will compare how the different community detection algorithms perform in predicting the 5 functional groups.",
    "id": 152,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 152,
    "created_at": "2023-03-01T18:46:32.919582Z",
    "updated_at": "2023-03-01T18:46:32.919612Z",
    "lead_time": 6.943
  },
  {
    "corpusid": 248366486,
    "section": "8/24",
    "subsection": 1.0,
    "section_pos_in_pct": 0.333,
    "text": "Results for the average front position and front roughness are provided in Fig. 12. The time evolution of the average front position turns out to be of the expected power-law form, h (t) ∼ t δ , with δ 0.58. Hence, the average velocity of the front decays with time, as expected due to the time-decreasing KPZ coupling in Eq. (16). However, the amplitude of front fluctuations does increase with time as indicated by the powerlaw behavior of the roughness, w(t) ∼ t β with β 0.35, seen in Fig. 12. For reference, recall that for the 1D KPZ universality class one has β KPZ = 1/3 [15][16][17]. However, we believe closeness of the growth exponent to the KPZ value is coincidental, as it does not occur for other exponents. This is confirmed by Fig. 13, which shows the structure factor S(k, t) for different times. Note that the large-k behavior of the various curves is time-independent and exhibits no anomalous shift with time, in contrast with the kMC results shown e.g. in Fig. 9. Hence, scaling behavior is standard Family-Vicsek type for Eq. (16), as further confirmed by the **data** collapse performed in Fig. 13(b) according to Eq. (12), whereby the u-independent behavior obtained for large u in the numerical scaling function implies α = α loc 1. Hence, neither α = 1 nor the z 2.65 value implied by the collapse, are anywhere close to the 1D KPZ values (α KPZ = 1/2 and z KPZ = 3/2), in spite of the fact that their ratio β = α/z 0.37 is not so far from β KPZ .",
    "id": 153,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 153,
    "created_at": "2023-03-01T18:46:38.902386Z",
    "updated_at": "2023-03-01T18:46:38.902413Z",
    "lead_time": 5.798
  },
  {
    "corpusid": 15394341,
    "section": "13/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.722,
    "text": "Figure S9 :\nS9CC and Jaccard similarity for two similar earthquakes. (A) Two similar normalized earthquake waveforms, from 1266.95 s (blue) and 1629 s (red) in the CCOB.EHN continuous **data** on 2011-01-08, have a high CC = 0.9808 (Eq. 1), overlapping almost perfectly. (B) Corresponding fingerprints of these waveforms also have high overlap, with a high Jaccard similarity of 0.7544 (Eq. 5). White: both fingerprints are 1; red: one fingerprint is 1 and the other is 0; black: both fingerprints are 0. Here the Jaccard similarity J(A, B) equals the number of white elements (where both fingerprints A and B are 1) divided by the number of white elements plus the number of red elements (where either A or B is 1, but not both).\n",
    "id": 154,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 154,
    "created_at": "2023-03-01T18:46:45.336844Z",
    "updated_at": "2023-03-01T18:46:45.336870Z",
    "lead_time": 6.245
  },
  {
    "corpusid": 6811986,
    "section": "18/20",
    "subsection": 1.0,
    "section_pos_in_pct": 0.9,
    "text": "All the complexity upper bounds for the instance-level problem carry over straightforwardly using the simple approach of substituting in each potential output a tuple from V and utilizing the prior algorithms on the resulting Boolean queries. The complexity for each substitution preserves the upper bounds since they hold in the presence of constants, and the iteration over tuples can be absorbed in the complexity classes given in our upper bounds: for **data** complexity the iteration is polynomial, while for combined complexity the number of tuples can be exponential, but our bounds are at least exponential. Further, GFP-Datalog definability for negative implications also extends straightforwardly to the non-Boolean case: Theorem 27 extends with the same statement and proof, while the argument in Theorem 28 is easily extended to show that there is a GFP-Datalog program that returns the complement of NQI(Q, C, S) within the active domain.",
    "id": 155,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 155,
    "created_at": "2023-03-01T18:46:49.584622Z",
    "updated_at": "2023-03-01T18:46:49.584647Z",
    "lead_time": 4.06
  },
  {
    "corpusid": 17671315,
    "section": "20/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.625,
    "text": "Step 1. Select any target X listed in Table 1. We call X \"calibration target\". Let F X (t) be the **data** for the target X, which are pre-processed as in subsection 2.3.1, see Fig. 5.",
    "id": 156,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 156,
    "created_at": "2023-03-01T18:46:54.303849Z",
    "updated_at": "2023-03-01T18:46:54.303887Z",
    "lead_time": 4.53
  },
  {
    "corpusid": 117751701,
    "section": "1/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.077,
    "text": "The Jacobi operator J in the Hilbert space l 2 (N) is the operator whose matrix representation with respect to the canonical basis in l 2 (N) is a semi-infinite Jacobi matrix of the form \n       q 1 b 1 0 0 · · · b 1 q 2 b 2 0 · · · 0 b 2 q 3 b 3 0 0 b 3 q 4 . . . . . . . . . . . . . . .         ,(1.1)\nwhere q n ∈ R and b n > 0 for any n ∈ N (see in [2] the definition of the matrix representation of an unbounded symmetric operator). J is closed by definition and it may be self-adjoint or have deficiency indices (1,1). In this work we deal with self-adjoint operators, so, if J = J * , we consider its self-adjoint extensions denoted J (g) , where g ∈ R ∪ {∞} (see Definition 1 a)). If J = J * we assume J (g) = J for all g ∈ R ∪ {∞} (see Definition 1 b)). The two spectra inverse problem for Jacobi operators J (g) takes as input **data** the spectra of two operators in a operator family obtained by perturbing J (g) in a certain way. The solution of the problem is the finding of the matrix (1.1) and the \"boundary condition at infinity\" g if necessary. The case of the operator family consisting of rank-one perturbations of a self-adjoint Jacobi operator has been amply studied in [8,12,13] and, in the more general setting of rank-one perturbations of J (g) , in [19,23]. Rank-one perturbations can be seen as a change of the \"boundary condition at the origin\" for the corresponding difference equation (see [19,Appendix]). We remark that the case of finite Jacobi matrices has also been thoroughly studied (see [5,6,9,11,14]).",
    "id": 157,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 157,
    "created_at": "2023-03-01T18:47:00.548207Z",
    "updated_at": "2023-03-01T18:47:00.548232Z",
    "lead_time": 6.019
  },
  {
    "corpusid": 119383949,
    "section": "1/4",
    "subsection": 2.0,
    "section_pos_in_pct": 0.25,
    "text": "Since the orbit bifurcations are of leading order inh, we now want to check if they have an increased influence on the amplitude of the conductance oscillations. In Fig. 3(a) we show the quantity Tr M of four typical periodic orbits (shown to the right) taking part in two successive bifurcations (where Tr M = 2) under variation of the magnetic field strength B. The left one is a tangent bifurcation, the right one a pitchfork bifurcation. In Fig. 3(b), the contribution of these orbits to the conductance is plotted. The dotted line gives the result of the trace formula Eq. (1). The amplitudes are diverging (arrows!) at the bifurcations. The uniform approximation (solid line) removes the divergences. Fig. 3(c) represents the corresponding **data** for a system scaled to have 10 times larger actions, thus being closer to the semiclassical limit. It is important now to note that the amplitudes in the uniform approximation are nearly constant over the bifurcations. We have thus shown that the bifurcations have no locally dominant influence on the conductance of the present system [23]. Having established this result, we can further simplify our semiclassical treatment. Whereas for individual orbits a uniform treatment of the bifurcations is vital, their influence becomes smaller if a larger number of orbits is included. This is demonstrated in Fig. 4, where the total δG xx has been calculated for s d = 1.86 including all relevant (∼ 60) periodic orbits. The solid line shows the result of the uniform approximation, whereas the dotted line corresponds to the standard Gutzwiller approach. To remove the spurious divergences in Eq. (1) (and those due to bifurcations of higher codimension in the uniform approach), we have additionally convoluted δG xx over the magnetic field B (cf. Ref. [24]). The results are very similar [25]. In particular, the maximum positions are practically identical. In the following, we therefore use simply Eq. (1) with an additional convolution over B.",
    "id": 158,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 158,
    "created_at": "2023-03-01T18:47:07.336373Z",
    "updated_at": "2023-03-01T18:47:07.336417Z",
    "lead_time": 6.561
  },
  {
    "corpusid": 55185767,
    "section": "7/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.438,
    "text": "Let us now treat the case that Y is VG, using terminology and results from Section 5. Let us first suppose that the definable functions which take values in VG and which appear in the build-up of f (namely in the forms of generators (2) and (3) of Section 3.1) are linear over X, that W is the definable set\n(4.1.2) {(x, y) ∈ X × VG | α(x) ≤ y β(x)}\nwhere is < or no condition and where α, β : X → VG are definable functions, and that all other build-up **data** of f (namely, generators (4) of Section 3.1 and h and e as in (3.2.1)) factor through the projection W → X. Then the conclusion follows from Lemma 4.1.2. Indeed, for any K in Loc 0 , any x ∈ X K and any ψ in D K , f K,ψ (x, y) is a finite sum of terms T i of the form\n(4.1.3) c i,K,ψ (x)y a i q b i y K\nfor integers a i ≥ 0, rational numbers b i and c i in C exp (X). The integrability of f K,ψ (x, y) over y in W K,x is automatic when is < and we get g from Lemma 4.1.2. When is no condition, we regroup the terms if necessary, so that the pairs (a i , b i ) are mutually different for different i. By observing different asymptotic behavior of these terms for growing y, we may consider the subsum i∈J T i with i ∈ J if b i < 0. This time we apply Lemma 4.1.2 to this sub-sum to find g.",
    "id": 159,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 159,
    "created_at": "2023-03-01T18:47:11.550785Z",
    "updated_at": "2023-03-01T18:47:11.550810Z",
    "lead_time": 3.995
  },
  {
    "corpusid": 119351763,
    "section": "10/10",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "For every rational value of the reduced magnetization of the array, m = µ /µ < 1, (ignoring sign) there exist very numerous possible arrangements of up and down dots, one (or more) of which must have minimum dipolar interaction energy W m . We have calculated that energy for a range of excellent candidates for these ground states at various values of m, in particular those of the chessboard AFM state of m = 0 and the uniform FM state with m = 1. We suggest and argue that, for the sequence of optimal configurations, W m and dW m /dm increase monotonically with m, and this is supported by our **data** in Fig. 4. (dW m /dm is likely to be locally discontinuous). An analytic formula for W m (m) is derived on the assumption that for m → 1, the minority down dots are located near points on an equilateral triangular super-lattice. A similar formula is derived for m → 0 relating to the excess up dots. Remarkably, these formulae agree at m = 1/2 (though their gradients differ) and fit the **data** for specific states very well, especially near m = 0 and 1. A similar formula involving a square super-lattice instead of the triangular super-lattice gives results in precise agreement with the **data** for specific structures with m = 0, 0.6 and 0.8, these being those structures in which the minority dots do indeed lie on square super-lattices. It differs from the first asymptotic formula by little more than 1.11% over the appropriate range 0.5 ≤ m ≤ 1.",
    "id": 160,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 160,
    "created_at": "2023-03-01T18:47:16.367641Z",
    "updated_at": "2023-03-01T18:47:16.367670Z",
    "lead_time": 4.622
  },
  {
    "corpusid": 55747635,
    "section": "1/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "To proceed with their dynamics, one needs to know all sorts of effective actions where various potentially interesting references are given in [8]. We would like to take advantage of Conformal Field Theory (CFT) and try to release more information about the structures of the BPS effective actions. Indeed one of our aims is to work out with CFT to get more **data** and increase our knowledge of deriving various string theory couplings and likewise various techniques to Effective Field Theory (EFT) couplings along the way can also be explored.",
    "id": 161,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 161,
    "created_at": "2023-03-01T18:47:21.462338Z",
    "updated_at": "2023-03-01T18:47:21.462364Z",
    "lead_time": 4.904
  },
  {
    "corpusid": 253553223,
    "section": "1/28",
    "subsection": 2.0,
    "section_pos_in_pct": 0.036,
    "text": "The remainder of this paper is structured as follows. Section 2 gives the problem setting and fixes some basic notation for the rest of the paper. Section 3 formally introduces the precomposition operator, investigates its mathematical properties and its establishes its connection to the infinite-dimensional regression problem. In Section 4, we lay the groundwork for a detailed statistical analysis of regression with Hilbert-Schmidt operators by combining spectral regularisation techniques with concentration bounds for sub-exponential Hilbertian random variables. In particular, we derive basic rates under the assumption of Hölder-type source conditions. Section 5 discusses the consequences of our results for specific statistical fields and connects them to the relevant literature. Section 6 recapitulates the key points of our work and their implications in the abstract context of learning infinite-dimensional information from **data** by comparing the infinite-dimensional response setting to standard regression with real-valued responses.",
    "id": 162,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 162,
    "created_at": "2023-03-01T18:47:26.642375Z",
    "updated_at": "2023-03-01T18:47:26.642401Z",
    "lead_time": 4.888
  },
  {
    "corpusid": 6811986,
    "section": "9/20",
    "subsection": 3.0,
    "section_pos_in_pct": 0.45,
    "text": "The **data** complexity bound in Theorem 5 is tight even for inclusion dependencies. The proof, proceeds by showing that a \"universal machine\" for alternating PSpace can be constructed by fixing appropriate Q, C, S in a PQI problem.",
    "id": 163,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 163,
    "created_at": "2023-03-01T18:47:29.405757Z",
    "updated_at": "2023-03-01T18:47:29.405788Z",
    "lead_time": 2.574
  },
  {
    "corpusid": 118649874,
    "section": "1/22",
    "subsection": 1.0,
    "section_pos_in_pct": 0.045,
    "text": "Although all of these LEP2 and Tevatron measurements are in agreement with Standard Model predictions, it is generally considered that should non-Standard Model physics play a role in this process, its effects will be manifest greater at greater energy scales. In particular, the effects of anomalous W W γ and W W Z triple gauge boson couplings, are predicted to grow with the invariant mass of the gauge boson pair [16]. Thus, even though fewer events will be recorded, the high energy **data** still being accumulated by CDF and D0 combined, has the potential to surpass the sensitivity of the LEP2 **data** to such effects. At the LHC the inclusive cross section for vector boson pair production is around a factor of ten higher than at the Tevatron, moreover, the luminosity is also substantially higher. It is then expected that in the near future very large quantities of high energy weak boson pairs will be recorded by the LHC experiments and that the non-Abelian nature of the weak bosons will be tested with a new level of vigor. To facilitate such precision tests theoretical predictions, ideally in the form of fully exclusive Monte Carlo simulations, should be made as accurate as possible.",
    "id": 164,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 164,
    "created_at": "2023-03-01T18:47:34.076881Z",
    "updated_at": "2023-03-01T18:47:34.076909Z",
    "lead_time": 4.485
  },
  {
    "corpusid": 238249036,
    "section": "2/5",
    "subsection": 1.0,
    "section_pos_in_pct": 0.4,
    "text": "FIG. 2 .\n2Neutron multiplicity dependence of (upper) hα core i and (lower) hm μμ i of μ þ μ − pairs in ultraperipheral Pb-Pb collisions at ffiffiffiffiffiffiffiffi s NN p ¼ 5.02 TeV. The vertical lines on **data** points depict the statistical uncertainties, while the systematic uncertainties of the **data** are shown as shaded areas. The dot-dashed line shows the STARLIGHT prediction, and the dashed line corresponds to the leading-order QED calculation of Ref.[48].\n",
    "id": 165,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 165,
    "created_at": "2023-03-01T18:47:37.401023Z",
    "updated_at": "2023-03-01T18:47:37.401052Z",
    "lead_time": 3.139
  },
  {
    "corpusid": 2423505,
    "section": "8/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.348,
    "text": "Covariance shift [Shimodaira, 2000], is another relaxation of DA. Here, given an observation, the class distributions are same in the source and target domains, but the marginal **data** distributions are different. P t (Y |X) = P s (Y |X), but P t (X) = P s (X). This situation arise, for example, in active learning, where the P s (X) tend to be biased to lie near the margin of the classifier. At a first glance, this situation appears not to present a problem, since P t (Y |X) = P s (Y |X), which we can estimate from the data. Here is why it becomes a problem in practice. Assuming, first of all, that the model family we use is mismatched to the data, i.e. regardless of what parameter we choose the model won't fit the underlying distribution. Under this assumption, covariate shift becomes a problem for the following reason. The optimal fit of the source **data** will be such that it minimize model error in the dense area of P s (X) (because these areas will dominate the error). Now, since P t (X) is different from P s (X), the learned model will not be optimal for the target **data** (again, since the model family is mismatched).",
    "id": 166,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 166,
    "created_at": "2023-03-01T18:47:43.798227Z",
    "updated_at": "2023-03-01T18:47:43.798256Z",
    "lead_time": 6.205
  },
  {
    "corpusid": 207778550,
    "section": "20/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.417,
    "text": "In the following discussion, it is assumed that the evaluation methods employed are based on the least-squares approach, e.g., as embodied in code GMAP [49]. The estimated USU contributions, when significant, need to be introduced as components of augmented covariance matrices associated with all the input experimental data. It has been shown by Capote and Neudecker [78] that when more than a one-dimensional quantity is to be evaluated (e.g., energy-dependent cross sections), the evaluated mean values, as well as the derived covariances, will be altered by the use of augmented covariance matrices that include the USU contributions. This is intuitively evident because a change in the inclusive **data** covariance matrix changes the effective weighings of the input **data** points and this will alter the evaluated mean values. Therefore, a-priori determined USU contribution (e.g., as done for a one-dimensional quantity 252 Cf(sf) ν tot , see Sect. V A below) cannot be added a-posteriori to evaluated uncertainties while keeping the originally evaluated mean values. Instead, the evaluations need to be repeated including these augmented covariance matrices that contain an USU component in the experimental covariance matrix.",
    "id": 167,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 167,
    "created_at": "2023-03-01T18:47:49.742746Z",
    "updated_at": "2023-03-01T18:47:49.742776Z",
    "lead_time": 5.76
  },
  {
    "corpusid": 118072666,
    "section": "7/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.538,
    "text": "The average multiplicity is calculated with the formula\nB(B → φX) = 1 2N BB B(φ → K + K − ) 16 i=1 N φ B,i ε i ,(1)\nwhere N φ B,i is the number of φ mesons in momentum bin i found in the **data** and assumed to come from B mesons. This number is obtained by performing the background fit to the on-resonance **data** samples after subtracting the off-resonance **data** samples, scaled to the on-resonance luminosity. The efficiency ε = ε 2T ε K + ε K − is the product of the reconstruction efficiency and the kaon identification efficiencies for each track. The quantity N BB is the number of BB events in the **data** sample, which is measured to be N BB = (18.7 ± 0.2) × 10 6 using a technique described elsewhere [10].",
    "id": 168,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 168,
    "created_at": "2023-03-01T18:47:59.006747Z",
    "updated_at": "2023-03-01T18:47:59.006773Z",
    "lead_time": 9.085
  },
  {
    "corpusid": 87345260,
    "section": "6/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.462,
    "text": "Given a **data** set and n random perturbations on this data, if the index is robust, all (or most) perturbations would yield the same general ranking. Therefore, in the context of conservation in an optimal situation, we would prefer areas that:",
    "id": 169,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 169,
    "created_at": "2023-03-01T18:48:04.200650Z",
    "updated_at": "2023-03-01T18:48:04.200675Z",
    "lead_time": 4.96
  },
  {
    "corpusid": 119313773,
    "section": "1/21",
    "subsection": 2.0,
    "section_pos_in_pct": 0.048,
    "text": "The paper is organized as follows. In Section 2 we briefly review linear response theory and the fluctuation-dissipation theorem. In Section 3 we propose a goodness-of-fit test to probe for the validity of linear response in time series. In Section 4 we discuss the logistic map, demonstrate the mechanism leading to the breakdown of linear response for this one-dimensional map and show how this breakdown might not be apparent with time series of insufficient length. We show the effect of finite **data** size as well as how the choice of the observable can either mask or emphasize the non-smoothness of the invariant measure. In Section 6 we show further that an application of the FDT in situations where linear response does not exist cannot provide any reliable statistical information, not even in an averaged sense. We conclude with a summary in Section 7.",
    "id": 170,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 170,
    "created_at": "2023-03-01T18:48:10.432995Z",
    "updated_at": "2023-03-01T18:48:10.433018Z",
    "lead_time": 6.003
  },
  {
    "corpusid": 55648852,
    "section": "1/40",
    "subsection": 1.0,
    "section_pos_in_pct": 0.025,
    "text": "Mt. Etna is one of the most fascinating natural laboratories for studying the deep structure of an active volcano with seismic tomography. Mt. Etna's eruptive dynamics are the result of a complex interaction between magma in its plumbing system, the regional (deep crust) tectonic regime, and local (shallow crust; volcanic edifice) structures, partially controlled by flank instability. Magma ascent driving conditions (e.g., structural setting, tectonic forces) are not yet completely understood. One of the main limitations in understanding the volcano's eruptive dynamics is the insufficient knowledge of its intermediate and deep crust. As indicated in Ibáñez et al. [2016, in this volume] in the last 25 years, several seismic and tomographic experiments have been focused on the study of Mt. Etna's structure using different techniques and **data** sets. The TOMO-ETNA experiment is an international and multi-institutional joint effort focused into providing answers to some of the open questions associated to the structure and dynamics of Mt. Etna. This experiment will take advantage of the use of both active and passive seismic sources and the combination of information of other seismic and geophysics measurements such as marine seismic reflection [Coltelli et al. 2016, in this volume;Firetto Carlino et al. 2016, in this volume] or magnetic profiles [Cavallaro et al. 2016, in this volume].",
    "id": 171,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 171,
    "created_at": "2023-03-01T18:48:22.001854Z",
    "updated_at": "2023-03-01T18:48:22.001900Z",
    "lead_time": 11.341
  },
  {
    "corpusid": 236522469,
    "section": "9/33",
    "subsection": 1.0,
    "section_pos_in_pct": 0.273,
    "text": "Matthews' Similarity and Correlation are classification metrics considered to be less biased as they use multiple input variables; they incorporate both the set imbalance and the amount of **data** referring to classes [34]. In this regard, the inclusion of species caused lower values of those metrics in comparison to Accuracy and the F-score since they have bias related to the number of characteristics, and, specifically, the F-score does not depend on the true negatives (TN). The high similarity observed here demonstrates that the classifier can identify true negatives, just as the high sensitivity indicates that the classifier was able to identify true positives [35].",
    "id": 172,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 172,
    "created_at": "2023-03-01T18:48:35.260580Z",
    "updated_at": "2023-03-01T18:48:35.260611Z",
    "lead_time": 13.05
  },
  {
    "corpusid": 245800515,
    "section": "8/16",
    "subsection": 3.0,
    "section_pos_in_pct": 0.5,
    "text": "First of all we note that if u is continuously differentiable, then D β t u(0) = 0 and consequently (1.1) implies Au 0 = f (0). Therefore if we take as initial **data** U 0 = u 0 , the correction function vanishesĜ ≡ 0. Next, for u to be smooth, in general f is not smooth but behaves as\nf (t) ∼ f (0) + c 1 t 1−β\nasymptotically as t → 0. To understand the origin of this singularity, simply substitute u = u(0) + u ′ (0)t in the fractional equation (1.1).",
    "id": 173,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 173,
    "created_at": "2023-03-01T18:48:44.986344Z",
    "updated_at": "2023-03-01T18:48:44.986375Z",
    "lead_time": 9.529
  },
  {
    "corpusid": 253553223,
    "section": "15/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.536,
    "text": "Corollary 4.14 (Convergence rates in probability). Suppose the regularisation strategy g α has qualification q ν + s. Suppose that Y ∈ L ψ 2 (P; Y), X ∈ L ψ 2 (P; X ), θ ⋆ ∈ Ω(ν, R), and 0 < α < 1. Let δ ∈ (0, 1 e ] and s ∈ [0, 1 2 ]. If the regularisation parameter α = α n is chosen to depend on the number n of **data** points via\nα n := 1 √ n 1 ν+1\n, then, for n n 0 := max X 4 L ψ 2 (P;X ) , 1152e 2 X 4 L ψ 2 (P;X ) log(1/δ)\n1 ν 1+ν ,\nwith P ⊗n -probability at least 1 − 2δ,\nθ ⋆ − θ αn C s XX S 2 (X ,Y) 3κ log(1/δ) 1 √ n s+ν 1+ν\n, whereκ := max{κ ν,C XXγ R, 64κ D,B eB ψ 2 } < ∞ is obtained from the constants appearing in Propositions 4.12 and 4.13.",
    "id": 174,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 174,
    "created_at": "2023-03-01T18:48:51.350557Z",
    "updated_at": "2023-03-01T18:48:51.350585Z",
    "lead_time": 6.171
  },
  {
    "corpusid": 235727288,
    "section": "17/46",
    "subsection": 1.0,
    "section_pos_in_pct": 0.37,
    "text": "AIS with geometric paths is often considered the goldstandard for evaluating decoder-based generative models (Wu et al., 2017). In this section, we evaluate whether qpaths can improve marginal likelihood estimation for a vari-  First, we use AIS to evaluate the trained generative model on the true test set, with a Gaussian prior π 0 (z) = p(z) as the base distribution and true posterior π 1 (z) = p(z|x) ∝ p(x, z) as the target. Intermediate distributions then becomẽ π β (z) = p(z)p(x|z) β . We report stochastic lower bound estimates (Grosse et al., 2015) of E pdata(x) log p(x) in Fig. 6( When exact posterior samples are available, we can use a reverse AIS chain from the target density to the base to obtain a stochastic upper bound on the log marginal likelihood (Grosse et al., 2015). While such samples are not available on the real data, we can use simulated **data** drawn from the model using ancestral sampling x, z ∼ p(z)p(x|z) as the dataset, and interpret z as a posterior sample. We use the Bidirectional Monte Carlo (BDMC) gap, or difference between the stochastic lower and upper bounds obtained from forward and reverse chains on simulated data, to evaluate the quality of the AIS procedure.",
    "id": 175,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 175,
    "created_at": "2023-03-01T18:48:58.058210Z",
    "updated_at": "2023-03-01T18:48:58.058241Z",
    "lead_time": 6.512
  },
  {
    "corpusid": 238249036,
    "section": "4/5",
    "subsection": 1.0,
    "section_pos_in_pct": 0.8,
    "text": "FIG. 1 .\n1Neutron multiplicity dependence of acoplanarity distributions from γγ → μ þ μ − for p μ T > 3.5 GeV, jη μ j < 2.4, jy μμ j < 2.4, and 8 < m μμ < 60 GeV in ultraperipheral Pb-Pb collisions at ffiffiffiffiffiffiffiffi s NN p ¼ 5.02 TeV. The α distributions are normalized to unit integral over their measured range. The dot-dot-dashed and dotted lines indicate the core and tail contributions, respectively, found using a fit to Eq. (1). The vertical lines on **data** points depict the statistical uncertainties, while the systematic uncertainties and horizontal bin widths are shown as gray boxes.\n",
    "id": 176,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 176,
    "created_at": "2023-03-01T18:49:01.610353Z",
    "updated_at": "2023-03-01T18:49:01.610387Z",
    "lead_time": 3.355
  },
  {
    "corpusid": 55648852,
    "section": "8/40",
    "subsection": 1.0,
    "section_pos_in_pct": 0.2,
    "text": "Two broadband seismic stations of the permanent monitoring network are installed at Pozzo Pitarrone [Zuccarello et al. 2016, in this volume], one at surface (EPIT) and one at the well bottom (EPIP), at depth of about 130 meters. A temporary seismic array was deployed in the area around Pozzo Pitarrone between May and October 2014. The array consisted of 8 three component broadband stations arranged in two roughly concentric rings of radii 100 m and 200 m centered at EPIT station. The seismometers used for the array were Guralp CMG-40T 60 s (http://www.guralp.com) and Nanometrics Trillium 120 s compact (http://www. nanometrics.ca), and signals were acquired by Nanometrics Taurus **data** loggers recording at 200 sps, with 24 bits dynamic range.",
    "id": 177,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 177,
    "created_at": "2023-03-01T18:49:46.954861Z",
    "updated_at": "2023-03-01T18:49:46.954886Z",
    "lead_time": 45.152
  },
  {
    "corpusid": 249258070,
    "section": "51/119",
    "subsection": 4.0,
    "section_pos_in_pct": 0.429,
    "text": "The breakdown of self-initiated activities across these three units were 64.2% for patrol, 17.5% for special crime units, and 18.3% for the differential response team. Unfortunately, the HPD CFS **data** do not provide detailed information about the nature of self-initiated activities, such as offense or incident type for patrol officers. Activities performed by specialized crime investigation and DRT officers might be deduced approximately using the division information. But to further breakdown the activity to specific units produces small frequencies of specific offenses, which would have been insufficient to conduct meaningful analysis. For example, there were only 338 self-initiated activities recorded under gang division, and 165 self-initiated activities recorded under robbery division during the study period. The **data** do include some information on dispositional outcomes of police self-initiated activities. The most frequent type of action for self-initiated activities is \"collecting information\" for each of the three types of police units. The disposition of \"collecting information\" is used in instances where the incident is resolved without needing to generate an offense report or referral for further investigation. Some examples are if the officer resolves the issue by mediation, the complaint is a civil issue rather than criminal, or if it is to provide further information about an offense report that already exists. Other self-initiated actions include making an offense or supplementary report, making an arrest, issuing tickets, etc.",
    "id": 178,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 178,
    "created_at": "2023-03-01T18:50:00.374026Z",
    "updated_at": "2023-03-01T18:50:00.374049Z",
    "lead_time": 13.194
  },
  {
    "corpusid": 232380196,
    "section": "24/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.857,
    "text": "Experimental results on long-tailed CIFAR The comparison between the proposed hybrid networks and existing methods on long-tailed CIFAR datasets is presented in Table 1. The compared methods cover various categories of ideas for imbalanced classification, including loss re-weighting [7], margin modification [3], **data** augmentation [19], decoupling [37] and some other newly proposed ideas [34,13]. As can be seen from the table, our hybrid networks outperform the compared methods on almost all the settings.",
    "id": 179,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 179,
    "created_at": "2023-03-01T18:50:13.912481Z",
    "updated_at": "2023-03-01T18:50:13.912505Z",
    "lead_time": 13.328
  },
  {
    "corpusid": 53612739,
    "section": "7/14",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "We now measure the ability for each of the ramp profiles in Sec. II to prepare our spin system into the ground state of Eqn. 1 at B = 0. For this measurement, we use N = 6 ions and create AFM spin-spin interactions of the form J i,j ≈ (0.77 kHz)/|i−j|. These long-range AFM interactions lead to a fully-connected, frustrated system as all couplings cannot be simultaneously satisfied. Nevertheless, the ground state of the system is easily calculable for 6 spins and is found to be a superposition of the two Néel-ordered AFM states, (|010101 + |101010 )/ √ 2. Fig. 3(a) shows the probability of creating the AFM ground state when the transverse field B(t) is ramped using linear, exponential, and local adiabatic profiles. The total ramp time t f is varied from 0 to 2.4 ms, with a new ramp profile calculated for each t f . Each **data** point is the result of 4000 repetitions of the same experiment, with error bars that account for statistical uncertainty as well as estimated drifts in the Ising coupling strengths. In agreement with the predictions in Sec. II, the **data** show that local adiabatic ramps prepare the ground state with higher fidelity than exponential or linear ramps.",
    "id": 180,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 180,
    "created_at": "2023-03-01T18:50:22.752690Z",
    "updated_at": "2023-03-01T18:50:22.752723Z",
    "lead_time": 8.605
  },
  {
    "corpusid": 119335883,
    "section": "3/26",
    "subsection": 1.0,
    "section_pos_in_pct": 0.115,
    "text": "Suppose an election has average vote share in districts party A lost y (0 ≤ y ≤ 1 2 ) and average vote share in districts party A won z ( 1 2 ≤ z ≤ 1) with y = z. Then the Declination-preferred seat share for this set of **data** is\nS = 1 − 2z 2(y − z)\nNote that for S = 1−2z 2(y−z) , ∂S ∂y = 2z − 1 2(y − z) 2 ≥ 0 which indicates that, as y goes up (which can be reasonably interpreted as party A getting more cracked) then S goes up (party A wins more seats). In Figure 11 we can see the level curves for S = 1−2z 2(y−z) with the allowable values of y and z, a different visualization of the information in Figure 10. We see, for example, that when the seat share S is low, y can take on any value but z is quite restricted. Conversely, when S is high, z can take on any value but y is restricted.",
    "id": 181,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 181,
    "created_at": "2023-03-01T18:51:34.213637Z",
    "updated_at": "2023-03-01T18:51:34.213662Z",
    "lead_time": 71.25
  },
  {
    "corpusid": 29602952,
    "section": "1/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.053,
    "text": "Cystic fibrosis is a hereditary disease that affects the mucus glands of the lungs, liver, pancreas, and intestines, causing progressive disability due to multisystem failure. The CFTR gene, found in Chromosome 7, is the cause of cystic fibrosis, where mutations result in proteins that are too short because of premature end to production. We have been analyzing **data** on cystic fibrosis for the School of Medicine, Medical College of Georgia, and because of confidentiality issues we cannot present these **data** in this paper. Although these **data** are very sparse with only a few individuals reported cystic fibrosis in southern Georgia, our **data** set has the same structure as one that has been used repeatedly in the literature. Table 1 gives a set of **data** on cystic fibrosis, which was presented by Crow [3] to illustrate the need to take account of the method of ascertainment in segregation analysis. One can countthe total number of offspring to be 269, the total number of affected offspring to be 124, and the total number of probands to be 90. Thus, one might estimate the segregation ratio to be 124/269 = .4610, and the ascertainment probability to be 90/124 = .7258. Again, these simple estimates are too inflated. Note that 46.1% is far in excess of the 25% expected on simple recessive inheritance (cystic fibrosis is autosomal recessive). One reason for the excess is the ascertainment bias -the exclusion of families where the parents are heterozygous, but fail to have a homozygous recessive child. These would add to the number of normal children and thereby reduce the proportion affected. This **data** set was also used in [4] for illustration.",
    "id": 182,
    "sentiment": "data availability statement",
    "annotator": 1,
    "annotation_id": 182,
    "created_at": "2023-03-01T18:52:10.845695Z",
    "updated_at": "2023-03-01T18:52:10.845726Z",
    "lead_time": 10.674
  },
  {
    "corpusid": 5227101,
    "section": "3/10",
    "subsection": 1.0,
    "section_pos_in_pct": 0.3,
    "text": "Focal field simulations. Focal field calculations were applied to investigate the effects of primary classical aberrations on the PSFs created by different binary PMs upon focusing the corresponding circularly polarized pupil function with a high NA objective. Thus, phase modulation experiments could be verified under strictly controlled conditions regarding misalignment and aberration. To account for significant contributions of polarization effects when focusing with a high NA objective lens, the vectorial diffraction theory of Richards and Wolf 26,27 was applied, to compute focal fields and intensity distributions. 2D-Fast Fourier Transform (FFT) operations allow for efficient computation of vectorial Debye integrals for arbitrary polarization states of the pupil function 14,28 and were used in the implementation of Boruah and Neil 14 . Comparison of simulation results with experimental **data** confirmed the correctness of our implementation.",
    "id": 183,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 183,
    "created_at": "2023-03-01T18:52:27.241463Z",
    "updated_at": "2023-03-01T18:52:27.241490Z",
    "lead_time": 16.173
  },
  {
    "corpusid": 54041858,
    "section": "7/45",
    "subsection": 1.0,
    "section_pos_in_pct": 0.156,
    "text": "Location was determined from the difference in times of arrival of the two outer sensors, t. In addition, the speed of sound of the extensional wave or the time it takes for sound to travel between the outer sensors, tx, was required, where x is the distance between the two outer sensors. The speed of sound (tx/x) increases throughout the test as damage develops [6,24]. This was accounted for by determining the tx from events which occurred outside the outer sensors, presumably in the grips [25]. There was a period of no AE activity between 100,000 and 130,000 s (Figure 4a). For the following, only the location **data** after 130,000 s is presented since it leads up to failure. The location of the events prior to 100,000 s were evenly distributed across the gage section. The AIC derived t values typically differed less than 1 s of the actual first peak t. However, to get the most accurate t, the times of arrival of the first peaks on sensors 1 and 2 for events which occurred after 130,000 s were determined manually [13] and used to determine location. Examples of valley and peak waveforms are shown in Figure 6 with arrows indicating first peaks for sensors 1 and 2, top and bottom, respectively. Note that the first peak for the valley event on the top and bottom sensor is negative and the first peak for the peak event is positive; this is significant and is explained below.  ",
    "id": 184,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 184,
    "created_at": "2023-03-01T18:53:02.616123Z",
    "updated_at": "2023-03-01T18:53:02.616151Z",
    "lead_time": 35.151
  },
  {
    "corpusid": 119634392,
    "section": "1/9",
    "subsection": 3.0,
    "section_pos_in_pct": 0.111,
    "text": "In [18], Kazuo obtains a unique global solution to equation (1.2) in dimension three provided the initial **data** is in the Sobolev space H m,2 (R 3 ), with m > max{5/2, 1 + 2γ 1 }, provided γ 1 and γ 2 satisfy the inequality 2γ 1 + γ 2 ≥ 5 and that\n(1.3) ∞ 1 ds sg 2 1 (s)g 2 (s) = ∞.\nThe goal of this paper is to obtain a much wider array of existence results, specifically existence results for initial **data** with low regularity and for initial **data** outside the L 2 setting. We will also, when applicable, use the energy bound from [18] to extend these local solutions to global solutions. Our plan is to follow the general contraction-mapping based procedure outlined by Kato and Ponce in [7] for the Navier-Stokes equation, with two key modification.",
    "id": 185,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 185,
    "created_at": "2023-03-01T18:53:14.281257Z",
    "updated_at": "2023-03-01T18:53:14.281281Z",
    "lead_time": 11.419
  },
  {
    "corpusid": 119080499,
    "section": "14/30",
    "subsection": 1.0,
    "section_pos_in_pct": 0.467,
    "text": "The localized nature of the vacancy state in LaH x is consistent with the temperature dependence of the d.c. resistivity **data** at room temperatures, which has a temperature depedence consistent with variable range hopping [11]. Localized states are a prerequisite for variable range hopping. In the next section we will discuss the electronic band width of the vacancy states.",
    "id": 186,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 186,
    "created_at": "2023-03-01T18:53:18.076638Z",
    "updated_at": "2023-03-01T18:53:18.076662Z",
    "lead_time": 3.588
  },
  {
    "corpusid": 54041858,
    "section": "8/45",
    "subsection": 3.0,
    "section_pos_in_pct": 0.178,
    "text": "of the events prior to 100,000 s were evenly distributed across the gage section. The AIC derived t values typically differed less than 1 s of the actual first peak t. However, to get the most accurate t, the times of arrival of the first peaks on sensors 1 and 2 for events which occurred after 130,000 s were determined manually [13] and used to determine location. Examples of valley and peak waveforms are shown in Figure 6 with arrows indicating first peaks for sensors 1 and 2, top and bottom, respectively. Note that the first peak for the valley event on the top and bottom sensor is negative and the first peak for the peak event is positive; this is significant and is explained below.  From the determination of first peak times of arrival, the location was determined from:\nLocation = (x/2)(t/tx)(1)\nwhere x is the length between the outer sensors (25 mm). The location of each event after 130,000 s is plotted in Figure 7a versus the time of the test where the value 0 is the center of the gage section. Each **data** point represents a single event and the width of the **data** point is proportioned to the average AE energy of the given event. There are scattered and infrequent peak events for the over 70,000 s (~20 h) leading up to the heightened period of AE activity just prior to failure. The events at the end of the test are so dense that the period after 200,000 s (box in Figure 7a) are plotted in Figure 7b,c for the valley and peak events, respectively. Most of the peak and all of the valley events are concentrated in the 0 to +1.5 mm location from the center of the gage, which is where failure took place. Whatever occurred just prior to failure would not correspond to distributed matrix cracking along the gage length. Therefore, one would expect to observe localized damage near the fracture surface within an approximate 2 mm length or less of the composite, as shown below.",
    "id": 187,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 187,
    "created_at": "2023-03-01T18:53:31.052285Z",
    "updated_at": "2023-03-01T18:53:31.052308Z",
    "lead_time": 12.738
  },
  {
    "corpusid": 201251303,
    "section": "1/10",
    "subsection": 2.0,
    "section_pos_in_pct": 0.1,
    "text": "Probit models with spatial dependencies were first studied by McMillen (1992), and he proposed an EM algorithm to produce consistent maximum likelihood estimates for the model. In SAR probit model, the spatial dependence structure adds complexity to the estimation of parameters. The main assumption of the model is that the distribution of errors is known and is often assumed to be normal. Parameter estimation using a full maximum likelihood method is problematic because the likelihood function involves n integrals, where n is the sample size. To avoid the direct calculation of ndimensional integration, several estimators have been proposed that can produce consistent estimates when **data** are spatially autocorrelated and heteroscedastic (e.g., Beron Pinkse and Slade (1998) proposed estimators for the parameter of SAR probit model who becomes infeasible for large samples because they require the inversion of × matrices. LeSage (2000) used Bayesian estimates through the Markov Chain Monte Carlo and Gibbs sampling, which sampled sequentially from the complete conditional distribution for all parameters. Klier and McMillen (2008) have proposed a linearized version of the GMM estimator that avoids the infeasible problem of inverting × matrices when employing large samples and show that standard GMM reduces to a nonlinear two-stage least squares problem.",
    "id": 188,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 188,
    "created_at": "2023-03-01T18:53:38.092602Z",
    "updated_at": "2023-03-01T18:53:38.092627Z",
    "lead_time": 6.814
  },
  {
    "corpusid": 222090354,
    "section": "2/21",
    "subsection": 1.0,
    "section_pos_in_pct": 0.095,
    "text": "Definition 2.8. For ρ 0 ∈ Z E,p and ρ ∈ AC([0, ∞); P p (X)) a curve of maximal slope with ρ(0) = ρ 0 , we denote by ω ρ (ρ 0 ) the ω-limit set with initial **data** ρ 0 subordinate to ρ, defined by\nω ρ (ρ 0 ) := ρ ∈ P p (X) : ∃t n → ∞ s.t. ρ(t n ) σ →ρ as n → ∞ .\nWe now give a characterisation of stationary states in relation to the weak upper gradient G which we will use extensively in the sequel.",
    "id": 189,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 189,
    "created_at": "2023-03-01T18:53:40.906973Z",
    "updated_at": "2023-03-01T18:53:40.907004Z",
    "lead_time": 2.611
  },
  {
    "corpusid": 246165803,
    "section": "23/45",
    "subsection": 1.0,
    "section_pos_in_pct": 0.511,
    "text": "Both algorithms were trained, with highly acceptable results. Both algorithms had over 90% accuracy during training, with the KNN having 95.55% accuracy and the ANN scoring 96.79% over the training data. Figures 10 and 11 show the confusion matrices generated with the test data. While the KNN algorithm did have minor errors in classifying non-anomalous **data** and curves, the ANN had trouble classifying only curves. Thus, the number 0 represents healthy data; number 1 represents possible potholes, number 2 speed bumps, and number 3 harsh curves.  Lastly, both algorithms were given 85% of the whole dataset to train and 15% to test. These tests consisted of accuracy for training, and F1-Score for test data, as well as a confusion Matrix, to determine the specific classifications in which they did not perform well enough.",
    "id": 190,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 190,
    "created_at": "2023-03-01T18:53:58.534697Z",
    "updated_at": "2023-03-01T18:53:58.534725Z",
    "lead_time": 17.425
  },
  {
    "corpusid": 207778550,
    "section": "27/48",
    "subsection": 6.0,
    "section_pos_in_pct": 0.562,
    "text": "The most significant quantity in all these criticality experiments is the mass of the assembled fissile material in the benchmark assembly when it is close to the criticality point (critical mass). Measured physical parameters (other than nuclear data) that influence determination of the critical mass are the geometry of the fissile material, its isotopic composition and density, the geometry and material composition of the support structure, and characteristics of the surrounding environment. The neutron leakage from cylinders with various geometries is rather sensitive to the energy-angular distribution of neutrons scattered in the benchmark fissile material and less sensitive for geometrically similar spheres. Uncertainties of the physical parameters (e.g., dimensions) for manufactured cylinders of fissile material are much smaller than those for comparable spheres. Criticality measurements are very sensitive to fissile material geometries. Errors in determining benchmark physical parameters, underestimation of their uncertainties, failure to apply required corrections, and deficiencies or simplifications in the models used in simulations can affect the ∆k eff comparisons significantly. The excessive impact of these benchmarks on **data** validation and/or **data** adjustment could be mitigated by considering USU.",
    "id": 191,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 191,
    "created_at": "2023-03-01T18:54:03.361246Z",
    "updated_at": "2023-03-01T18:54:03.361272Z",
    "lead_time": 4.622
  },
  {
    "corpusid": 117119864,
    "section": "51/83",
    "subsection": 2.0,
    "section_pos_in_pct": 0.614,
    "text": "\nFermi surface map for T = 120K >> TK = 20K of CeRu2Si2 compared to ARPES map of LaRu2Si2, showing same size of large hole surface contour for both compounds. Absence of reduced hole surface size due to Ce 4f electron inclusion, as predicted by LDA and observed at very low T in dHvA experiments, is evidence of Ce 4f electron exclusion from Fermi surface at T >> TK, as proposed in Refs.[77,78].[from Ref. 18]. 5. Summary of below (102 eV) and above (108 eV and 112 eV) resonance ARPES **data** in panels (a)-(c) for heavy Fermion material URu2Si2, showing confinement of 5f weight (panel (b)) to interior of Ru d-band hole pocket (panel (c)), as in simple 2-band ansatz for the Anderson lattice model where a Kondo-renormalized f-state at εf′ hybridizes to the d-band, as shown in panels (d) and (e). [from Ref. 18] 6. Bulk sensitive PES spectra taken at photon energy 500 eV, showing V 3d valence band in all three phases of MI transition material (V0.982Cr0.018)2O3. Paramagnetic metal (PM) phase weight near EF could be quasi-particle weight transferred from Hubbard peaks of the antiferromagnetic insulating (AFI) or paramagnetic insulating (PI) phases, as in the dynamic mean field theory [91, 106] of the MI transition. [unpublished **data** from collaboration of Ref. 102]. 7. (a) schematic band structure of quasi-1-d metal Li0.9Mo6O17; (b) ARPES spectra showing all four bands A -D; (c) overlaid plot of ARPES spectra for comparison of band C lineshape to spectral theory [Ref. 126] of Tomonaga-Luttinger model in panel (d), showing lack of Fermi liquid quasi-particle due to Luttinger-liquid-like electron fractionalization, presently the only such ARPES example. [from Refs. 135 and 150].\n",
    "id": 192,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 192,
    "created_at": "2023-03-01T18:55:04.552479Z",
    "updated_at": "2023-03-01T18:55:04.552502Z",
    "lead_time": 60.995
  },
  {
    "corpusid": 18538029,
    "section": "1/9",
    "subsection": 2.0,
    "section_pos_in_pct": 0.111,
    "text": "The simplest possibility for the dark energy component is cosmological constant Λ or equivalently vacuum energy [6]. In the past, a nonzero cosmological constant has been advocated and then with the improved observational **data** discarded several times [7]. Due to this checked history and the difficulty in understanding the observed Λ in the framework of modern quantum field theory, now most physicists and astronomers prefer other candidates for dark energy, including a frustrated network of topological defects such as strings or walls [8] and an evolving scalar field referred to by some as quintessence [9] etc. As shown in literatures, it is difficult to discriminate against these different possibilities either by the SNeIa **data** alone [10] or only by the CMBR **data** [11]. This led some authors to consider the combination of the SNeIa measurements with the anisotropy of CMBR [12] or the large scale structures [13].",
    "id": 193,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 193,
    "created_at": "2023-03-01T18:55:21.153040Z",
    "updated_at": "2023-03-01T18:55:21.153069Z",
    "lead_time": 16.363
  },
  {
    "corpusid": 38256305,
    "section": "10/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.204,
    "text": "Local site conditions are detected by different methods. To provide a first approximation of these conditions, an approach was developed to characterize potential ground motions based on known correlations between variations in shear-wave velocity and topographically-distinctive landforms by applying geomorphometry, a quantitative description of landforms based on DEMs . Wald and Allen (2009) described a methodology for deriving maps of seismic site conditions using topographic slope as a proxy [46][47][48]. Vs 30 measurements (the average shear-velocity down to 30 m) were correlated against topographic slope. They also favorably compared topographic slope-based Vs 30 maps with existing site condition maps based on geology. We converted the Vs 30 **data** into point-shape files, which served as a base for IDW (Inverse Distance Weighted) interpolations ( Figure 9). The resulting interpolation maps were overlain then with geologic and morphometric maps.",
    "id": 194,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 194,
    "created_at": "2023-03-01T18:55:31.279601Z",
    "updated_at": "2023-03-01T18:55:31.279629Z",
    "lead_time": 9.911
  },
  {
    "corpusid": 117119864,
    "section": "40/83",
    "subsection": 3.0,
    "section_pos_in_pct": 0.482,
    "text": "Resonant ARPES reveals that parts of the FS show easily observable 4f weight at 120K. As presented in Ref. [18] this 4f weight is found in k-space in the vicinity of the low mass parts of the FS, the small hole pockets around Z and the small electron pocket around Γ. The angle integrated 4f spectrum at and above TK obtained directly [23] or by k-summing the resonant ARPES spectra [83] has the gener al appearance of the impurity-model spectra of Figs. 2 and 3. Although this finding may be plausible in view of the general theoretical correlation of smaller mass, larger energy scale and larger spectral weight, there is presently no spectral theory of t he Anderson lattice capable of providing insight into ARPES **data** at this level of detail. The so called \"LDA + DMFT\" theory discussed in section 5.2 below is promising for further analysis of such **data** for systems where the Fermi surface is well described by the LDA.",
    "id": 195,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 195,
    "created_at": "2023-03-01T18:55:40.691004Z",
    "updated_at": "2023-03-01T18:55:40.691034Z",
    "lead_time": 9.174
  },
  {
    "corpusid": 207778550,
    "section": "14/48",
    "subsection": 3.0,
    "section_pos_in_pct": 0.292,
    "text": "It may be surmised from the preceding discussion that uncovering the existence of USU, and developing procedures by which these uncertainties can be taken into consideration, requires evaluators to examine features of the **data** points themselves, including both their mean values and assigned uncertainties, and not simply to accept the author-provided mean values and uncertainties as a matter of faith. Failure to consider the possible existence of USU, if the contributions are significant, will result in the very deficiencies that manifest themselves in the aforementioned clues.",
    "id": 196,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 196,
    "created_at": "2023-03-01T18:55:45.274588Z",
    "updated_at": "2023-03-01T18:55:45.274614Z",
    "lead_time": 4.372
  },
  {
    "corpusid": 251500724,
    "section": "14/26",
    "subsection": 1.0,
    "section_pos_in_pct": 0.538,
    "text": "Figure 2 .\n2The kC temperature as a function of time during the printing stages: (a) cooling down from the loading temperature to the extruding temperature (the experiment at 40 • C is shown); (b) cooling down of the generated droplet (see text) after extrusion, during the self-supporting stage. The droplet was printed at 40 • C on the printer plate kept at 25 • C. Lines correspond to the best fit of the **data** to Equation(1).\n",
    "id": 197,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 197,
    "created_at": "2023-03-01T18:55:48.354261Z",
    "updated_at": "2023-03-01T18:55:48.354321Z",
    "lead_time": 2.868
  },
  {
    "corpusid": 251719245,
    "section": "5/21",
    "subsection": 1.0,
    "section_pos_in_pct": 0.238,
    "text": "Rank-based construction guarantees that each vertex is reachable from at least vertices in the graph. As reflected in Table 2, no singletons were created even for the graph with the least number of edges. With this construction, even with 621 edges, we can already get relationships involving all genes in the **data** set. In contrast with the graph obtained by value-based with = 0.90 with 1, 177 edges,  we can only relate about 63% of the genes in the **data** set. However, this construction also allows edges with weak correlation to be part of the network. We show the comparison of the total number of edges between all the graphs obtained by value-based and rank-based construction. In Figure 6, we see that the total number of edges as we increase is far below the total number of edges for the value-based construction. The highest number of edges is with parameter = 10 which is approximately 17% of the total number of edges of the graph with = 0.70. In Figures 7 and 8, we show the two graphs obtained by rankbased construction with parameters = 2 and = 10, respectively. We visualized both graphs using the Fruchterman-Reingold algorithm to compute for the position of the nodes and used the biological classification to color the set of nodes. With = 2, the resulting graph consists of all the 384 genes in the original **data** set. The genes belonging to groups are not visually separated as compared to the graph obtained by = 10. Even though the graph uses way less number of edges as compared to the graph obtained through valuebased with = 0.70. The visual closeness of the nodes belonging to the same functional group is present. The temporal relationship genes that are present in the original NMDS visualization are also reflected. Visually, we can see the proximity of genes belonging to the same functional groups for both value-based and rank-based construction with = 0.70 and = 10, respectively. According to Ruan et al., [11], the comparison of the rank-based and the value-based construction has not been rigorously examined in the literature. In the following section, we will test whether which of the following graphs can be used to predict functional groups using community detection algorithms.",
    "id": 198,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 198,
    "created_at": "2023-03-01T18:56:03.993444Z",
    "updated_at": "2023-03-01T18:56:03.993472Z",
    "lead_time": 15.431
  },
  {
    "corpusid": 88514824,
    "section": "2/12",
    "subsection": 2.0,
    "section_pos_in_pct": 0.167,
    "text": "The rest of the paper is organized as follows. The second section introduces the proposed method that estimates the Hurst exponent with a Bayesian approach. The third section presents the simulation results and compare the estimation performance of the proposed method to the traditional regression method. The fourth section illustrates an application of the proposed method to a real-life **data** set, a turbulence velocity signal, that is known to possess Hurst exponent H = 1/3. The last section is devoted to the concluding remarks and a future research direction.",
    "id": 199,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 199,
    "created_at": "2023-03-01T18:56:09.109105Z",
    "updated_at": "2023-03-01T18:56:09.109135Z",
    "lead_time": 4.918
  },
  {
    "corpusid": 126349038,
    "section": "23/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.821,
    "text": "Fig. 11\n11Contours of (a) axial velocity and crossflow streamlines, and (b) TKE at x 5 0.6 for b 5 0 deg. Subplots show: experimental **data** (left); S1 (middle), and S2 (right).",
    "id": 200,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 200,
    "created_at": "2023-03-01T18:56:12.875968Z",
    "updated_at": "2023-03-01T18:56:12.875994Z",
    "lead_time": 3.561
  },
  {
    "corpusid": 52141668,
    "section": "8/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "As the result of the low accuracy in detecting deception, we perform an additional experiment. In this case, we also try to use features from the  Figure 3: Comparison accuracy and F-measure between using development **data** and test **data** acoustic/prosody that can be extracted from the recorded sound **data** of IDC. In accordance with previous research related to detecting deception using speech analysis (Enos, 2009;Graciarena et al., 2006;Hirschberg et al., 2005), we use features from silence, energy, and pitch category then apply some normalization techniques to the extracted features.",
    "id": 201,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 201,
    "created_at": "2023-03-01T18:56:21.251117Z",
    "updated_at": "2023-03-01T18:56:21.251144Z",
    "lead_time": 8.162
  },
  {
    "corpusid": 251711372,
    "section": "101/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.849,
    "text": "The remainder of this paper is organized as follows. Section 2 reviews the relevant literature. Section 3 provides the testable research hypotheses. Section 4 presents the **data** and the event study methodology. In Section 5, we present and discuss our main findings. Section 6 concludes the paper.",
    "id": 202,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 202,
    "created_at": "2023-03-01T18:56:27.397701Z",
    "updated_at": "2023-03-01T18:56:27.397737Z",
    "lead_time": 5.954
  },
  {
    "corpusid": 119479526,
    "section": "16/29",
    "subsection": 3.0,
    "section_pos_in_pct": 0.552,
    "text": "Figures 13a and 13b display the results of these analyses with and without hyperparameters. Without hyperparameters, the best fit model lies between **data** sets A and B, which follow the same trend, and set C, which has a different intercept. In contrast, the best fit model using hyperparameters, shown in Fig. 13b agrees with the true model used to generate the **data** in this example. In this figure we have scaled the Likelihood for both models we find a values −2.6 for the model with hyperparameters and −323.2 without.",
    "id": 203,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 203,
    "created_at": "2023-03-01T18:56:32.620910Z",
    "updated_at": "2023-03-01T18:56:32.620937Z",
    "lead_time": 5.032
  },
  {
    "corpusid": 118072666,
    "section": "9/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.692,
    "text": "By selecting two identified oppositely-charged kaons from a sample of Υ (4S) **data** and subtracting the combinatorial and continuum background, we measure the average multiplicity of φ mesons in B meson decays. Our measurement of B(B → φX) = (3.41 ± 0.06 ± 0.12)% is consistent with both previous measurements at the 1.5σ level, although it is significantly more precise.",
    "id": 204,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 204,
    "created_at": "2023-03-01T18:56:40.579028Z",
    "updated_at": "2023-03-01T18:56:40.579059Z",
    "lead_time": 7.718
  },
  {
    "corpusid": 238249036,
    "section": "5/5",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "FIG. 2 .\n2Neutron multiplicity dependence of (upper) hα core i and (lower) hm μμ i of μ þ μ − pairs in ultraperipheral Pb-Pb collisions at ffiffiffiffiffiffiffiffi s NN p ¼ 5.02 TeV. The vertical lines on **data** points depict the statistical uncertainties, while the systematic uncertainties of the **data** are shown as shaded areas. The dot-dashed line shows the STARLIGHT prediction, and the dashed line corresponds to the leading-order QED calculation of Ref.[48].\n",
    "id": 205,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 205,
    "created_at": "2023-03-01T18:56:45.452620Z",
    "updated_at": "2023-03-01T18:56:45.452651Z",
    "lead_time": 4.632
  },
  {
    "corpusid": 14915342,
    "section": "22/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.786,
    "text": "Theorem 4 .\n4Consider the following commutative diagram. Then π N (H) : M H → M H /N (H) is a bundle reduction of π : M → M/G and π ′ : M H → M H /(N (H)/H) is a principal bundle reduction of M H → M H /N (H). There is a one-to-one correspondence between principal bundle connections on M H → M H /(N (H)/H) and singular connections on M . Furthermore, the curvature form for the singular connection restricted to M H is equal to the curvature form Ω of the principal connection. Proof. As we have already remarked, every G-orbit in M intersects M H in a unique N (H)-orbit. Therefore for m ∈ M H , π N (H) (m) = π(ι(m)), and then M/G ≃ M H /N (H). Furthermore, since H fixes every point of M H , the free action of N (H)/H on M H has the same orbits as the action of N (H) on M H and therefore the bundle M H → M H /(N (H)/H) is a principal bundle reduction of M H → M H /N (H). To set up the one-to-one correspondence of connections, we start with a connection on the principal bundle π ′ : M H → M H /(N (H)/H). Denote this connection by Γ consisting of horizontal spaces Q m for each m ∈ M H and its connection form by ω : T M H → n/h. We show how to induce from this **data** a connection on M → M/G where the connection **data** will consist of a horizontal distribution invariant with respect to the group action. Given m ′ ∈ M we have m ′ = g · m for some m ∈ M H and g ∈ G.",
    "id": 206,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 206,
    "created_at": "2023-03-01T18:56:55.449831Z",
    "updated_at": "2023-03-01T18:56:55.449854Z",
    "lead_time": 9.79
  },
  {
    "corpusid": 2482529,
    "section": "22/37",
    "subsection": 1.0,
    "section_pos_in_pct": 0.595,
    "text": "(Note that the error is the error relative to the target , not the error relative to the testing **data** . The difference between these two errors is random noise, known as irreducible prediction error.) Typically there is a trade-off between bias and variance -decrease in one usually causes increase in the other -but we have seen here that it is sometimes possible to minimize both simultaneously.\n1 4 ⁄ d i t i ≠ d i t i = squared error E g i t i - ( ) 2 ( ) = E g i t i - ( ) 2 ( ) E g i ( ) t i - ( ) 2 E E g i ( ) g i - ( ) 2 ( ) + = E g i t i - ( ) 2 ( ) t i E g i β i - ( ) 2 ( ) β i",
    "id": 207,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 207,
    "created_at": "2023-03-01T18:57:00.506903Z",
    "updated_at": "2023-03-01T18:57:00.506930Z",
    "lead_time": 4.844
  },
  {
    "corpusid": 16542351,
    "section": "6/8",
    "subsection": 2.0,
    "section_pos_in_pct": 0.75,
    "text": "\nsti cs on b events atLEP w i l lsoon al l ow to study pol ari zati on e ects i n a purel y b sam pl e,thati sforeseen to retai n m ostofthei ni ti alb-quark pol ari zati on,i . e. an orderofm agni tudel argerthan i n theprevi ousi ncl usi veanal ysi s.In refs.[ 16,25,26] and[ 27] ,som estudi esofexcl usi ve decaysofpol ari zed b arepresented.O fcourse,the am bi gui ti es i n the know l edge ofthe b-quark fragm entati on al so appl y to pol ari zati on m easurem ents based on sem i l eptoni c b-baryon decays.A rel evant questi on i s w hether, w i th a reasonabl e stati sti cs on sem i l eptoni c b decays (som e **data** are al ready avai l abl e[ 28] ),LEP **data** w i l lno l onger need to be norm al i zed w i th l ower-energy data. O ne coul d study the rati os hx N ' i b (P )=hx N ' i B ofthe l epton m om ents for the b sam pl e over those for the B -m eson sam pl e,both m easured at LEP.In thi s case,one gets\n",
    "id": 208,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 208,
    "created_at": "2023-03-01T18:57:23.072374Z",
    "updated_at": "2023-03-01T18:57:23.072407Z",
    "lead_time": 22.364
  },
  {
    "corpusid": 119253123,
    "section": "7/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.219,
    "text": "A pile-up rejection procedure is applied to the set of **data** taken at √ s = 7 TeV: events with more than one distinct reconstructed primary vertex are rejected. This cut has a negligible effect on simulated events without pile-up: only 0.06% of the events are removed. We have compared a selection of high pile-up probability runs (see section 3) with a sample of low pile-up probability runs. The UE distributions differ by 20-25% between the two -4 - samples. After the above mentioned rejection procedure, the difference is reduced to less than 2%. Therefore, in the runs considered in the analysis, the effect of pile-up is negligible.",
    "id": 209,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 209,
    "created_at": "2023-03-01T18:57:27.121341Z",
    "updated_at": "2023-03-01T18:57:27.121369Z",
    "lead_time": 3.845
  },
  {
    "corpusid": 85554746,
    "section": "16/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.842,
    "text": "Figure 4 .\n4Observational stations of selected networks and their relative density for (a) ground-based atmospheric and greenhouse gas, (b) ground-based meteorological and (c) oceanic observation on and around the African continent. More detailed information of the represented networks can be found in table 2 of the supplementary material. The heatmaps are based on kernel density estimations of all stations of the considered networks within a bounding box of 40 to −40 degrees latitude and −50 to 80 degrees longitude. Note that the operational status of each station has not been taken into account since this information was not available for all networks. The source for the station **data** for most of the networks is the WMO's OSCAR tool.\n",
    "id": 210,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 210,
    "created_at": "2023-03-01T18:57:44.154364Z",
    "updated_at": "2023-03-01T18:57:44.154392Z",
    "lead_time": 16.826
  },
  {
    "corpusid": 246863515,
    "section": "13/44",
    "subsection": 1.0,
    "section_pos_in_pct": 0.295,
    "text": "We need to prepare for an experimental dataset D E and an observational dataset D O . For the experimental dataset, we directly use **data** from San Diego, which includes n (1) E = 6978 people in the treatment group and n (0) E = 1154 people in the control group. For the observational dataset, we consider inject synthetic confounding into the Riverside **data** (which originally include N 1 = 4405 people in the treatment group and N 0 = 1040 people in the control group).",
    "id": 211,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 211,
    "created_at": "2023-03-01T18:58:00.920060Z",
    "updated_at": "2023-03-01T18:58:00.920087Z",
    "lead_time": 16.524
  },
  {
    "corpusid": 53869932,
    "section": "3/3",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "\nwe present our results for the magnetization behavior for a variety of parameter values. In each panel, **data** from several J 4 values is presented side-by-side, with each panel using **data** from a different J 3 value. Several trends are discernible in this figure. The width of the 1/2 plateau increases with increasing J 3 . The critical magnetic field at which\n",
    "id": 212,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 212,
    "created_at": "2023-03-01T18:58:18.468062Z",
    "updated_at": "2023-03-01T18:58:18.468089Z",
    "lead_time": 17.319
  },
  {
    "corpusid": 249258070,
    "section": "111/119",
    "subsection": 3.0,
    "section_pos_in_pct": 0.933,
    "text": "The CFS **data** also captured proactive activities conducted by three distinct types of police units: patrol, specialized crime investigation, and the DRTs. Other than responding to calls, patrol officers also perform patrol duties, investigate suspicious activities, and form relationships with community members. Officers in specialized crime investigation divisions, such as robbery, vice, auto theft, gang, major assaults and family violence, and homicide divisions etc., mainly conduct investigations concerning specific type of offenses. Officers in DRT units do not respond to calls for service, instead, they proactively work with the community, focusing on problem solving and improving quality of life issues. These activities are all recorded as self-initiated activities within the CAD system.",
    "id": 213,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 213,
    "created_at": "2023-03-01T18:58:33.905568Z",
    "updated_at": "2023-03-01T18:58:33.905598Z",
    "lead_time": 15.232
  },
  {
    "corpusid": 118072666,
    "section": "8/13",
    "subsection": 4.0,
    "section_pos_in_pct": 0.615,
    "text": "The use of one single kaon selection efficiency for all φ momenta was compared to the use of separate values above and below p φ = 1.2 GeV/c. The observed difference in the average multiplicity was 0.9%. This is below the statistical error on the kaon identification efficiencies, hence no additional error was assigned to this source. The statistical error on the kaon selection efficiencies is treated as part of the statistical error in this analysis as it is obtained from the same **data** set as our signal and scales appropriately.",
    "id": 214,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 214,
    "created_at": "2023-03-01T18:58:40.340926Z",
    "updated_at": "2023-03-01T18:58:40.340952Z",
    "lead_time": 6.232
  },
  {
    "corpusid": 235829209,
    "section": "6/47",
    "subsection": 2.0,
    "section_pos_in_pct": 0.128,
    "text": "There are also some other advanced machine-learning techniques available that can better evaluate accuracy of an ANN model. For example, using the Kfold cross validation method [31] ensures that every observation from the original dataset has the chance of appearing in training and test sets, especially when limited input **data** is available. In our examples for lattice optimization, rather than using the time-consuming cross-validation that K-fold requires, sufficient training **data** were generated with a particle tracking simulation code, because using only one-turn tracking for such rings requires much less of a demand on computational resources.",
    "id": 215,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 215,
    "created_at": "2023-03-01T18:58:51.372610Z",
    "updated_at": "2023-03-01T18:58:51.372634Z",
    "lead_time": 10.788
  },
  {
    "corpusid": 1520137,
    "section": "9/35",
    "subsection": 1.0,
    "section_pos_in_pct": 0.257,
    "text": "Compared with most of the previous studies on the influences by the spatial context, our study uses simpler stimuli that can be more easily or quantitatively manipulated and described. Consequently, we not only model our **data** using a simple Bayesian inference and decision model, but also use this model to deduce that, at least in inference, the underlying neural mechanisms do not cause contextual facilitation or suppression of input sensitivities observed at the visual encoding stage [6,31]. Some of the previous studies [4,14], using more controlled stimuli, have also shown that human inference is like that of an ideal observer in a Bayesian inference. In these studies, the Bayesian inferences were based on the known or built in statistics of visual inputs. In comparison, we model a Bayesian influence using a model of the visual input statistics, parameterized by P(yes), k, and r n , which we show is consistent with the Gestalt grouping laws which in turn is presumably based on the actual statistics of natural visual inputs. Furthermore, since the target input was independent of the context in the stimulus presentation by the experimenter, the observers' context-dependent perception of the target suggests that they did not modify their internal belief or statistical model of the visual world by sampling the recent stimulus inputs for this task.",
    "id": 216,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 216,
    "created_at": "2023-03-01T18:59:09.233575Z",
    "updated_at": "2023-03-01T18:59:09.233604Z",
    "lead_time": 17.631
  },
  {
    "corpusid": 3640435,
    "section": "28/47",
    "subsection": 1.0,
    "section_pos_in_pct": 0.596,
    "text": "The corresponding **data** shapes are shown in Table 5. Recall that cuDNN treats (N, P, Q, K, C, R, S) convolutions as implicit (NPQ, K, CRS) matrix multiplications. DeepSpeech  16 79  341 32  1  5 20 431024 100  Conv1  16 38  166 32  32  5 10 100928 1600  Conv2  OCR  16 24  240 32  16  3 3  92160  144  Conv3  16 12  120 64  32  3 3  23040  288  Conv4  Face Recognition  8  54  54  64  64  3 3  23328  576  Conv5  8  27  27  128  128  3 3  5832  1152  Conv6  16    The performance benefits of ISAAC (see Figure 9) are noticeable but not as large as they were for GEMM. This is because cuDNN was optimized from the ground up with both Maxwell and DeepBench-like problems in mind (Large NPQ, small K and intermediate CRS).\nN P Q K C R S NPQ CRS Name\nNonetheless, we note substantial performance gains (1.5× to 2×) over cuDNN for the deep reductions found in Conv7 and Conv8.",
    "id": 217,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 217,
    "created_at": "2023-03-01T18:59:14.083851Z",
    "updated_at": "2023-03-01T18:59:14.083878Z",
    "lead_time": 4.636
  },
  {
    "corpusid": 38256305,
    "section": "11/49",
    "subsection": 2.0,
    "section_pos_in_pct": 0.224,
    "text": "Geosciences 2016, 6, 20 12 of 20 covers, such as extensive erosion surfaces (peneplains). Therefore, a careful and accurate comparison with the geologic and geomorphologic conditions in the investigation area is necessary. The reservoir area is characterized by estimated Vs 30 **data** ranging between 600 and 760 m/s. The dam site area is obviously not affected by higher susceptibility to soil amplification, as in the case of lower Vs 30 velocities (<400 m/s) within broader valleys and depressions.",
    "id": 218,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 218,
    "created_at": "2023-03-01T18:59:21.163139Z",
    "updated_at": "2023-03-01T18:59:21.163167Z",
    "lead_time": 6.872
  },
  {
    "corpusid": 135401293,
    "section": "8/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "Based on the experimental work an Artificial Neural Network (ANN) model was developed to predict BSFC and BTE. The input parameters were load, percentage of biodiesel, injection angle, injection pressure. The predictive ability of the developed network model for BSFC and BTE is excellent. The selected input parameter for this work is based on e review of the literature [23][24][25][26]. The comparison is made between network predicted Thermal efficiency with experimental **data** and which is shown in Fig. 11. The percentage of deviation between ANN predicted BTE with experimental BTE is exhibited in Fig. 12. The developed ANN model for Brake Thermal Efficiency (BTE) has a very low MSE content of 0.0069 along with RMSE of 0.66522% and MEP of 1.12% across all the test points. The correlation coefficient (R) for network estimated values is 0.995355. ",
    "id": 219,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 219,
    "created_at": "2023-03-01T18:59:29.365607Z",
    "updated_at": "2023-03-01T18:59:29.365640Z",
    "lead_time": 7.999
  },
  {
    "corpusid": 17309701,
    "section": "8/16",
    "subsection": 2.0,
    "section_pos_in_pct": 0.5,
    "text": "We presented for the first time an abundance analysis for the system and found results in agreement with those for B stars in the solar neighbourhood for both components. This analysis led to a systematic uncertainty of some 500 K for the effec-tive temperatures of the components. Thus, these two parameters of this seemingly well-known system remain uncertain despite our detailed analysis based onéchelle spectroscopy. It is not uncommon to find systematic uncertainties and deviations between photometrically and spectroscopically derived effective temperatures of B-type stars (e.g. De Ridder et al. 2004 for a discussion), and even to have uncertainties of order 500 K for such stars among methods based on the same **data** (e.g. Smalley & Dworetsky 1995, Morel et al. 2006, Kaiser 2006. Previous works on CV Vel did not include a spectroscopic temperature estimate and were thus not able to estimate the systematic uncertainty of the component's temperatures.",
    "id": 220,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 220,
    "created_at": "2023-03-01T18:59:38.395780Z",
    "updated_at": "2023-03-01T18:59:38.395813Z",
    "lead_time": 8.802
  },
  {
    "corpusid": 221635250,
    "section": "13/22",
    "subsection": 1.0,
    "section_pos_in_pct": 0.591,
    "text": "DMA evaluation with cluster standards\n1 / FWHM Diff = CV 1/2 ; C = 1.55 Volt − 1/2 (2)\nThe value of C analogously obtained for the prior Perez DMA model was 1.35 V − 1/2 , indicating a superior performance of the LT version, perhaps as a result of the better laminarization or the larger convergence angle. One can see at all pump settings that the **data** for the lower voltages (smaller clusters) approach best the diffusive limit. Yet, past a certain voltage, the resolution shows a drastic reduction. This reduction cannot be due to turbulence, since each series is obtained at fixed flow rate with only the size of the cluster varying. We believe it is primarily an artificial consequence of the fact that the larger clusters are not completely resolved from each other (top spectrum in Fig. 4 above 0.4 kV), so that either the tails from neighbor cluster peaks or ion impurities between main members of the cluster series create a background that is not accounted for by the Gaussian fitting. The resolutions plotted in Fig. 5 for each Q cannot accordingly be trusted past the voltage at which they fall markedly below the diffusion line, which starts at the voltage at which subsequent peaks are no longer baseline separated from each other. This corruption naturally sets in at smaller voltages for smaller pump speeds, so the upper envelope of the various **data** sets provides a more reliable measure of the real resolution, uncorrupted by peak merging. For instance, the spectrum of Fig. 4 corresponding to a pump speed of 9000 rpm shows isolated peaks for all the clusters included (monomer to octamer), so all the open circles of Fig. 5 ought to give uncorrupted resolutions. The resolutions measured at 9000 rpm for three of the **data** exceeds 37, well above the best value of about 30 observed with the prior Perez DMA. If the zero drift correction had not been implemented, the maximal resolving power would have been 34, still substantially higher than in our earlier study with the shorter laminarization trumpet. The fact that the measured resolution at increasing voltages falls clearly below the diffusive limit indicates that there is some level of non-ideality with this instrument. In other words, while the ideal peak width is 0 for non-diffusing particles in the limit q/Q→0, the actual value is non zero:",
    "id": 221,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 221,
    "created_at": "2023-03-01T19:00:11.591162Z",
    "updated_at": "2023-03-01T19:00:11.591192Z",
    "lead_time": 32.987
  },
  {
    "corpusid": 2482529,
    "section": "11/37",
    "subsection": 1.0,
    "section_pos_in_pct": 0.297,
    "text": "We generate and from by randomly flipping bits in with probability p. The probability that the class of a training example or a testing example matches the target is , but the probability that the class of the training example matches the class of the testing example is . target\nt 1 … t 32 , , t = = train α 1 … α 32 , , α = = test β 1 … β 32 , , β = = t i α i β i 0 1 , { } ∈ , , α β t t 1 p - 1 2p - 2p 2 + P α i t i = ( ) 1 p - = P β i t i = ( ) 1 p - = P α i β i = ( ) 1 2p - 2p 2 + = α i β i t i = = 1 p - ( ) 2 α i β i t i ≠ = p 2 1 p - ( ) 2 p 2 + 1 2p - 2p 2 + = α β t 2 n (9)(10)\nIf the i-th bias strength gene has a value , then there is a probability that the individual will guess ; otherwise, there is a probability that the individual will guess . This simplified model does not describe the learning mechanism. We are dealing with a level of abstraction where the exact learning mechanism is not important. Since we assume that all 32 possible cases are in the training **data** , the individual can learn by simply storing the training data. In a more complex model, the genotype could encode the architecture of a neural network, and back propagation could be used to learn from the training **data** (Balakrishnan and Honavar, 1995).",
    "id": 222,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 222,
    "created_at": "2023-03-01T19:00:21.323325Z",
    "updated_at": "2023-03-01T19:00:21.323349Z",
    "lead_time": 9.501
  },
  {
    "corpusid": 57758460,
    "section": "1/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.091,
    "text": "Water quality deterioration has a great influence on the aquatic biota and the ecosystem of a river. The increase in suspended solids limits the light penetration which has major impacts on algae and macrophytes while nutrient enrichment can lead to the depletion of oxygen and subsequently fish kill [9][10][11]. Exposure to high turbidity and suspended solids impacts fish growth and increases the mortality of fish [12][13][14][15][16]. Hence, water quality monitoring is important in order to evaluate the quality of the river for the health of sensitive aquatic organisms. The baseline **data** is also useful in management decision for improving and protecting the environment.",
    "id": 223,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 223,
    "created_at": "2023-03-01T19:00:24.852726Z",
    "updated_at": "2023-03-01T19:00:24.852754Z",
    "lead_time": 3.318
  },
  {
    "corpusid": 17671315,
    "section": "22/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.688,
    "text": "Theorem 2 guarantees existence and uniqueness of the solution of the GLK equation only when the **data** f (t) belongs to the range of the operator of the forward problem, i.e. for the case of errorless data. However, in the realistic case of an error in the data, stability with respect to this error is not guaranteed by Theorem 2. Hence, the question about regularizing properties of GLK remains open. On the other hand, estimate (41) of Theorem 1 ensures stability of AGCM with respect to a small error in the data, and the same is true for the 3-d version of this method [6,9,25].",
    "id": 224,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 224,
    "created_at": "2023-03-01T19:00:30.650439Z",
    "updated_at": "2023-03-01T19:00:30.650466Z",
    "lead_time": 5.59
  },
  {
    "corpusid": 126349038,
    "section": "12/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.429,
    "text": "\nFigure 7shows the vortical structures in the flow, similar toFig. 3for b ¼ 0 deg. The flow is asymmetric with respect to the center plane and the effects of the static drift angle dominate the flow. The crossflow from windward (WW) to leeward (LW) sides generates two primary vortices with counter clockwise rotation due to the sonar dome and WW bilge keel. The experimental **data** inFig. 7(a) clearly display the sonar dome (SDTV) and bilge keel (BKTV) tip vortices whose strength and progression are different from those shown for SDV and FBKV. SDTV is first observed at x ¼ 0.1 and BKTV at x ¼ 0.6. Additional secondary vortices are also observed: counter-rotating WW-and LW-FBKV with same rotation directions as PS and SS for b ¼ 0 deg; clockwise rotation ABKV; WW-and LW-free-surface vortices (FSV) due to wave breaking with clockwise and counter clockwise rotation on the LW and WW. WW-FBKV is first observed at x ¼ 0.2, and LW-FBKV at x ¼ 0.3. ABKV is first observed at x ¼ 0.8. LW-FSV is observed at x ¼ 0.12, and WW-\n",
    "id": 225,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 225,
    "created_at": "2023-03-01T19:25:17.271191Z",
    "updated_at": "2023-03-01T19:25:17.271217Z",
    "lead_time": 1486.402
  },
  {
    "corpusid": 232380196,
    "section": "24/28",
    "subsection": 2.0,
    "section_pos_in_pct": 0.857,
    "text": "Among these methods, CE denotes the simplest baseline which directly uses cross-entropy to train the network on the long-tailed datasets. As expected, this baseline method achieves the worst performance, which reveals the limitation of cross-entropy in dealing with imbalanced data. Although the performance can be improved by using advanced loss functions tailored for long-tailed **data** [3,7,22], these methods ignore the different properties of feature learning and classifier learning. BBN [37] takes a step further by decoupling the head **data** and tailed **data** modeling. But several factors of BBN compromise the full potential of decoupling learning: 1) It unifies the representation for two **data** streams with different properties in the penultimate layer; 2) Crossentropy loss is not an ideal loss for imbalanced **data** in both streams; 3) The final predication in testing phase is calculated as the sum of two prediction functions from two branches with equal weights, which is inconsistent with the training phase. Our methods address such limitations in that: 1) The projection module in our feature learning branch adapts the image representation to a space more suitable for contrastive loss; 2) We use different loss functions to learn the features and classifiers and conclude supervised contrastive loss can be a better substitute for cross-entropy in learning features from imbalanced data; 3) We use a single classifier learning function to predict the class labels for each sample, which avoids the gap between training and testing. Within our methods, SC based hybrid network, a.k.a Hybrid-SC, performs better than the PSC counterpart, a.k.a Hybrid-PSC, but the latter still performs on par with or better than the compared methods.",
    "id": 226,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 226,
    "created_at": "2023-03-01T19:25:49.762361Z",
    "updated_at": "2023-03-01T19:25:49.762393Z",
    "lead_time": 32.222
  },
  {
    "corpusid": 17309701,
    "section": "3/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.188,
    "text": "To measure the ω(O − C) 2 values, given in Table 4, we have re-analyzed the **data** of Feast (1954) and Andersen (1975). This showed that the **data** of Andersen (1975) and those of our paper have similar quality while those of Feast (1954) have clearly lower quality, as already emphasised by Andersen (1975). For this reason, we omitted Feast's **data** in the remaining of this paper. We combined the newly obtained radial-velocity values ( Table 2) with those previously obtained by Anderson (1975). This gave a total of 62 radial-velocity values for each of the components to which we assigned equal weights. As can be seen in Table 4, we find an offset in average velocity V 0 between the two radial-velocity sets. This could be due to the difference in used spectral lines in the two studies, but it may also be that we find a downward trend in V 0 over time due to the presence of a third body. We have too few epochs to model any V 0 trend for the time being. For this reason, we shifted the **data** towards the CORALIE    Table 2 and the sine curve corresponds to the elements given in Table 4. The center-of-mass velocity is indicated by the dashed-dotted line.",
    "id": 227,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 227,
    "created_at": "2023-03-01T19:26:17.462003Z",
    "updated_at": "2023-03-01T19:26:17.462039Z",
    "lead_time": 8.463
  },
  {
    "corpusid": 221635250,
    "section": "15/22",
    "subsection": 3.0,
    "section_pos_in_pct": 0.682,
    "text": "Prediction (3 b) is shown in Fig. 8a for the same value C = 1.55 previously inferred from cluster studies, and for two values of FWHM ∞ . This quantity was associated in the prior cluster **data** with any possible non-ideal broadening mechanism from geometrical or flow imperfections in the DMA. This was then justified because the cluster standards are believed to be strictly monomobile. The same has never been established for any viral standard, so that now FWHM ∞ combines nonidealities in the response of the instrument with the perhaps finite width of the mobility distribution of the viral particles. It is noteworthy that, once the diffusive and q/Q corrections Fig. 6. Comparison of the measured DMA dimensionless shape factors K(α) [filled circles, including the theoretical (0,1) value] and the approximate prediction (1c) for slowly varying shapes. have been included in the theory, it fits the **data** with a value of FWHM ∞ approximately half-way between 0% and 1%. In other words, even if the DMA had a perfectly ideal response, we can state that the intrinsic width of the size distribution of this virus is about 0.5%. Alternatively, even if the virus was perfectly monodisperse, the resolving power of the DMA for non-diffusing particles at q/Q→0 would be about 200! These inferences are based on the linear-left criterion to determine the resolution. The choice is justified because our goal is to gauge the imperfections on either the viral size distribution or the DMA response, rather than the evident imperfections in the electrospraying process responsible for the asymmetry of the peaks. We conclude from the analysis of the **data** for the CBPV particle that both the DMA and the particle standard are closer to the ideal than could have been anticipated.",
    "id": 228,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 228,
    "created_at": "2023-03-01T19:26:32.843481Z",
    "updated_at": "2023-03-01T19:26:32.843509Z",
    "lead_time": 15.218
  },
  {
    "corpusid": 17309701,
    "section": "7/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.438,
    "text": "We conclude to have found evidence of periodic line-profile variability in the primary and secondary of CV Vel with a period between 5.7 and 6.4 days. This is almost equal to the orbital period. Moreover, the quoted frequency error is a formal 1σ leastsquares error, and is an underestimation of the true error given that we covered less than two full cycles with unblended Mg II lines. It therefore seems that line-profile variability occurs with a period very close or equal to the orbital one. This is potentially interesting as it could point towards the excitation of a tidally induced g-mode oscillation in the components. However, given the phase relation we obtained, and the absence of any periodic signature in < v 2 >, it is more likely that a reflection and/or variable limb-darkening effect lies at the origin of the detected variability. We need a more extensive **data** set to firmly establish the correct interpretation of CV Vel's line-profile variability.",
    "id": 229,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 229,
    "created_at": "2023-03-01T19:27:01.761261Z",
    "updated_at": "2023-03-01T19:27:01.761288Z",
    "lead_time": 28.751
  },
  {
    "corpusid": 38256305,
    "section": "19/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.388,
    "text": "The result of the weighted overlay of causal, morphometric factors influencing the susceptibility to soil amplification was merged with the estimated shear wave velocity **data** acquired from U.S. Geological Survey according to Allen and Wald [46]. When the Weighted Overlay (WO) result was merged with estimated Vs 30 data, a clear correlation between higher WO values (>4) and lower Vs 30 values (<400 m/s) was revealed. The results obtained from the present effort can help to indicate areas having a strong possibility of major earthquakes. The newly-created database from the present study can unravel likely places where relatively higher damages are expected ( Figure 15). Dark red areas ( Figure 15) correlate with relatively higher susceptibility owing to their morphometric properties (such as broad valleys, depressions with outcropping unconsolidated sediments). Larger fault zones are more likely to act as seismic reflectors. However, vertical or horizontal movements operative along the fault zones have the potential to influence surface water permeability. This aspect needs further probing.  ",
    "id": 230,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 230,
    "created_at": "2023-03-01T19:27:15.184079Z",
    "updated_at": "2023-03-01T19:27:15.184112Z",
    "lead_time": 13.259
  },
  {
    "corpusid": 246863515,
    "section": "3/44",
    "subsection": 1.0,
    "section_pos_in_pct": 0.068,
    "text": "Here Equations (2) and (3) mean that U and X together account for all confounding in the observational data, but the observed covariates X alone are not enough. Moreover, we impose the overlap condition 0 < P (A = 1 | U, X, G = O) < 1, which is a standard assumption in causal inference literature. Because of the unmeasured confounders U , the observational **data** alone is not enough to identify the treatment effect parameter τ in eq. (1).",
    "id": 231,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 231,
    "created_at": "2023-03-01T19:27:19.287607Z",
    "updated_at": "2023-03-01T19:27:19.287634Z",
    "lead_time": 3.945
  },
  {
    "corpusid": 135401293,
    "section": "10/16",
    "subsection": 2.0,
    "section_pos_in_pct": 0.625,
    "text": "The present work also exhibits fruitful application of ANN model to estimate the output parameters of compression ignition engine fueled by NOME and Biogas with varying injection pressures. The results of developed ANN model is compared with actual results found by experimentation. From the error analysis, it was evident that the ANN predicted **data** matched the experimental **data** with high accuracy and correlation coefficient (R) values ranging from 0.977 to 0.999. The MEP was observed to be in the range of 1.1-4.57% with very low MSE. Study of proposed Back propagation network results concludes that ANN is one of the powerful predictor tools.\n",
    "id": 232,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 232,
    "created_at": "2023-03-01T19:27:24.932581Z",
    "updated_at": "2023-03-01T19:27:24.932608Z",
    "lead_time": 5.484
  },
  {
    "corpusid": 17671315,
    "section": "22/32",
    "subsection": 3.0,
    "section_pos_in_pct": 0.688,
    "text": "As it was pointed out in subsection 2.3.2, to have unbiased studies of these experimental data, it is important that the calibration factor should be chosen the same for all five targets of Table 1. However, it was shown in subsection 5.2 that it is impossible to choose such a calibration factor for GLK, which would provide satisfactory values of inclusion/background contrasts for all five targets. On the other hand, AGCM has worked successfully with the uniform calibration factor CF = 10 −7 for all five targets in the case of blind data. We point out that these conclusions are relevant only for the specific case of the above set of experimental **data** and for the above specific **data** pre-preprocessing procedure. On the other hand, we do not possess other sets of 1-d experimental **data** at the moment, and we are also unaware about other appropriate **data** pre-processing procedures.",
    "id": 233,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 233,
    "created_at": "2023-03-01T19:27:30.995690Z",
    "updated_at": "2023-03-01T19:27:30.995718Z",
    "lead_time": 5.912
  },
  {
    "corpusid": 119080499,
    "section": "25/30",
    "subsection": 1.0,
    "section_pos_in_pct": 0.833,
    "text": "FIG. 5 .FIG. 6 .\n56Electron hopping integral t(d) as a function of the distance d between two H ions. The hydrogen hopping integral decreases rapidly with increasing distance between the two H-ions. The equilibrium separation between the two nearest Htet's, and between neighboring Htet and Hoct are 5.29 a.u.and 4.58 a.u., respectively. The solid line shown here is fitted to the three **data** points. A cubic cell with the center of H − 2 as the origin is shown. Total charges of ions enclosed in the cell are neutral. Charges in unit of e on an edge, surface and corner are counted as a quarter, half and eighth of the original charges respectively. Only a few examples are given in the figure. Dotted lines are guides to the eyes.\n",
    "id": 234,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 234,
    "created_at": "2023-03-01T19:27:36.508639Z",
    "updated_at": "2023-03-01T19:27:36.508669Z",
    "lead_time": 5.357
  },
  {
    "corpusid": 16212589,
    "section": "3/6",
    "subsection": 6.0,
    "section_pos_in_pct": 0.5,
    "text": "If the limiting slope were less steep than we measured, then we should have seen orders of magnitude greater activity at the most negative potentials. For example, a \"limiting\" slope of e-fold/4 mV (six charges) fits our **data** between -60 and -40 mV. Extending this curve to -80 mV would give a Po of 8 * 10 -5, or 400-fold higher than we actually saw. Since this extra activity was absent, we must conclude that the slope could not have been as low as six effective charges.",
    "id": 235,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 235,
    "created_at": "2023-03-01T19:27:40.221236Z",
    "updated_at": "2023-03-01T19:27:40.221275Z",
    "lead_time": 3.555
  },
  {
    "corpusid": 67205543,
    "section": "9/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "The testing **data** including 5 dimension features of 30 grains (15 bread wheat grains and 15 durum wheat grains) and their testing results are tabulated in Table 2 to further inspect the **data** and results. While the number of \"2\" is assigned to specify bread grains, \"1\" is appointed to define the durum grains as targets of the ANN-ABC model. The ANN-ABC model proposed in this study accurately classifies 19 grains with 0 (zero) and 11 grains with very small absolute errors. Therefore, it classifies the total grains of 30 with a negligible MAE of 0.0034 and with 100% accuracy. It demonstrates that the proposed IPT based ANN-ABC model can be successfully utilized to classify the wheat grain varieties in an automatic manner.  ",
    "id": 236,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 236,
    "created_at": "2023-03-01T19:27:46.950142Z",
    "updated_at": "2023-03-01T19:27:46.950174Z",
    "lead_time": 6.567
  },
  {
    "corpusid": 40828380,
    "section": "6/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.462,
    "text": "Our interest in the first example is to study the effect which the parameter p has on various performance measures. The **data** for this example is as follows: K 15, L-5, the service rates for servers 1 and 2 are geometrically decreasing with (a) 3 (1) 2 (2) 2, It can be verified that A-4.",
    "id": 237,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 237,
    "created_at": "2023-03-01T19:27:51.162120Z",
    "updated_at": "2023-03-01T19:27:51.162154Z",
    "lead_time": 4.046
  },
  {
    "corpusid": 235829209,
    "section": "1/47",
    "subsection": 2.0,
    "section_pos_in_pct": 0.021,
    "text": "Therefore, an intuitive method for detecting chaos directly, purely from **data** is possible. In other words, predictability itself can act as a chaos indicator. From our studies we observed that by using the predictability of less-complex surrogate models, and a small volume of training data, some nonlinear behaviors in a dynamical system can be well characterized.",
    "id": 238,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 238,
    "created_at": "2023-03-01T19:27:55.929607Z",
    "updated_at": "2023-03-01T19:27:55.929630Z",
    "lead_time": 4.607
  },
  {
    "corpusid": 236522469,
    "section": "6/33",
    "subsection": 1.0,
    "section_pos_in_pct": 0.182,
    "text": "Validation of the classifications was performed on the dataset for the test. For this, statistical metrics were used to evaluate and test the performance of the adjusted CNN. The metrics are specified in Table 3, of which are calculated according to the results of the classifications. The Kappa index is a measure of agreement used in nominal scales that gives us an idea of how far the observations deviate from those expected, at random, thus indicating to us how legitimate the interpretations are. It measures the percentage of the **data** values on the main diagonal of the table and then adjusts these values for the amount of agreement that might be expected [50]. Accuracy is an index that reflects the rate at which individuals are correctly classified into the category containing their true score. Ranking accuracy is usually attributed to the appropriateness and validity of your decisions based on the obtained score. A large value for the index indicates a high hit rate of individuals in the correct categories, and a low value indicates a lower rate of correct classification of individuals [51]. Adjusted F-score AGF =\n5 × PPV × TPR (4 × PPV)+ TPR × (1 + 0.5 2 ) × NPV × TNR (0.5 2 × NPV)+ TNR\nUse all confusion matrix elements and provide more weights to samples that are correctly classified in the lowest class.",
    "id": 239,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 239,
    "created_at": "2023-03-01T19:28:02.112380Z",
    "updated_at": "2023-03-01T19:28:02.112404Z",
    "lead_time": 5.938
  },
  {
    "corpusid": 53478339,
    "section": "24/24",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "Figure 8 .\n8Temperature records from (a) mid-Holocene Sr/Ca derived record from KR-AMA-2, (b) modern microatoll KR-MMA-1 Sr/Ca record, and (c) instrumental **data** obtained from Mourilyan Harbour in situ **data** logger (http://www.aims.gov.au). Temperature values given are the mean of each dataset, with gray shading indicating the 95% confidence interval of the mean.",
    "id": 240,
    "sentiment": "data availability statement",
    "annotator": 1,
    "annotation_id": 240,
    "created_at": "2023-03-01T19:28:31.082870Z",
    "updated_at": "2023-03-01T19:28:31.082899Z",
    "lead_time": 28.768
  },
  {
    "corpusid": 2423505,
    "section": "9/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.391,
    "text": "106 Figure 1. The MEGA model proposed in [DauméIII and Marcu, 2006]. This model assumes the **data** is in fact generated by three distributions, a target, a common and and source. The MEGA model learns a classifier for each space. Left is the standard logistic regression model. der these assumptions\nP t (x, y) P s (x, y) = P t (x) P s (x) P t (y|x) P s (y|x) (8) = P t (x) P s (x) .(9)\nAgain, a well founded solution can be identified by appropriate instance weighting of the loss function. [Shimodaira, 2000] explored this approach and show that the weighted model better estimate the **data** given a biased sampling function. The quantity Pt(x) Ps(x) can be estimated using e.g. nonparametric kernel estimation [Sugiyama andMülcer, 2005, Shimodaira, 2000]. [Huang et al., 2007] proposed to directly estimate the ratio, i.e. the difference between the two distributions. They use the Kernel Mean Match,\n1 N s N s i=1 β i φ(x s i ) − 1 N t N t i=1 φ(x t i ) 2 ,(10)\nmetric that measures the distribution distance in a Reproducing Kernel Hilbert Space.",
    "id": 241,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 241,
    "created_at": "2023-03-01T19:28:41.394991Z",
    "updated_at": "2023-03-01T19:28:41.395021Z",
    "lead_time": 10.109
  },
  {
    "corpusid": 118551842,
    "section": "6/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.857,
    "text": "For g modes the relevant property are sharp variations in the buoyancy frequency, giving rise to departures from the uniform period spacing in Eq. (2). This has been used extensively to characterize the properties of pulsating white dwarfs (for a review, see Fontaine & Brassard 2008). The variation in the buoyancy frequency outside convective cores also has a substantial effect on g modes in main-sequence stars, particularly visible when, as in the case of slowly pulsating B stars, high-order modes are observed. Miglio et al. (2008) carried out a careful analysis of the resulting signatures in the period spacings. Remarkably, this behaviour was recently observed in CoRoT **data** on a B star (Degroote et al. 2010a,b), promising very valuable constraints on the mixing outside convective cores in such stars.",
    "id": 242,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 242,
    "created_at": "2023-03-01T19:28:52.980443Z",
    "updated_at": "2023-03-01T19:28:52.980470Z",
    "lead_time": 11.403
  },
  {
    "corpusid": 17228594,
    "section": "6/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.857,
    "text": "Figure 2 :\n2c(r). c pert (r) : 1-loop perturbation theory. β = 12.5 closest to **data** and β = 5 farthest.\n",
    "id": 243,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 243,
    "created_at": "2023-03-01T19:28:57.823098Z",
    "updated_at": "2023-03-01T19:28:57.823122Z",
    "lead_time": 4.636
  },
  {
    "corpusid": 119335883,
    "section": "2/26",
    "subsection": 1.0,
    "section_pos_in_pct": 0.077,
    "text": "Districts 1, 2, and 9 were the only close races, and those districts flipping from one party to the other changes the Declination substantially, from δ ≈ −0.228 to δ ≈ 0.515. One might ask whether this could possibly happen in practice, to which we answer a definitive yes. Indeed, the **data** for Election 1 in Table 4 is nearly precisely the outcome of the 2012 US Congressional election in Arizona, where the percentages are the Democratic party's vote share. (We say nearly precisely because some districts had more than two parties receiving a substantial number of votes). If Districts 1, 2, and 9 had flipped, Election 2 could have easily been an outcome. Clearly the Declination is still subject to volatility, and in at least one state the kind of volatility that the Declination is susceptible to actually does occur.",
    "id": 244,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 244,
    "created_at": "2023-03-01T19:29:06.564022Z",
    "updated_at": "2023-03-01T19:29:06.564049Z",
    "lead_time": 8.541
  },
  {
    "corpusid": 207778550,
    "section": "12/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.25,
    "text": "A quantitative clue that USU need to be considered is manifested when a calculated global chi-square-perdegree-of-freedom parameter (χ 2 /df ) that exceeds 1 significantly is generated in an evaluation without consideration of an USU contribution. This is the case especially if care has been taken by an evaluator to eliminate clearly discrepant experimental data, and to consider all known sources of uncertainty (as well as their correlations) in a realistic manner. Unfortunately, it is common practice for evaluators to simply multiply all the input **data** uncertainties by the factor χ 2 /df when this happens, and thus artificially force χ 2 /df to be exactly 1, e.g., see [3,23,25]. This is not considered to be good evaluation practice because it treats all input **data** points equally, e.g., see [79]. Doing so may lead to biases owing to the excessive influence of those **data** points that contribute to the large chi square per degree of freedom. So, it is a poor substitute for actually uncovering the specific origin of the problem (e.g., discrepant **data** points, overlooked known sources of uncertainty, failure to consider correlations, etc.), or for performing a detailed examination of the individual terms that contribute to the calculated global value of χ 2 /df , and only then introducing a targeted USU contribution when no other options for eliminating the problem are envisioned.",
    "id": 245,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 245,
    "created_at": "2023-03-01T19:29:21.654026Z",
    "updated_at": "2023-03-01T19:29:21.654052Z",
    "lead_time": 14.893
  },
  {
    "corpusid": 118072666,
    "section": "3/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.231,
    "text": "The selection of φ → K + K − candidates requires two oppositely-charged tracks that satisfy 0.1 < p T < 10 GeV/c, have at least 12 hits in the DCH, are consistent with originating from the primary interaction point, and satisfy kaon identification criteria based on dE/dx measurements and Cherenkov radiation. Tracks are assigned a kaon mass hypothesis and neutral two-track combinations are formed. Candidates are selected if their invariant mass is in the range 1.004 < m KK < 1.036 GeV/c 2 . This mass window is equivalent to about 4.5 standard deviations on either side of the nominal φ mass, where the RMS spread in the m KK distribution is due to both the natural φ width and the detector resolution. This relatively large acceptance is chosen to reduce the effect of a possible mass resolution difference between **data** and Monte Carlo, at the expense of signal-to-background sig-nificance.",
    "id": 246,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 246,
    "created_at": "2023-03-01T19:29:26.434481Z",
    "updated_at": "2023-03-01T19:29:26.434526Z",
    "lead_time": 4.601
  },
  {
    "corpusid": 249258070,
    "section": "91/119",
    "subsection": 4.0,
    "section_pos_in_pct": 0.765,
    "text": "The breakdown of self-initiated activities across these three units were 64.2% for patrol, 17.5% for special crime units, and 18.3% for the differential response team. Unfortunately, the HPD CFS **data** do not provide detailed information about the nature of self-initiated activities, such as offense or incident type for patrol officers. Activities performed by specialized crime investigation and DRT officers might be deduced approximately using the division information. But to further breakdown the activity to specific units produces small frequencies of specific offenses, which would have been insufficient to conduct meaningful analysis. For example, there were only 338 self-initiated activities recorded under gang division, and 165 self-initiated activities recorded under robbery division during the study period. The **data** do include some information on dispositional outcomes of police self-initiated activities. The most frequent type of action for self-initiated activities is \"collecting information\" for each of the three types of police units. The disposition of \"collecting information\" is used in instances where the incident is resolved without needing to generate an offense report or referral for further investigation. Some examples are if the officer resolves the issue by mediation, the complaint is a civil issue rather than criminal, or if it is to provide further information about an offense report that already exists. Other self-initiated actions include making an offense or supplementary report, making an arrest, issuing tickets, etc.",
    "id": 247,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 247,
    "created_at": "2023-03-01T19:29:35.910415Z",
    "updated_at": "2023-03-01T19:29:35.910446Z",
    "lead_time": 9.304
  },
  {
    "corpusid": 210840140,
    "section": "2/4",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "An integrated approach for the assessment of marine pollution generates large and heterogeneous datasets including variables with very different metrics and distributions. All of these must be considered when selecting the method for **data** analysis. In this Research Topic, Regoli et al. applied a Weight of Evidence (WOE) model to results from the monitoring of offshore platforms, based on multiple lines of evidence including chemical analyses and biological responses. The assessment of environmental quality was summarized in terms of hazard quotients, providing a useful tool for the application of risk assessment in environmental management. Lehtonen et al. assessed pollution impacts in coastal areas of Finland with a WOE approach, using caged mussels. The integration of chemical and biological effect **data** revealed that the health of marine organisms in the area of study is being altered by pollution. Parmentier et al. assessed the effects of tributyltin (TBT) on the brown shrimp (Crangon crangon) and suggest that long-term endocrine disruption effects was the reason for previously unexplained declines in C. crangon and other crustacean populations in the German Bight. Förlin et al. studied perch (Perca fluviatilis) populations at reference sites from the Swedish National Monitoring Program where a decline in fish health had been previously observed. Alterations in several biological responses, confirmed by mRNA expression levels, were related to higher concentrations of natural, bioactive brominated compounds measured in the perch, attributed to algal blooms in the Baltic Sea. Yang et al. studied the distribution and levels of persistent organic pollutants in surface sediments of Qingduizi Bay. Polychlorinated biphenyls (PCBs) and hexachlorocyclohexane (HCH) levels were attributed to their extensive historical use in adjacent areas. However, the DDT profile showed recent usage and fresh inputs from aquaculture activities, with levels that could be potentially toxic to marine organisms.",
    "id": 248,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 248,
    "created_at": "2023-03-01T19:29:45.753656Z",
    "updated_at": "2023-03-01T19:29:45.753686Z",
    "lead_time": 9.675
  },
  {
    "corpusid": 199124939,
    "section": "1/8",
    "subsection": 4.0,
    "section_pos_in_pct": 0.125,
    "text": "The effect of the push pulse can thus be summarised as projecting the 2 1 Agwavefunction into a manifold of spatially separated triplet-pairs by using an optical perturbation that selectively couples the triplet-pair amplitudes of the 2 1 Agwavefunction to the 1 (TT)* excited states, and their decay pathways towards nearly-free entangled triplet-pairs. Not only does this show how triplet-pairs can be harvested with low energy photons from non-luminescent polymers, it also provides a type of analysis of the total triplet character in the many-body 2 1 Agstate, as well as the possibility to study real-time, real-space triplet motion in an organic material. In fact, as ultrafast time-resolved microscopy is rapidly emerging as a viable experimental techniques 80 , it may even become possible in the near future to observe and manipulate the individual triplets in the nearly-free pairs. Push-induced dynamics of PDA with push pulse centred at 940 nm and tpush of 400 fs. The negative signals indicate that the effect of the push is to increase the 'hot' ground state and 2 1 Agpopulations. c. Push-induced response of the 2 1 Agstate when pushing at 400 fs. The 2 1 Aglifetime is increased to 15 ± 0.1 ps in this case. d. Comparison of decay (circles; solid line fit) for spectral region associated with the 2 1 Agstate in pumpprobe measurements, and 2 1 Agand 'hot' ground state in pump-push-probe experiments (tpush = 200 fs; mapped back to single scale where all pulses arrive at t = 0 fs). The PIA associated with 2 1 Agrapidly decays in pump-probe experiments; application of the push however enhances the lifetime of this PIA by at least one order of magnitude. e. Spectral cuts of pump-push-probe spectrum. The 'hot' ground state and 2 1 Ag -PIAs are present almost immediately after the push pulse with the latter being slightly delayed. The **data** is shown for tpush = 400 fs, with the times in the legends indicating the pump-probe delay. f. Magnification of the spectral region associated with the pushed 2 1 Ag -PIA; the spectra are normalised at 900 nm. Dashed overlay is a fit of two Gaussians to the profile (see SI, Figure S10 for further details) and highlights the narrowing of the spectrum at longer time delays following the push.",
    "id": 249,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 249,
    "created_at": "2023-03-01T19:29:53.443957Z",
    "updated_at": "2023-03-01T19:29:53.443986Z",
    "lead_time": 7.518
  },
  {
    "corpusid": 240554273,
    "section": "18/22",
    "subsection": 1.0,
    "section_pos_in_pct": 0.818,
    "text": "\nThe linear regression model tends to provide good **data** and estimations for the model and byconstructing a linear regression model in which the response variable is y, and the independent variable is and depends on the Kumaraswamy Lindley distribution as follows where where the random error has density function (KWL) with unknown parameters ., The parameter is the location parameter of yi where the parameter ( ) ( ) the log-likelihood function for the vector parameters ( ) can expression as following",
    "id": 250,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 250,
    "created_at": "2023-03-01T19:29:58.805208Z",
    "updated_at": "2023-03-01T19:29:58.805241Z",
    "lead_time": 5.189
  },
  {
    "corpusid": 55381955,
    "section": "7/12",
    "subsection": 1.0,
    "section_pos_in_pct": 0.583,
    "text": "The comparison of the expected spectra of light nuclei with the recent **data** of KASCADE-Grande and ARGO appears very problematic: while individually both sets of **data** can be fitted with reasonable values of the SN parameters and CR acceleration efficiencies, there is no model that can fit both at the same time. KASCADE-Grande **data** require a SN energy ESN = 2 × 10 51 erg, an efficiency ξCR = 20% and a rate of explosion of ℜ = 1/110 yr −1 that lead to a maximum energy EM ∼ = 3.7 PeV, whereas we can fit ARGO **data** with ESN = 10 51 erg, ℜ = 1/15 yr −1 and ξCR ∼ = 5.2% that lead to EM ∼ = 507 TeV. The disagreement between these two experiments is a purely experimental problem, that requires a serious and careful assessment of the systematic uncertainties involved in the adopted experimental techniques.\n",
    "id": 251,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 251,
    "created_at": "2023-03-01T19:30:06.970760Z",
    "updated_at": "2023-03-01T19:30:06.970786Z",
    "lead_time": 7.947
  },
  {
    "corpusid": 17671315,
    "section": "2/32",
    "subsection": 3.0,
    "section_pos_in_pct": 0.062,
    "text": "Hence, a rigorous definition of the \"approximate global convergence\" property was introduced in [6,9,24,25]. In simple terms, it means that first an approximate mathematical model is introduced. The approximation of AGCM is a mild one: it amounts to the truncation of an asymptotic series with respect to 1/s, s → ∞, where s > 0 is the parameter of the Laplace transform with respect to t of the above hyperbolic PDE. This truncation is done only on the first iteration. Next, a numerical method is developed within the framework of the resulting mathematical model. Next, a theorem is proved, which claims that this method delivers a good approximation for the exact solution of that CIP without any a priori knowledge of a small neighborhood of that solution. The common perception of global convergence is that one should obtain correct solution if iterations would start from almost any point. That theorem claims, however, that a small neighborhood of the exact solution is achieved if iterations would start not from any point but rather from a function which can be obtained without any knowledge of that small neighborhood. That theorem was confirmed in numerical studies of both computationally simulated and experimental data. 3-d experimental **data** were treated in [8,6,20]. In particular, the most difficult blind **data** case was considered in [6,20,24]. We refer to the paper [29] and references cited there for another non-local method for a coefficient inverse problem.",
    "id": 252,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 252,
    "created_at": "2023-03-01T19:30:20.495618Z",
    "updated_at": "2023-03-01T19:30:20.495643Z",
    "lead_time": 13.312
  },
  {
    "corpusid": 87345260,
    "section": "6/13",
    "subsection": 2.0,
    "section_pos_in_pct": 0.462,
    "text": "1st/2nd/3rd), but in any order. The researcher could consider only the fi rst position in the ranking and evaluates the persistence of this area , or could consider the whole ordered ranking. These measures could be too strict and will be sensitive to the smallest perturbation to the **data** set, while the fi rst to third position would be enough in terms of conservation planning. Given any measure of success, the re-sampling approach in conservation have some possible applications as:",
    "id": 253,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 253,
    "created_at": "2023-03-01T19:30:28.509451Z",
    "updated_at": "2023-03-01T19:30:28.509494Z",
    "lead_time": 7.83
  },
  {
    "corpusid": 17671315,
    "section": "22/32",
    "subsection": 4.0,
    "section_pos_in_pct": 0.688,
    "text": "To explain Fig. 10, we present the GLK equation (56) as\nw(z, t) − CF (X) 2 z −zf (t − τ )w(z, τ )dτ = 1 2 , t ∈ [−z, z], ∀z ∈ [0, T /2].(61)\nIn (61) f (t) is the pre-preprocessed **data** as in in subsection 2.3.1 andf (t) is the odd extension of f (t). Denote β := z · CF (X) sup |t|<T /2 f (t) . If β ∈ (0, 1), then one can solve integral equation (61) via the resolvent series, see, e.g. the book [35]. It is clear from this series that the solution w(z, t) changes almost with an exponential speed when the calibration factor CF (X) changes. This explains, at least partially, the exponential behavior of GLK curves of Fig. 10. This is because the **data** for the inverse problem are introduced in the operator. On the other hand, almost linear behavior of AGCM curves on Fig. 10 can be explained by the fact that AGCM uses the logarithm of the solution w of the problem (13), (14), see (18).\n",
    "id": 254,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 254,
    "created_at": "2023-03-01T19:30:35.450618Z",
    "updated_at": "2023-03-01T19:30:35.450646Z",
    "lead_time": 6.756
  },
  {
    "corpusid": 207847805,
    "section": "1/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "Dust evolution starts at the earliest stage of star formation, during the formation of cores that slowly contract to form pre-stellar cores (PSC), to the collapse of these PSCs into protostars and protoplanetary disks (PPD). The contraction time of PSCs is still a debated question and ranges from less than 1 My [1,2] to 10 My lifetime [3]. PSCs are dense (>10 5 H . cm −3 ), compact (< 10 4 AU) and cold (5-12 K) objects, making their study difficult. They are very opaque and most gaseous species are depleted onto grains preventing the study of the inner parts. Dust is the only tracer that is present from cloud edge to the densest part, allowing to characterize cloud density structure. Nevertheless, dust itself is a poor tracer in visible and near-infrared (NIR) since its absorption is too high to detect the reddening of the stars above A V ∼ 50 mag. Recent observations of PSCs with Spitzer [4][5][6] and PPDs with SPHERE [7] in scattered light has brought hope to put more constraints on dust properties. Indeed, all parameters involved in the dust emission (dust temperature, density, grain size, emissivity and spectral index) are varying across the cloud leading to degenerate solutions when dust emission is used alone [8]. Studies combining dust scattering and emission reproduced successfully PPD observations, allowing to constrain the geometry and density structure in PPDs [e.g. 9]. On the other hand, a consistent multi-wavelength modeling of PSCs remains an unachieved goal. Grain emissivity in the far-infrared (FIR) has been linked consistently to its absorption efficiency in the near-infrared (NIR) at short wavelengths only [10] or at low resolution with 5 Planck **data** [11], and thus never reproducing observations of the densest part. With our on-going NIKA2 open time Program, we aim at building a consistent multiwavelength picture of two neighbor molecular clouds, L183 and L134 (116±6 pc [C. Zucker priv. comm.] and 107±5 pc [12] respectively) hosting 4 PSCs.",
    "id": 255,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 255,
    "created_at": "2023-03-01T19:30:50.414800Z",
    "updated_at": "2023-03-01T19:30:50.414823Z",
    "lead_time": 14.797
  },
  {
    "corpusid": 246863515,
    "section": "1/44",
    "subsection": 7.0,
    "section_pos_in_pct": 0.023,
    "text": "In contrast, our paper does not need long-term outcome observations in the experimental **data** but only need them in observational data. Moreover, our paper does not view short-term outcomes as proxies for the long-term outcome, so we avoid these previous surrogate criteria. Instead, we view them as proxies for unmeasured confounders to correct for confounding bias. See also discussions in Section 2.3.",
    "id": 256,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 256,
    "created_at": "2023-03-01T19:30:59.037491Z",
    "updated_at": "2023-03-01T19:30:59.037534Z",
    "lead_time": 8.452
  },
  {
    "corpusid": 207778550,
    "section": "17/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.354,
    "text": "When ranges of **data** are involved, e.g., as characterized by particle energy, then USU contributions can be estimated as population standard deviations σ P in the average normalization factors of absolute **data** sets that span the whole energy range. This method was used in estimating USU contributions for R-matrix **data** in performing the ENDF/B-VIII standards evaluation [52].",
    "id": 257,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 257,
    "created_at": "2023-03-07T15:41:15.696122Z",
    "updated_at": "2023-03-07T15:41:15.696151Z",
    "lead_time": 145.467
  },
  {
    "corpusid": 207778550,
    "section": "6/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.125,
    "text": "Experimental input **data** used in evaluations need to be statistically consistent (i.e., largely free of problematic outliers), and generated from analyses of measured results that incorporate accurate and inclusive modeling of the relationships between the quantities that actually are measured and those that are sought to be obtained indirectly from the raw **data** (derived results). Estimated uncertainty magnitudes and reported correlations need to be reasonable as well as comprehensive. Data provided by experimenters, and archived in original form in publications or computer-accessible **data** repositories, e.g., in EXFOR [55], are considered to be raw data. Raw **data** usually need to be adjusted to be consistent with contemporary standards and fundamental nuclear properties such as decay parameters [56][57][58] (half lives and branching ratios, etc.). Additionally, a comprehensive UQ exercise should be undertaken. This is a point that will be stressed in the whole paper. As mentioned in Sect. II, rarely can experimental **data** published by original authors be accepted by evaluators directly as reported, especially if many years have transpired since the measurements were performed. Consequently, it is the responsibility of evaluators to revise the input **data** used in their evaluations, as needed, and to insure that the information is comprehensive. Otherwise, the results of these evaluations are likely to be flawed and misleading.",
    "id": 258,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 258,
    "created_at": "2023-03-07T15:41:49.168919Z",
    "updated_at": "2023-03-07T15:41:49.168951Z",
    "lead_time": 33.279
  },
  {
    "corpusid": 218971725,
    "section": "4/10",
    "subsection": 1.0,
    "section_pos_in_pct": 0.4,
    "text": "If we have informative priors on the parameters in both outcome and imputation model, the influence functions can be derived by combining phase-1 **data** and priors. The optimal wave-1 allocation can be estimated by Equation (5). Based on this idea, we propose the following optimal multi-wave sampling design:",
    "id": 259,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 259,
    "created_at": "2023-03-07T15:41:58.243246Z",
    "updated_at": "2023-03-07T15:41:58.243277Z",
    "lead_time": 8.885
  },
  {
    "corpusid": 17671315,
    "section": "3/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.094,
    "text": "It was pointed out in all publications about AGCM for experimental **data** [8,6,20,24] that there is a huge misfit between measured **data** and computationally simulated data, in the case of waves propagation in a non-attenuating medium. This misfit is evident from a simple visual comparison of simulated and experimental curves, see, e.g. Figures 3 and 4. This discrepancy is the main difficulty for the numerical treatment of experimental **data** by any computational method. In other words, the experimental **data** are lying far away from the range of the operator, which needs to be inverted to solve the CIP. It is well known that this is a compact operator. Hence, its range is very narrow and the inversion problem is unstable. Therefore, regularization is necessary. Still, the regularization theory gives recipes only in the case when the right hand side of the operator equation is not far from the range of that operator. However, in the case of those experimental data, the right hand side is actually far away from that range. Therefore, the crucial step of all above cited publications about experimental data, was the **data** pre-processing procedure. This procedure has extracted such a piece of the **data** from the whole data, which looked somewhat similar with the computationally simulated data. Still, it was impossible to estimate the distance between the extracted **data** and the range of that compact operator. Furthermore, those **data** pre-processing procedures cannot be rigorously justified neither from the Physics nor from the Mathematics standpoint. They were based on the intuition only. The single criterion of their success was the accuracy of resulting solutions of corresponding CIPs. Since accurate results were obtained for the blind real **data** in [6,20,24], then those **data** pre-processing procedures were unbiased. We describe below the **data** pre-processing procedure. It consists of two steps: extracting a piece of **data** from the whole **data** and multiplying this piece by a calibration factor. This factor is unknown and needs to be determined numerically.",
    "id": 260,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 260,
    "created_at": "2023-03-07T15:42:28.786328Z",
    "updated_at": "2023-03-07T15:42:28.786360Z",
    "lead_time": 30.358
  },
  {
    "corpusid": 1520137,
    "section": "27/35",
    "subsection": 1.0,
    "section_pos_in_pct": 0.771,
    "text": "Compared with most of the previous studies on the influences by the spatial context, our study uses simpler stimuli that can be more easily or quantitatively manipulated and described. Consequently, we not only model our **data** using a simple Bayesian inference and decision model, but also use this model to deduce that, at least in inference, the underlying neural mechanisms do not cause contextual facilitation or suppression of input sensitivities observed at the visual encoding stage [6,31]. Some of the previous studies [4,14], using more controlled stimuli, have also shown that human inference is like that of an ideal observer in a Bayesian inference. In these studies, the Bayesian inferences were based on the known or built in statistics of visual inputs. In comparison, we model a Bayesian influence using a model of the visual input statistics, parameterized by P(yes), k, and r n , which we show is consistent with the Gestalt grouping laws which in turn is presumably based on the actual statistics of natural visual inputs. Furthermore, since the target input was independent of the context in the stimulus presentation by the experimenter, the observers' context-dependent perception of the target suggests that they did not modify their internal belief or statistical model of the visual world by sampling the recent stimulus inputs for this task.",
    "id": 261,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 261,
    "created_at": "2023-03-07T15:42:37.613367Z",
    "updated_at": "2023-03-07T15:42:37.613399Z",
    "lead_time": 8.641
  },
  {
    "corpusid": 119313773,
    "section": "5/21",
    "subsection": 2.0,
    "section_pos_in_pct": 0.238,
    "text": "As seen in Section 4 the non-smoothness of the invariant measure is caused by the rapid displacement of spikes upon perturbation. The smaller and narrower the spike, the faster it moves. This points to an issue of resolution: the faster spikes carry less mass and therefore require a certain amount of **data** to be reliably resolved; the slower spikes carry more mass but their smaller displacement upon perturbation requires sufficient **data** to be resolved. This means that a sufficiently large amount of **data** is needed for the breakdown parameter q to accurately estimate the mismatch Eq, and to determine whether a system obeys linear response or not. This issue of resolving the mismatch Eq is an additional finite size issue to the one discussed in Section 3 whereby N needs to be sufficiently large to assure that the observed p-value is properly estimated (cf. (15)).",
    "id": 262,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 262,
    "created_at": "2023-03-07T15:42:48.502076Z",
    "updated_at": "2023-03-07T15:42:48.502104Z",
    "lead_time": 10.701
  },
  {
    "corpusid": 17837291,
    "section": "20/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.385,
    "text": "All mirror transformations we will use later will be of the type µ f , ν g as above. Moreover, all Euler **data** we will encounter will have property (i) of Theorem 5.4. The transformations µ f , ν g clearly preserve this property. Since A is linked to B, it follows thatÃ is linked toB. By Theorem 5.4, we conclude that A =B. Now our assertion follows from eqns. (6.6).",
    "id": 263,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 263,
    "created_at": "2023-03-07T15:42:53.413782Z",
    "updated_at": "2023-03-07T15:42:53.413808Z",
    "lead_time": 4.742
  },
  {
    "corpusid": 117119864,
    "section": "12/83",
    "subsection": 4.0,
    "section_pos_in_pct": 0.145,
    "text": "Another important point concerns the bulk sensitivity of the CeRu2Si2 ARPES **data** of Ref. [18]. These spectra were not measured at the high photon energies that enhance bulk sensitivity and which are generally necessary [23] for bulk sensitive Ce 4f studies. However it was found [18,78] that some cleaved surfaces yield 4f spectra at low photon energies, e.g. in RESPES at the Ce 4d edge, that are essentially the same as those at high photon energy, e.g. RESPES at the Ce 3d edge. Such surfaces were used to obtain the **data** of Ref. [18]. The explanation [83] probably lies in the particular crystal structure of CeRu2Si2, which admits of two cleavages, one that exposes Ce at the top layer and another that places Ce in a buried layer. The latter situation is expected to enable bulk sensitive studies even at lower photon energies.",
    "id": 264,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 264,
    "created_at": "2023-03-07T15:43:08.838603Z",
    "updated_at": "2023-03-07T15:43:08.838629Z",
    "lead_time": 15.241
  },
  {
    "corpusid": 118072666,
    "section": "6/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.462,
    "text": "Since the efficiency to reconstruct a φ depends on the φ momentum, differences between the momentum spectrum in **data** and the generated spectrum in the Monte Carlo sample must be considered. Therefore, the analysis was carried out separately in 16 bins of φ momentum. The bins are chosen to have equal (with the exception of the lowest momentum range) numbers of reconstructed φ mesons in the Monte Carlo. Figure 3 shows the efficiency ε 2T as a function of the φ momentum. ",
    "id": 265,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 265,
    "created_at": "2023-03-07T15:43:17.439036Z",
    "updated_at": "2023-03-07T15:43:17.439059Z",
    "lead_time": 8.436
  },
  {
    "corpusid": 1520137,
    "section": "17/35",
    "subsection": 1.0,
    "section_pos_in_pct": 0.486,
    "text": "Figure 6 .\n6Fit to Data in Experiment 3 by the Bayesian Model (A-C) The red, magenta, and blue curves and **data** points indicate respective quantities associated with different contextual contrasts C c ¼ 0.01, 0.05, and 0.4, respectively. The fitted Bayesian parameters (and their 95% confidential intervals) are k ¼ 3.91 (1.95, 5.87), r n ¼ 1.60 3 10 À3 (1.45 3 10 À3 , 1.73 3 10 À3 ), and P(yes) ¼ 0.97 (0.96, 0.98) for the 2-sided, P(yes) ¼ 0.87 (0.83, 0.91) for the 1-sided, and P(yes) ¼ 0.92 (0.89, 0.95) for the sparse context. RMSNFE ¼ 1.07. (D) The prior P(yes) for the three different contextual configurations. The error bars denote SEMs in (A-C), and 95% confidence intervals in (D). doi:10.1371/journal.pcbi.0040014.g006",
    "id": 266,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 266,
    "created_at": "2023-03-07T15:43:22.649485Z",
    "updated_at": "2023-03-07T15:43:22.649516Z",
    "lead_time": 5.044
  },
  {
    "corpusid": 129244480,
    "section": "25/39",
    "subsection": 1.0,
    "section_pos_in_pct": 0.641,
    "text": "Borehole Windermere 4 in Figure 14 (located in the northern part of the basin) provides an example of the mixed gas zone characteristics at around 600-700 m depth. While isotope **data** from this borehole are limited, the d 13 C of the CH 4 is -60.4% at 700 m depth. The same sample also contains 35.1% methane, 59.27% CO 2 , 3.65% ethane and 1.6% nitrogen. In other words, mostly biogenic methane occurs with deep CO 2 and nitrogen of unknown (but not air-contamination) origin, as evidenced by the trends shown in Figure 14. The pattern exhibited by the increasing ethane concentration curve strongly mirrors that of the CO 2 100 m above it and the nitrogen 50 m below it (Figure 14). This may be a further illustration of hydrochemical transitioning from less saline to more saline conditions changing the relative solubility of gases in the presence of CO 2 (cf. Boreham et al. 2011).",
    "id": 267,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 267,
    "created_at": "2023-03-07T15:43:27.749054Z",
    "updated_at": "2023-03-07T15:43:27.749091Z",
    "lead_time": 4.93
  },
  {
    "corpusid": 16212589,
    "section": "3/6",
    "subsection": 5.0,
    "section_pos_in_pct": 0.5,
    "text": "These observations can be made with standard analysis techniques, although they require processing large amounts of **data** with demonstrable reliability. Each minute of recording uses ~3 megabytes of storage. These files must be checked for baseline stability and extraneous channel activity, then processed to determine Na + channel amplitude, duration, and frequency. This task was made easier using mean-variance analysis, which condensed minutes of **data** into a small set of twodimensional mean-variance histograms. Such histograms display the quality of the data, and permit subsequent analysis of both open times and Po for components with defined amplitude. Use of this technique greatly facilitated obtaining **data** of the highest quantity and quality.",
    "id": 268,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 268,
    "created_at": "2023-03-07T15:43:34.157895Z",
    "updated_at": "2023-03-07T15:43:34.157921Z",
    "lead_time": 6.245
  },
  {
    "corpusid": 87345260,
    "section": "1/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.077,
    "text": "To evaluate the diversity of an area using phylogenies as a general frame, two main perspectives could be used, evolutionary distinctiveness (ED) or phylogenetic diversity ( PD ). Evolutionary distinctiveness refers to species-specifi c measures developed to assign scores to the species and therefore the areas they inhabit (Vane-Wright et al. 1991 ). The measures are topology -based indices, calculated as \"the sum of basic taxic weights, Q, and the sum of standardised taxic weights, W.\" (Schweiger et al. 2008 ), and therefore are also known as Taxonomic distinctiveness indices. Phylogenetic diversity (PD) is a distance-based index using minimum spanning path of the subset in the tree (Faith 1992 ). Redding et al. ( 2008 ) identifi ed some of the major differences between ED and PD. PD is effective only if all the species within the optimal subset are protected, otherwise other optimal subsets are possible; unlike ED, PD is not species-specifi c and thus does not offer priority species rankings, which are important to species conservation approaches as the IUCN Red List of Threatened Species. Furthermore, topologies are more stable than branch length s. Increasing the number of characters or changing the set of characters seldom leads to entire shifts in the relationships among species, whereas branch lengths change considerably from one set of characters to another and permit only to state about the evolution of the **data** set that generated the topology and the branch lengths (Brown et al. 2010 ).",
    "id": 269,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 269,
    "created_at": "2023-03-07T15:43:42.142874Z",
    "updated_at": "2023-03-07T15:43:42.142916Z",
    "lead_time": 7.803
  },
  {
    "corpusid": 207778550,
    "section": "20/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.417,
    "text": "This section presents several specific examples where the possibility of identifying the need for and an estimate of USU contributions is explored. These examples are all based on situations that incorporate actual nuclear **data** rather than artificial data. However, they sometimes treat isolated and/or truncated **data** sets for reactions that normally would be linked to several other reactions through measured ratios in an actual comprehensive, simultaneous evaluation. By this means, the examples introduced here have been simplified and reduced in scope in such a way as to make discussions of USU more transparent than generally would be the case for actual evaluations. In no instance should the results generated in these examples be interpreted as substituting for the results provided in existing evaluations.",
    "id": 270,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 270,
    "created_at": "2023-03-07T15:43:50.780888Z",
    "updated_at": "2023-03-07T15:43:50.780912Z",
    "lead_time": 8.449
  },
  {
    "corpusid": 6962515,
    "section": "7/22",
    "subsection": 1.0,
    "section_pos_in_pct": 0.318,
    "text": "We represented the thesaurus as two matrices where T syn is the synonym graph and T ant is the antonym graph. The signed graph can then be written in matrix form asŴ = γW + β ant T ant W +β syn T syn W , where computes Hadamard product (element-wise multiplication). The parameters γ, β syn , and β ant are tuned to the **data** target dataset using cross validation. The reader should note that σ and are not found using a target dataset, but instead using cross validation and grid search to minimize the number of negative edges within clusters and the number of disconnected components in the cluster.",
    "id": 271,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 271,
    "created_at": "2023-03-07T15:43:58.862886Z",
    "updated_at": "2023-03-07T15:43:58.862915Z",
    "lead_time": 7.902
  },
  {
    "corpusid": 249258070,
    "section": "91/119",
    "subsection": 3.0,
    "section_pos_in_pct": 0.765,
    "text": "The CFS **data** also captured proactive activities conducted by three distinct types of police units: patrol, specialized crime investigation, and the DRTs. Other than responding to calls, patrol officers also perform patrol duties, investigate suspicious activities, and form relationships with community members. Officers in specialized crime investigation divisions, such as robbery, vice, auto theft, gang, major assaults and family violence, and homicide divisions etc., mainly conduct investigations concerning specific type of offenses. Officers in DRT units do not respond to calls for service, instead, they proactively work with the community, focusing on problem solving and improving quality of life issues. These activities are all recorded as self-initiated activities within the CAD system.",
    "id": 272,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 272,
    "created_at": "2023-03-07T15:44:06.500298Z",
    "updated_at": "2023-03-07T15:44:06.500336Z",
    "lead_time": 7.462
  },
  {
    "corpusid": 207778550,
    "section": "40/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.833,
    "text": "\nonline) Estimated cross-section ratio as a function of the USU variance σ 2 δ . The dotted lines represent the ratio based on an unweighed average of the **data** in",
    "id": 273,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 273,
    "created_at": "2023-03-07T15:44:20.375500Z",
    "updated_at": "2023-03-07T15:44:20.375529Z",
    "lead_time": 13.705
  },
  {
    "corpusid": 119415579,
    "section": "8/41",
    "subsection": 4.0,
    "section_pos_in_pct": 0.195,
    "text": "After identifying Prot and obtaining σP rot , either through a period search analysis or from the literature, final 1 st and 2 nd order sinusoidal fits to each star's Bz (t) and Hp(t) measurements were derived (2 nd order fits were only derived for those **data** sets consisting of more than 5 **data** points). The epoch of each star was defined such that Bz (t0) = |C0 + C1| (i.e. the maximum, unsigned longitudinal field strength) while φ1 and φ2 were constrained such that C1 > 0 and C2 > 0. Note that the way in which the epoch is defined and the way in which C1 and C2 are constrained implies that, for the fits to Bz , φ1 = ±π/2 while φ2 is a free parameter; for the fits to Hp, both φ1 and φ2 are unrestricted free parameters.",
    "id": 274,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 274,
    "created_at": "2023-03-07T15:44:39.016800Z",
    "updated_at": "2023-03-07T15:44:39.016824Z",
    "lead_time": 18.412
  },
  {
    "corpusid": 118072666,
    "section": "10/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.769,
    "text": "FIG. 1 :\n1Top: Invariant mass distributions of candidates passing all selection requirements except that for the mass. The solid histogram shows candidates in the on-resonance **data** sample, while the dashed histogram shows the off-resonance sample, scaled to the luminosity of the on-resonance data.\n",
    "id": 275,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 275,
    "created_at": "2023-03-07T15:44:47.954473Z",
    "updated_at": "2023-03-07T15:44:47.954510Z",
    "lead_time": 8.713
  },
  {
    "corpusid": 207778550,
    "section": "24/48",
    "subsection": 4.0,
    "section_pos_in_pct": 0.5,
    "text": "The covariance matrix due to systematic effects for the three energy groups is given in Table VIII. The **data** in Table VIII reveal a relatively strong energy dependence for the uncertainty due to systematic effects. Such an energy dependence is difficult to explain from an experimental point of view, considering the relatively small energy region covered by the **data** points included in the analysis. The values in Table VIII are not consistent with those derived from a MLE analysis.",
    "id": 276,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 276,
    "created_at": "2023-03-07T15:45:35.324384Z",
    "updated_at": "2023-03-07T15:45:35.324414Z",
    "lead_time": 47.134
  },
  {
    "corpusid": 118490678,
    "section": "10/15",
    "subsection": 1.0,
    "section_pos_in_pct": 0.667,
    "text": "Figure 1 :\n1(Color online) Midrapidity p T spectra at 200 GeV for D 0 (D 0 ) meson, inclusive electron, nonphotonic electron, and muon. The solid (dashed) lines are from σ NN cc by STAR (PHENIX). D 0 **data** tagged with (CuCu) or (dAu) are obtained from Cu+Cu or d+Au collisions based on the binary scaling. Data are from Refs.[9,10,11,12,13].\n",
    "id": 277,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 277,
    "created_at": "2023-03-07T15:45:45.698538Z",
    "updated_at": "2023-03-07T15:45:45.698565Z",
    "lead_time": 10.162
  },
  {
    "corpusid": 119438874,
    "section": "4/6",
    "subsection": 1.0,
    "section_pos_in_pct": 0.667,
    "text": "The model proposed to describe the possible two-stage superconducting phase transition in 2D (and quasi-2D) metallic systems is in fact very simplified in order to investigate their most typical and general features. All the more surprisingly that it catches some essential details which are characteristic for underdoped HTSC copper oxides. In particular, the experimental **data** demonstrates [28,29] that i) indeed for low n f the critical temperature T c is proportional to n f (what is simply ǫ F ), ii) T c shows saturation when n f approaches so called \"optimal doping\" (i.e. carrier concentration when T c as function of n f reaches its highest possible in given compound value), iii) the ratio T c /ǫ F in these and other \"exotic\"",
    "id": 278,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 278,
    "created_at": "2023-03-07T15:46:17.127755Z",
    "updated_at": "2023-03-07T15:46:17.127786Z",
    "lead_time": 31.232
  },
  {
    "corpusid": 15294885,
    "section": "21/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.84,
    "text": "variable [ 13 ,\n13Definition 3.1] and by a relation between M(f ) andM (f ) shown in [17, Eq. (5)]. By using the phase φ(f ) ofM (f ), we can rewrite the complementary PSD asM (f ) = |M(f )|e jφ(f ) = k(f ) M(f )M(−f )e jφ(f ) , where 0 ≤ φ(f ) ≤ 2π. In the next lemma, the properties of the impropriety frequency and the phase functions are provided. Lemma 2: The impropriety frequency function k(f ) and the phase function φ(f ) satisfy 0 ≤ k(f ) ≤ 1, k(−f ) = k(f ), and φ(−f ) = φ(f ), ∀f. (17) Proof: Sincem[−k] =m[k] by definition, we haveM (−f ) =M (f ), which implies φ(−f ) = φ(f ). This also leads to k(−f ) = k(f ) by (16). By using the property |M(f )| 2 ≤ M(f )M(−f ) shown in [17, Eq. (5)], we have 0 ≤ k(f ) ≤ 1. ✷ For example, an uncorrelated real-valued PAM **data** sequence results in k(f ) = 1, ∀f , whereas any proper-complex **data** sequence results in k(f ) = 0, ∀f . By using the impropriety frequency function, we can rewrite the MSE (15) in the form of a function ofs(f ) as a function of s(f ).",
    "id": 279,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 279,
    "created_at": "2023-03-07T15:46:27.844505Z",
    "updated_at": "2023-03-07T15:46:27.844531Z",
    "lead_time": 10.532
  },
  {
    "corpusid": 207778550,
    "section": "14/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.292,
    "text": "If a collection of experimental **data** to be evaluated can be organized into two or more groups, based on distinct experimental techniques employed in the measurements, and if evaluation of these **data** groups separately, according to the various group characteristic experimental techniques, produces noticeable differences in both the evaluated mean values and uncertainties for the distinct groups, this is strong evidence for the existence of a method-related bias attributable to USU that must be taken into consideration by an increase in the evaluated uncertainties.",
    "id": 280,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 280,
    "created_at": "2023-03-07T15:47:06.521057Z",
    "updated_at": "2023-03-07T15:47:06.521080Z",
    "lead_time": 38.474
  },
  {
    "corpusid": 207778550,
    "section": "10/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.208,
    "text": "We should keep in mind that it's meaningless to discuss a systematic USU if only one experimental **data** point is available for a particular physical process. Furthermore, there is no possibility to identify a systematic USU effect if all the available **data** correspond to just one experiment, no matter the number of **data** points within it.",
    "id": 281,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 281,
    "created_at": "2023-03-07T15:47:21.452189Z",
    "updated_at": "2023-03-07T15:48:02.361636Z",
    "lead_time": 52.059999999999995
  },
  {
    "corpusid": 119479526,
    "section": "16/29",
    "subsection": 2.0,
    "section_pos_in_pct": 0.552,
    "text": "(16)). In the hyperparameter method, we initialize one hyperparameter for each of the three **data** sets.",
    "id": 282,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 282,
    "created_at": "2023-03-07T15:48:13.373761Z",
    "updated_at": "2023-03-07T15:48:13.373792Z",
    "lead_time": 7.937
  },
  {
    "corpusid": 235829209,
    "section": "47/47",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "\nthey represent conservative Hamiltonian systems, while the surrogate models established from **data** are usually not. The lack of symplecticity can result in artificial damping or excitation in long-term transformations. The established surrogate models should not be used for the DA computation. However, while constructing data-driven chaos indicators, surrogate models were only used to characterize the sensitivity of transformations to their initial conditions. Even if the transformations are not perfectly symplectic, it does not affect such applications. Due to the lack of a real physics model, this chaos indicator cannot replace the diffusion rate obtained with the FMA, which measures the regularity of resonant motions of a nonlinear dynamical system. Using this indicator as the optimization objective might not be as competitive as the direct tracking-based optimization if not taking the computation cost into account. This method\n",
    "id": 283,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 283,
    "created_at": "2023-03-07T15:48:41.348521Z",
    "updated_at": "2023-03-07T15:48:41.348545Z",
    "lead_time": 27.766
  },
  {
    "corpusid": 184487142,
    "section": "18/47",
    "subsection": 2.0,
    "section_pos_in_pct": 0.383,
    "text": "For both ImageNet and CIFAR-10, we trained the base classifier with random horizontal flips and random crops (in addition to the Gaussian **data** augmentation discussed in Section 3.2).",
    "id": 284,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 284,
    "created_at": "2023-03-07T15:48:48.272882Z",
    "updated_at": "2023-03-07T15:48:48.272906Z",
    "lead_time": 6.687
  },
  {
    "corpusid": 118926097,
    "section": "1/8",
    "subsection": 1.0,
    "section_pos_in_pct": 0.125,
    "text": "The goodness of the UE simulation provided by MC event generators and corresponding tunes can be tested by comparing predictions with available data. These **data** are generally measurements of the number of charged particles and their transverse momentum sum in different regions of the phase space relative to the direction of the hardest objects in the event. In particular, the hard object, which might be a jet, a charged particle or a Z boson, identifies a direction in the transverse plane. The transverse plane is then divided into four regions, according to their azimuthal angle: a \"toward\" and an \"away\" region sensitive to the hard scattering and its recoiling object, and two \"transverse\" regions, more sensitive to UE contributions. In recent measurements, the two transverse regions are further divided into separate measurements. The transverse region with the highest activity is called \"transMAX\" while the one with the smallest activity is labelled as \"transMIN\". The charged-particle multiplicity and the transverse momentum sum of the charged particles, measured as a function of the transverse momentum of the leading charged particle, are referred to as \"UE observables\" in the following.",
    "id": 285,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 285,
    "created_at": "2023-03-07T15:48:57.625214Z",
    "updated_at": "2023-03-07T15:48:57.625379Z",
    "lead_time": 9.159
  },
  {
    "corpusid": 1520137,
    "section": "2/35",
    "subsection": 1.0,
    "section_pos_in_pct": 0.057,
    "text": "We study how visual perception of a target bar can be biased by contextual bars in the image, and how a Bayesian model of object inference can account for the data. Human observers are more likely to perceive a target bar when the contextual contrast, i.e., the luminance difference between the contextual bars and background, is weaker rather than stronger. Relative to the situation without the context, they are biased to perceive the target in a context of weak contrast when the target can perceptually group well with the context, as if the context fills in the target. Meanwhile, they are biased not to perceive the target in a context of strong contrast, as if the context suppresses the perception, regardless of whether it could perceptually group well with the would-be target. The Bayesian model illustrates that the context influences the perception by biasing (1) observers' prior belief that a target should be present and (2) observers' internal model of the likely input contrasts from a target bar. Our **data** suggest that brain areas beyond the primary visual cortex along the visual pathway are responsible for inferring object causes for input images.",
    "id": 286,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 286,
    "created_at": "2023-03-07T15:49:02.568707Z",
    "updated_at": "2023-03-07T15:49:02.568741Z",
    "lead_time": 4.755
  },
  {
    "corpusid": 249258070,
    "section": "71/119",
    "subsection": 2.0,
    "section_pos_in_pct": 0.597,
    "text": "Collectively, there were ten categories of police reactive and proactive activities coded from the data, and these categories make up the dependent variables for the current study. Police reactivity to CFS were coded into seven different categories using an approach consistent with previous research utilizing CFS **data** (see Wu & Lum, 2017). Violent crimes included CFS involving reported shootings, robbery, and several types of assaults. Property offenses was a category comprised of burglary, theft, and forgery type offenses. Disorder incidents consisted of an array of disturbances, including general, family, and noise CFS. Suspicious incidents included calls where an alarm was sounded (e.g., vehicle alarm set off) and reports involving a suspicious person or vehicle. The trafficrelated activities category comprised CFS for traffic accidents, driving while intoxicated (DWI), and other traffic-related issues (i.e., road rage). Service-related activities included calls to assist first-responders, including other officers, fire fighters, or emergency medical services. Lastly, the non-crime events category consisted primarily of assisting specialized units with transporting individuals and responding to silent 911 calls.",
    "id": 287,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 287,
    "created_at": "2023-03-07T15:50:59.045814Z",
    "updated_at": "2023-03-07T15:50:59.045840Z",
    "lead_time": 116.289
  },
  {
    "corpusid": 5648146,
    "section": "9/18",
    "subsection": 2.0,
    "section_pos_in_pct": 0.5,
    "text": "Using mutual information [14,15], the time delay of each variable of the time series can be solved. Based on information theory, the mutual information between time series and its delay time series can be derived:\n( ) = ( , ) = ( ) + ( ) − ( , ) .(3)\nWe compute the mutual information ( ) by the histogrambased statistic estimator [15], and , which makes ( ) to the first local minimum point, is regarded as the delay time of th variable. By calculating time series **data** of all subjects, reasonable reconstructing time lag can be obtained (the average time lag of 36 subjects is ankle = 17 ± 7.3, knee = 21 ± 5.8, and hip = 19 ± 8.3).",
    "id": 288,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 288,
    "created_at": "2023-03-07T15:51:03.707783Z",
    "updated_at": "2023-03-07T15:51:03.707812Z",
    "lead_time": 4.44
  },
  {
    "corpusid": 119421228,
    "section": "1/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.091,
    "text": "Our theoretical model, described in Sec. II, consists of a diamond coated with a cover layer, which we also sometimes refer to as a \"protective layer\". We use the fluctuation-dissipation theorem to obtain noise spectra at room temperature, and we calculate the effective capacitance and loss tangent. The surface cover materials we consider are: glycerol, propylene carbonate (PC), polymethyl methacrylate (PMMA), polyvinylindene flouride (PVDF), perflouropolyether (PFPE), and dimethyl sulfoxide (DMSO). PMMA, PVDF, PFPE, glycerol, and PC have been commonly used in experimental work [48][49][50][51][52]. PMMA and PVDF are both solids, while all the others are liquid at room temperature. We analyze the noise spectra at frequencies ranging from 1 kHz − 10 MHz, typical in experiments, and compare the results with experimental **data** of Ref. [32].",
    "id": 289,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 289,
    "created_at": "2023-03-07T15:51:10.253032Z",
    "updated_at": "2023-03-07T15:51:10.253057Z",
    "lead_time": 6.345
  },
  {
    "corpusid": 207778550,
    "section": "5/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.104,
    "text": "In addition to the discussion in Sect. II, expositions on nuclear **data** evaluation methodology during preceding decades can be found in a monograph by Smith [3], in review papers by Capote et al. [22] and by Smith and Otuka [23], and elsewhere, e.g., as mentioned in a comprehensive report of the NEA WPEC SG-24 [24].",
    "id": 290,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 290,
    "created_at": "2023-03-07T15:51:18.116721Z",
    "updated_at": "2023-03-07T15:51:18.116750Z",
    "lead_time": 7.689
  },
  {
    "corpusid": 17837291,
    "section": "17/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.327,
    "text": "Since not all a q are zero and the functions e H ζ (q) are linearly independent, it follows that\nk is odd. Now the coefficient of α −k+1 becomes q e H ζ (q) (2b q − a q H ζ , d ) = 0.\nAgain by linear independence of the exponential functions, it follows that a q = 0 = b q for all q, which is a contradiction. \nA ′ d − B ′ d = X ω(α)e H ζ + ω(−α)e H ζ + H ζ ,d α .\nSince both A ′ d , B ′ d have coefficients which are polynomial in α, this shows that the class ω(α) has property (b) of the preceding lemma. Suppose A, B both come from Euler **data** Q, P respectively, ie. \nA d = i * 0 Q v d and B d = i * 0 P v d . Suppose also that j * 0 (P d )| q = j * 0 (Q d )| q at α = λ/δ.T q X. (ii) deg α (A d − B d ) ≤ −2.\nThen A = B.",
    "id": 291,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 291,
    "created_at": "2023-03-07T15:51:23.350932Z",
    "updated_at": "2023-03-07T15:51:23.350961Z",
    "lead_time": 5.047
  },
  {
    "corpusid": 118656364,
    "section": "25/51",
    "subsection": 1.0,
    "section_pos_in_pct": 0.49,
    "text": "In this section we study the simplest non-abelian case, corresponding to two M5-branes wrapped on S 2 , or equivalently we study the reduction of the 5d U (2) theory to 4d on an interval with Nahm pole boundary conditions. The flat 4d theory is given by a map into the 2-monopole moduli space M 2 , with the action given in (4.30). For the curved space theory we find a description in terms of a sigma-model into S 1 × R ≥0 supplemented by self-dual two-forms obeying some constraints. We provide a detailed analysis of the geometrical **data** 9 The factor i is due to our conventions in which A θ is purely imaginary. 10 These transformations correspond to gauge group elements g = e iα(θ) with α(0) = 0 and α(π) = 2πn. The quantization of n is required for g to be trivial at the endpoints of the θ interval.",
    "id": 292,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 292,
    "created_at": "2023-03-07T15:52:43.813104Z",
    "updated_at": "2023-03-07T15:52:43.813130Z",
    "lead_time": 80.282
  },
  {
    "corpusid": 55185767,
    "section": "9/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.562,
    "text": "We will now show that the subset M K,ψ,x,r of U K,x,r consisting of those y satisfying both inequalities (4.6.5) sup\nℓ,j,i |G ℓ,j,i,K,ψ (x, r)| C ≤ sup ℓ,j |f ℓ,j,K,ψ (x, y)| C ≤ sup ℓ |f ℓ,K,ψ (x, y)| C\nhas big volume in the sense that (4.6.6) Vol(U K,x,r ) ≤ q d+1 K Vol(M K,ψ,x,r ). Once this is proved, we are done for our original family (f ℓ ) ℓ by replacing d with d + 1 while keeping the **data** of the ϕ, G ℓ,j,i , and h ℓ,j,i .",
    "id": 293,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 293,
    "created_at": "2023-03-07T15:53:00.556715Z",
    "updated_at": "2023-03-07T15:53:00.556749Z",
    "lead_time": 16.558
  },
  {
    "corpusid": 235125570,
    "section": "19/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.655,
    "text": "\n1.4)  in 1D with the initial **data** Φ 0 (x) = (cos(x), cos(x)) T , electromagnetic potentials V (x) = 1/(1 + sin(x) 2 ) and A 1 (x) = (1 + sin(x))/(2 + cos(4x)). The same mesh size h = π/8 and time step τ = 10\n",
    "id": 294,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 294,
    "created_at": "2023-03-07T15:53:08.496361Z",
    "updated_at": "2023-03-07T15:53:08.496387Z",
    "lead_time": 7.758
  },
  {
    "corpusid": 38256305,
    "section": "2/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.041,
    "text": "Pore pressure diffusion is considered to play a key role in controlling the occurrence of earthquakes associated with Koyna and Warna reservoir [8][9][10][11][12][13][14][15][16][17][18][19]. Hence, the prevalent fault structure and tectonic setting in the region that influence seismicity need further investigation. The detailed inventory preparation of the near-surface fault and fracture pattern is likely to help detect relatively high permeable areas that allow intrusion and infiltration of surface water. Towards this end, satellite **data** were harnessed to detect near-surface fault and fracture zones and to understand the structural pattern via visual lineament analysis. The Sentinel satellite radar images available since 2015 from ESA are assumed to delineate sub-surface structures whose subtle morphological details are enhanced, which were earlier lacking in detail, due to radar backscatter of the surface. Landsat 8 **data** available since 2013 are also seen to show improvement when compared to previous Landsat missions because of enhanced radiometric and thermal resolutions. One of the goals of the present paper was to investigate whether additional structural/tectonic information can be gleaned from these newly-available satellite data.",
    "id": 295,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 295,
    "created_at": "2023-03-07T15:58:57.586122Z",
    "updated_at": "2023-03-07T15:58:57.586154Z",
    "lead_time": 348.916
  },
  {
    "corpusid": 248858280,
    "section": "22/24",
    "subsection": 1.0,
    "section_pos_in_pct": 0.917,
    "text": "Fig 11 .\n11A: Cyclovoltammograms of propolis when scavenging the superoxide radical, B: Collection efficiency of propolis; linear behavior considering all runs. The last run shows almost complete elimination of superoxide; C: Linear behavior of propolis considering the first 4 **data** on Fig 11A.",
    "id": 296,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 296,
    "created_at": "2023-03-07T15:59:01.147701Z",
    "updated_at": "2023-03-07T15:59:01.147728Z",
    "lead_time": 3.39
  },
  {
    "corpusid": 17837291,
    "section": "32/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.615,
    "text": "Consider the following example: the concave bundle V = O(−6) over P 3,2,1 , Ω = 1 6H . This example will be studied in our subsequent paper by using resolution of singularities. This is an example of \"local mirror symmetry\" studied in physics [33]. The mirror formula there can derived as a special case of our general result. In fact, the Euler **data** which computes the K d in this case is determined by\nj * 0 P d = 6d−1 m=1 (−6H + mα).\nThe corresponding equivariant Euler class, after taking nonequivariant limit with respect to the T action, is:\ne G (X 0 /W d ) = d m=1 (H − mα) 2d m=1 (2H − mα) 3d m=1 (3H − mα).\nThe corresponding hypergeometric series and Picard-Fuchs equation can be immediately written down. It turns out that the hypergeometric series gives the periods of a meromorphic 1-form for a family of elliptic curves [33].",
    "id": 297,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 297,
    "created_at": "2023-03-07T15:59:14.611827Z",
    "updated_at": "2023-03-07T15:59:14.611858Z",
    "lead_time": 13.288
  },
  {
    "corpusid": 201251303,
    "section": "2/10",
    "subsection": 2.0,
    "section_pos_in_pct": 0.2,
    "text": "Step 1. Generate **data** with = 500",
    "id": 298,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 298,
    "created_at": "2023-03-07T15:59:18.503651Z",
    "updated_at": "2023-03-07T15:59:18.503677Z",
    "lead_time": 3.702
  },
  {
    "corpusid": 17228594,
    "section": "3/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.429,
    "text": "From the P * P correlator one can extract the static quark-antiquark potential V (r) by V (r) = − 1 T ln P * P(r) . We prefer to look at the first and the second derivative of this potential for the force between the quark and the antiquark and information about sub-leading terms. To facilitate our comparison with string models we actually compute a scaled second derivative which we call c(r). This quantity is expected to become the Lüscher term (= − (d−2)π 24 ) asymptotically. On the lattice these are given by\nF(r) = V (r) −V (r − 1) & c(r) =r 3 2 [V (r + 1) +V (r − 1) − 2V (r)] (3.1) wherer = r + a 2 + O(a 2 ) andr = r + O(a 2 )\nare defined as in [9] to reduce lattice artifacts. For large distances, we are going to concentrate on the potential due to the NG string, the so called Arvis potential [16] given by V Arvis \n= σ r 1 − (d−2)π 12σ r 2 1/2\n. We will define the L.O. and N.L.O. approximations by retaining 1/r and 1/r 3 terms in the potential respectively. We will compare our lattice **data** on force and c(r) with predictions from leading order, NLO and from the full Arvis potential.",
    "id": 299,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 299,
    "created_at": "2023-03-07T15:59:30.237340Z",
    "updated_at": "2023-03-07T15:59:30.237399Z",
    "lead_time": 11.415
  },
  {
    "corpusid": 118661060,
    "section": "4/17",
    "subsection": 1.0,
    "section_pos_in_pct": 0.235,
    "text": "Much more remarkable in the figure is qualitative differences between the ONe core and the Fe cores. Among other things, the Fe cores emit a substantial amount of neutrinos from much earlier on than the ONe core. This is simply due to their higher temperatures as mentioned earlier. The electron-positron pairs are more abundant, producing neutrinos copiously. In fact, the ONe core is so cold until the electron capture on Mg is opened that electrons are strongly degenerate and, as a consequence, positrons are scarce, making the neutrino luminosity from the pair annihilation negligibly small. Unfortunately, the plasmon contribution is also tiny as long as the temperature is very low, since only a small amount of plasmons are thermally populated then. It is hence understandable that the neutrino luminosity becomes substantial in the ONe core only after the electron captures and the subsequent burnings of oxygen and silicon heat it up, which occur less than a second before collapse. Even after the formation of the NSE region (T 5 × 10 9 K) inside the core, the number luminosity of the ONe core is much smaller than those of the Fe cores. As a matter of fact, ∼ 10 53 neutrinos are emitted in the 15 M ⊙ model while in the 8.4 M ⊙ model the number of emitted neutrinos is only ∼ 10 51 . Note, however, that the luminosity increases very quickly in the last few hundred milliseconds in the ONe core and It is still increasing rapidly at the end of our calculation. At this point the central density is 10 11 g/cm 3 and no more **data** are available from the stellar evolution calculation. Unlike the Fe cores, the ONe core is rather slowly contracting even at this point, with the NSE region being expanding gradually. We hence expect the luminosity continues to rise further until neutrinos are trapped inside the core, which will be somewhat delayed, since the neutrinos considered here have smaller energies (∼ 5 MeV) than those produced by electron captures during collapse (∼ 10 MeV).",
    "id": 300,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 300,
    "created_at": "2023-03-07T16:02:48.400462Z",
    "updated_at": "2023-03-07T16:02:48.400489Z",
    "lead_time": 197.972
  },
  {
    "corpusid": 17671315,
    "section": "2/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.062,
    "text": "In publication [24] the **data** of the current paper were successfully treated in the most challenging case: the case of blind data. To do this, the newly developed the so-called approximately globally convergent numerical method (AGCM) was applied. Since these experi-mental **data** are the one-dimensional ones, then it is possible to use them for a comparison of numerical performances of the classical Gelfand-Levitan-Krein integral equation method (GLK) and AGCM. \"Blind\" means that first the **data** were delivered to the mathematical group by engineers of ARL, Sullivan and Nguyen, who coauthor both this paper and [24]. No information about targets was given to mathematicians, except their placements either above or below the ground. Next, the mathematical group made computations by AGCM and delivered results to engineers. Finally, engineers compared computational results with the truth and informed mathematicians about their findings, see Table 1 and Conclusion in subsection 3.5. The most important observation was that in all available sets of experimental **data** the values of computed dielectric constants of targets were well within limits tabulated in [32,33]. Since dielectric constants were not measured in experiments, then this answer was a quite satisfactory one.",
    "id": 301,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 301,
    "created_at": "2023-03-07T16:03:18.497635Z",
    "updated_at": "2023-03-07T16:03:18.497663Z",
    "lead_time": 29.913
  },
  {
    "corpusid": 207778550,
    "section": "9/48",
    "subsection": 3.0,
    "section_pos_in_pct": 0.188,
    "text": "The assumption was made [52] that the unrecognized systematic uncertainty is not energy dependent. While this approximation was valid for **data** partially evaluated by the R-matrix theory 1 , where the normalization of experimental **data** relied on the unitarity of the formalism, the situation is different for other cross sections of neutroninduced reactions on heavy elements. In the latter case the approximation of USU as an energy independent quantity may be questionable, but it was introduced in the standards evaluation as a first approximation toward deriving more realistic uncertainty estimates.",
    "id": 302,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 302,
    "created_at": "2023-03-07T16:03:28.630480Z",
    "updated_at": "2023-03-07T16:03:28.630519Z",
    "lead_time": 9.946
  },
  {
    "corpusid": 14912634,
    "section": "13/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.565,
    "text": "The utility of SODAR measurements for understanding power performance is more apparent when considering turbulent intensity. Power curves for Turbine 1 were stratified by nacelle cup anemometer I U and SODAR I U in figure 5. Stratification by nacelle I U (figure 5(a)) included stable or strongly stable periods (I U < 10%) and convective or strongly convective periods (I U > 13%). Too few **data** points were available to isolate the effects of strongly convective conditions for the nacelle-based parameter. Observed power yields followed the expected power curve regardless of stability class. Power differences were less than 5% and occurred when the nacelle 'true-flux' equivalent wind speed was between 4 and 7 m s −1 .",
    "id": 303,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 303,
    "created_at": "2023-03-07T16:03:41.031452Z",
    "updated_at": "2023-03-07T16:03:41.031475Z",
    "lead_time": 5.347
  },
  {
    "corpusid": 38256305,
    "section": "5/49",
    "subsection": 2.0,
    "section_pos_in_pct": 0.102,
    "text": "It is often difficult to identify faults and lineaments in the field, because of a plethora of geological conditions, like sedimentary cover, erosion, vegetation over-growth, scale and other factors, including the experience of the geologist or prohibitive costs for manned surveys. Accessibility is also another limiting factor. Because of all of these shortcomings, RS **data** are a valid source for getting accurate information of Earth surface features and land use/land cover patterns.",
    "id": 304,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 304,
    "created_at": "2023-03-07T16:03:44.781770Z",
    "updated_at": "2023-03-07T16:03:44.781798Z",
    "lead_time": 3.527
  },
  {
    "corpusid": 18538029,
    "section": "1/9",
    "subsection": 1.0,
    "section_pos_in_pct": 0.111,
    "text": "The Hubble expansion, the Cosmic Microwave Background Radiation(CMBR) and the primordial Big Bang Nucleosynthesis(BBN) are three cornerstones of the standard hot Big Bang cosmological model. Recently these three kinds of observations have been making a strong case for the existence of a nearly uniform component of dark energy with negative pressure: More than twenty experiments of CMBR measurements have now covered three orders of magnitude in multipole number, and are beginning to define the position of the first acoustic peak at a value that is consistent with a flat universe [1]; However, the deuterium abundance measured in four high redshift hydrogen clouds seen in absorption against distant quasars [2] combined with baryon fraction in galaxy clusters from X-ray **data** [3] give a low matter density universe, Ω m ∼ 0. 3-0.4; The discrepancy between the low matter density and a flat universe is observationally resolved by high redshift Ia type supernovae (SNeIa) observations [4], which implies an accelerating universe driven by a dark, exotic form of energy component. For a recent review of observational evidence for the dark energy component, reader is referred to Ref. [5]. It seems that determining the amount and nature of the dark energy is emerging as one of the most important challenges in cosmology.",
    "id": 305,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 305,
    "created_at": "2023-03-07T16:03:53.181725Z",
    "updated_at": "2023-03-07T16:03:53.181764Z",
    "lead_time": 8.193
  },
  {
    "corpusid": 126349038,
    "section": "14/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "FSV at x ¼ 0 . 2 .\n02The b ¼ 10 deg **data** showed similar vortices, except the SDTV strength reduced, both BKTV and ABKV strengths increased, TKE decreased for all the vortex cores, and FSVs were absent. Additionally, SDTV and BKTV display circular cross plane streamlines with normalized cross plane profiles showing Gaussian/bell-shaped similarity.As shown in Figs. 7(b)-7(d), the S2, S4, and S6 results show all the primary vortices observed in the experiments, and some additional secondary vortices. Solutions have some similarities and some dissimilarities for the prediction of secondary vortices. Experiments could not confirm the secondary vortices. S2 exhibits helical mode instability for SDTV and BKTV. S4 and S6 did not investigate the instantaneous solution to identify such instability.\n",
    "id": 306,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 306,
    "created_at": "2023-03-07T16:03:56.912145Z",
    "updated_at": "2023-03-07T16:03:56.912180Z",
    "lead_time": 3.527
  },
  {
    "corpusid": 207778550,
    "section": "26/48",
    "subsection": 3.0,
    "section_pos_in_pct": 0.542,
    "text": "The large international effort devoted to the project of evaluating, documenting, and cataloging integral **data** in readily accessible, machine-usable formats is usually justified on the grounds that system simulations are likely to continue to play an ever increasing role in future nuclear technology development. Additionally, it is assumed that integral data, which still tend to be considered as more accurate, on average, than differential data, will continue to be used to guide differential **data** development or to adjust differential **data** to specific applications.",
    "id": 307,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 307,
    "created_at": "2023-03-07T16:04:11.875052Z",
    "updated_at": "2023-03-07T16:04:11.875086Z",
    "lead_time": 14.76
  },
  {
    "corpusid": 16542351,
    "section": "3/8",
    "subsection": 1.0,
    "section_pos_in_pct": 0.375,
    "text": "In thi s paper,Ihave revi ewed di erent strategi es for detecti ng b pol ari zati on e ects through l epton energy spectra i n the i ncl usi ve sem i l eptoni c decays ofb hadrons. I have show n that,i fone concentrate on el ectron and m uon spectra i n the i ncl usi ve b hadrons decay, one has to deal w i th l arge theoreti cal uncertai nti es com i ng from the onl y-parti alknow l edge of the b fragm entati on functi on. A strategy to control these am bi gui ti es that m akes use ofboth LEP and l ower-energy PET R A **data** has been outl i ned. By com pari ng theoreti cal predi cti ons w i th experi m ental data, one can al ready excl ude l arge pol ari zati on e ectsi n the LEP i ncl usi ve l eptoni c spectrum . M uch m ore can be done w i th l arge stati sti cs on the b baryon sam pl e,by restri cti ng the pol ari zati on study to b baryons. In fact,the correspondi ng P e ect shoul d be about an order ofm agni tude l arger than i n the i ncl usi ve hadroni c spectrum ,si nce (contrary to b baryons) b m eson,w hi ch m ake up m ost ofthe hadroni c sam pl e,are expected notto retai n any ori gi nalspi n i nform ati on.In pri nci pl e,w hen restri cti ng to b sam pl es,a real i sti c m odel l i ng ofexcl usi ve decays i s requi red. It has been show n that, i f one can study al so neutri no spectra w i th good accuracy, the new vari abl e y = hE ' i=hE ii sal m ostfree oftheoreti caluncertai nti esand opti m i zesthe sensi ti vi ty to P ,due al so to the very good sensi ti vi ty ofneutri no spectra.",
    "id": 308,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 308,
    "created_at": "2023-03-07T16:04:23.455630Z",
    "updated_at": "2023-03-07T16:04:23.455656Z",
    "lead_time": 11.392
  },
  {
    "corpusid": 29602952,
    "section": "3/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.158,
    "text": "Suppose there are n families selected through ascertainment sampling. Letting the k th ascertained family have s k siblings, we assume that there are r k affected and ak ascertained. In Crow's **data** s k vary from 1 to 10. The simplest ascertainment model specifies that   k = 1, . . . , n. This is the basic ignorable selection model. The a k are really covariates, and this leads to improved precision. Thus, the joint probability mass function of (a k , r k ) is\n( , | , ) (1 ) (1 ) , k k k k k k k k r s r a r a k k k k s r p a r p p p r a π π π − −     = − −        (1)\na k = 0,...,r k , r k = 0,...,s k , k= 1,...,n. Note that (1) provides the likelihood for any family without conditioning on whether it is ascertained or not.",
    "id": 309,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 309,
    "created_at": "2023-03-07T16:04:31.435907Z",
    "updated_at": "2023-03-07T16:04:31.435932Z",
    "lead_time": 7.778
  },
  {
    "corpusid": 119144624,
    "section": "12/21",
    "subsection": 1.0,
    "section_pos_in_pct": 0.571,
    "text": "Our construction of an integrable system uses Anderson's toric degeneration as a key ingredient, so in this section we both review and slightly expand on his exposition in [An10]. First we recall how the semigroup algebra C[S] of R is related to the **data** of the extended valuationṽ. The link comes from a filtration of R defined usingṽ as follows. Let\nR ≥(m,u) := {f ∈ R |ṽ(f ) ≥ (m, u) or f = 0}\nand similarly for R >(m,u) . By the definition of the valuation v and the definition of the order (7.3) on N × Z n , each R ≥(m,u) (and R >(m,u) ) is a (finite-dimensional) vector subspace of R. Moreover, again by properties of valuations, for any (m, u) and (m ′ , u ′ ), we have\nR ≥(m,u) · R ≥(m ′ ,u ′ ) ⊆ R ≥(m+m ′ ,u+u ′ ) .\nWe can now define the associated graded ring grR with respect to this filtration as\ngrR := (m,u) R ≥(m,u) /R >(m,u) .\nThis is naturally an S-graded ring. In fact, sinceṽ has one-dimensional leaves, the spaces R ≥(m,u) /R >(m,u) have dimension 1 precisely when (m, u) ∈ S, and is 0 otherwise. Since the homogeneous elements of grR are not zero divisors, one shows that grR is isomorphic to the semigroup algebra C[S] (see [BG09,Remark 4.13]).",
    "id": 310,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 310,
    "created_at": "2023-03-07T16:04:43.348620Z",
    "updated_at": "2023-03-07T16:04:43.348652Z",
    "lead_time": 11.673
  },
  {
    "corpusid": 119080499,
    "section": "18/30",
    "subsection": 1.0,
    "section_pos_in_pct": 0.6,
    "text": "We performed a finite size calculation with lattice size of 15 × 15 × 15a 3 0 based on the model Hamiltonian (17). Numbers of states in the band structure of LaH 3 and LaH 2 are counted as a function of energy. The discrete densities of states were smoothed out by replacing delta peaks with Lorentzian distribution curve of width Γ = 0.3 eV. Our results for LaH 2 and LaH 3 are similar to the previous theoretical calculations by Gupta and Burger [36,38]. In Fig. 15, the pronounced two-peak structure is associated with the flat region of hydrogen bands near symmetry points X, L, K (refer to the band structure Fig. 9 and 10) while the small bump at the zero energy in LaH 2 is associated with the La band near symmetry points W and K. Experiments showed that the small bump shrinks as hydrogen concentration increases and eventually disappears when the sample approaches the trihydride. However Gupta and Burger expected a metallic state for LaH 3 from their LDA calculation which ignored electron correlation. In our calculations, the density of states goes to zero at −2.2 eV, indicating an insulating behavior for LaH 3 . However, the calculated widths for LaH 2 and LaH 3 are considerably smaller than the experiment results, a discrepancy similar to that in the LDA . The experimental **data** also shows a shift of the lowest energy peak when the concentration increases. The positions of calculated peaks do not significantly depend on the concentration and does not agree to the experiment. The origin of these discrepancies is not clear. Since our theory uses some parameters extracted from LDA, similar features except for the band gap may be expected from both theories.",
    "id": 311,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 311,
    "created_at": "2023-03-07T16:04:52.314148Z",
    "updated_at": "2023-03-07T16:04:52.314174Z",
    "lead_time": 8.737
  },
  {
    "corpusid": 209947757,
    "section": "4/18",
    "subsection": 2.0,
    "section_pos_in_pct": 0.222,
    "text": "In opposition of the free-free tests, electrodynamic long stroke shakers are incorporated for the purpose of excitation. To achieve a decent response of the structure, slow-paced logarithmic sine upsweeps of .   5 and edgewise force input, respectively. The positions have been selected with respect to exploitation of the full force capacity of the shakers on the one hand, and their maximum peak-to-peak travel on the other hand. Shaker positioning is realised with the help of heavyweight support platforms (see Figure 2). A less powerful shaker with ±2.5 cm travel is applied edgewise at the height of 6 metres. Large orthogonal displacements occur at the driving point when the sweep signal passes eigenfrequencies of flapwise bending modes. This is the reason for customising the excitation signal so as to ramp down the force amplitude at these frequencies (cf. [13]). In-phase multi-point excitation has been carried out for flapwise excitation solely. In that case a second shaker of equal type is attached to the blade shell at the same length coordinate but shifted 0.9 m towards the trailing edge. Using Single Virtual Driving Point (SVDP) processing, **data** from correlated multi-point swept sine excitation is stored directly into single columns of frequency response functions (FRFs) [14]. These FRFs do not deviate from those generated in single-point excitation runs but they are related to a virtual force that acts on a virtual, physically non-existent driving point. The method of SVDP processing is stated on the concept of equivalent complex powers, either generated from two correlated shakers or the virtual force acting on the virtual driving point.",
    "id": 312,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 312,
    "created_at": "2023-03-07T16:05:01.327154Z",
    "updated_at": "2023-03-07T16:05:01.327187Z",
    "lead_time": 8.815
  },
  {
    "corpusid": 17837291,
    "section": "15/52",
    "subsection": 2.0,
    "section_pos_in_pct": 0.288,
    "text": "Example 0. In the last section we have proved, using the gluing identity, that the **data** Q : Q d = ϕ ! (π * b T (V d )) associated with a mixed bundle V and a multiplicative class b T satisfies the Euler **data** identity. This indicates that the gluing identity is really the geometric origin of Euler data. This is what motivates our definition of Euler data. Note that since Q d is the equivariant push-forward of a class in H * G (M d (X)), the polynomial condition on Q is automatic. This condition will be needed when mirror transformation is discussed.  \nP d = − c 1 (L),d −1 k=1 (L + kα)\nis an Ω-Euler **data** where Ω = c 1 (L) −1 .",
    "id": 313,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 313,
    "created_at": "2023-03-07T16:05:15.584544Z",
    "updated_at": "2023-03-07T16:05:15.584703Z",
    "lead_time": 14.059
  },
  {
    "corpusid": 250549291,
    "section": "10/55",
    "subsection": 1.0,
    "section_pos_in_pct": 0.182,
    "text": "The grey model, which is constructed from the grey system theory, focuses on the insufficient information available, or the uncertainty of information that is [29]. It has been extensively utilized in the industries of finance, economics and the quantification of construction waste generation [30]. A major advantage of this method is that it can help predict problems with less **data** [29], which is extremely suitable for the current study. As the core of grey theory, the grey model (1,1) is a first-order linear differential equation of a single variable selected to forecast the generation amount of construction waste in Shanghai City in the next 10 years.",
    "id": 314,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 314,
    "created_at": "2023-03-07T16:05:24.286813Z",
    "updated_at": "2023-03-07T16:05:24.286847Z",
    "lead_time": 8.517
  },
  {
    "corpusid": 238834501,
    "section": "11/19",
    "subsection": 3.0,
    "section_pos_in_pct": 0.579,
    "text": "We use the lasso as a regression method with subset selection (for both the 'average' and the 'smoothing' type); the predictors are selected by including an ℓ 1 penalty on the vector of regression weights during the loss minimization, which effectively sets many of the weights to zero 54 . For comparison, we evaluate unweighted averages over N days similar to the eclipse day ('N-averages' from now on), as well as standard smoothing procedures (local regression, in both linear 5 and quadratic variants). Similarity between predictor and target days for the N-average is assessed as the pairwise Euclidean distance in the space spanned by times t • . Uncertainties for all methods can be quantified empirically by estimating the values for all t ∈ t • on all days d ∈ d • and subtracting the corresponding actual observations. This gives an error distribution for each time t ∈ t • . For the smoothing regression-the only method where the quantity of interest to us is actually the target-care needs to be taken to separate training and test data. To this end, we use four-fold cross-validation, so that approximately 1 year of **data** is in the test set at any time, and we calculate the error for a day only if it is in a test set. Figures 1 and 2 show the regression-based estimates and those obtained from comparison methods, respectively, for air temperature on the day of the eclipse at a few selected stations in and around La Serena. The box plots show the temperature depression T of the observation relative to the reference estimate. The plot elements representing uncertainty (shaded bands, boxes, whiskers and fliers) are constructed by adding the error distributions computed as described above to the point estimate. The regression methods' uncertainty is much reduced relative to that of the comparison methods; some but not all estimates of T fall outside of 95% of the calculated error distributions.",
    "id": 315,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 315,
    "created_at": "2023-03-07T16:06:46.620428Z",
    "updated_at": "2023-03-07T16:06:46.620454Z",
    "lead_time": 82.112
  },
  {
    "corpusid": 16212589,
    "section": "6/6",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "FmUgE 5 .\n5(A) Voltage-dependence of the steady state open probability of IFM1303QQQ as derived from mean-variance analyses. The solid line is fit to the most hyperpolarized **data** points with a steepness factor of e-fold per 2.2 mV. The filled squares are derived from one patch using measurements at 10-mV intervals to span the full range of probabilities. The open squares are from another patch, sampling the steep region of the curve at 5-mV intervals. (Due to the variability in activation potential from one cell to the next, the open squares have all been shifted by -10 mV so that they match the voltage range observed in the first patch.)\n",
    "id": 316,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 316,
    "created_at": "2023-03-07T16:07:19.286542Z",
    "updated_at": "2023-03-07T16:07:19.286574Z",
    "lead_time": 32.434
  },
  {
    "corpusid": 53380409,
    "section": "4/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.034,
    "text": "The **data** sample was collected by the ATLAS detector during the pp collision running of the LHC at ffiffi ffi s p ¼ 13 TeV in 2015 and 2016. Events were selected for the different channels with various triggers, as described in their respective papers [9-18]. Channels featuring charged or neutral leptons were selected with single or multiple electron and muon triggers with various p T thresholds and isolation requirements or with missing transverse momentum triggers with varying thresholds. A high-p T jet trigger was used in the fully hadronic channels. After requiring that the **data** were collected during stable beam conditions and with a functional detector, the integrated luminosity amounts to 36.1 fb −1 .",
    "id": 317,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 317,
    "created_at": "2023-03-07T16:07:26.023793Z",
    "updated_at": "2023-03-07T16:07:26.023821Z",
    "lead_time": 6.52
  },
  {
    "corpusid": 126349038,
    "section": "24/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.857,
    "text": "Fig. 12\n12Contours of (a) axial velocity and crossflow streamlines, and (b) TKE at x 5 0.4 for b 5 20 deg. Subplots show: experimental **data** (left), S2 (middle), and S6 (right).",
    "id": 318,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 318,
    "created_at": "2023-03-07T16:07:30.538356Z",
    "updated_at": "2023-03-07T16:07:30.538382Z",
    "lead_time": 4.329
  },
  {
    "corpusid": 29602952,
    "section": "1/19",
    "subsection": 2.0,
    "section_pos_in_pct": 0.053,
    "text": "When all families with affected offspring are ascertained, we say that there is complete ascertainment; otherwise there is incomplete ascertainment and in this case (unknown to the investigator) there are families with affected siblings who are not probands. When there is complete ascertainment, the proband probability is one; otherwise it is distinctly less than one. Fisher [2] first analyzed the **data** using complete ascertainment. His analysis was done using a truncated Binomial distribution. However, Fisher [2] also described a simpler method for the more appropriate incomplete ascertainment for these data. This discussion was further developed by Bailey [7] and Morton [8]. In this paper, we will focus on incomplete ascertainment as is evident in Table  1. Crow [3] pointed out for the cystic fibrosis **data** the need to adjust for ascertainment bias and incomplete ascertainment.",
    "id": 319,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 319,
    "created_at": "2023-03-07T16:07:40.071658Z",
    "updated_at": "2023-03-07T16:07:40.071685Z",
    "lead_time": 9.347
  },
  {
    "corpusid": 235125570,
    "section": "15/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.517,
    "text": "In the numerical simulations, we take d = 1 and the electromagnetic potentials to be\nV (t, x) = 1 − x 1 + x 2 , A 1 (t, x) = (x + 1) 2 1 + x 2 , x ∈ Ω ε ,(5.5)\nwe further take the initial **data** as [37,47] \nφ 1 (0, x) = 1 2 e −4x 2 e iS0(x) 1 + 1 + S ′ 0 (x) 2 , φ 2 (0, x) = 1 2 e −4x 2 e iS0(x) S ′ 0 (x), x ∈ Ω ε ,(5.with S 0 (x) = 1 40 (1 + cos(2πx)) , x ∈ Ω ε . (5.7)\nThe problem is solved numerically on the ε-dependent interval Ω ε = (−7 − T 0 /ε, 7 + T 0 /ε) with periodic boundary conditions. The 'reference' solution Φ(t, x) = (φ 1 (t, x), φ 2 (t, x)) T is obtained numerically by the TSFP method with a very fine mesh size and a very small time step, i.e., h e = 1/512 and τ e = 10 −4 . The errors are displayed at t = 1/ε. From Table 5.6, we can observe second order convergence in space and time for the CNFD method to solve the whole space problem (cf. first row in Table 5.6). The ε-resolution of the CNFD method is h = O(ε 1/2 ) and τ = O(ε 1/2 ), which is verified through the upper triangle parts of the table above the bold diagonal line. Tables 5.7 and 5.8 display the errors of the total probability e ρ (t = 1/ε) and the current density e J (t = 1/ε) by the CNFD method, which coincide with our error analysis again. The numerical results of the other three FDTD methods are similar, and we omit them for brevity.",
    "id": 320,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 320,
    "created_at": "2023-03-07T16:07:56.668598Z",
    "updated_at": "2023-03-07T16:07:56.668628Z",
    "lead_time": 16.418
  },
  {
    "corpusid": 52910603,
    "section": "25/27",
    "subsection": 1.0,
    "section_pos_in_pct": 0.926,
    "text": "\n) and laser-off (trials) as well as the respective indifference point for each condition. (B), Transitivity plots are shown for the blocked (left) and patent (right) fiber types. Transitivity measures represent the consistency of food pellet preferences across six sessions in which animals experienced each of the possible pairs of three pellets (A-C,A-B,B-C) for both fiber-type conditions. Stable and consistent preferences should be in accordance with the relation: IP A:C » IP A:B Â IP B:C which can be visualized in the scatterplots as points falling close to the identity line. The Y-and X-axes show the IP, in log scale, for the pellet pair with the largest preference difference, IP A:C , and the product of the IPs from the other two pairs (IP A:B Â IP B:C ). The within-session pairs of laser-on trials (yellow symbols) and laser-off trials (grey symbols) are indicated by a dark grey line connecting the scatterplot pairs. Histograms comparing laser-on and -off trials are shown in the background for each of the plots. The six sessions shown in (A) are marked with a red outline on the respective scatter points in (B) Data is provided in Supplementary file 3 andFigure 5-source **data** 1. (C) The distances between within-session pairs (the lengths of the grey lines connecting the scatterplot pairs in (B) plotted as the cumulative empirical distribution function for each of the fiber-type conditions (blocked: yellow, patent: brown). For **data** seeFigure 5-source **data** 1. DOI: https://doi.org/10.7554/eLife.38963.011The following source **data** is available for figure 5:Source **data** 1. Text File Containing the Source Data forFigure 5with the Same Layout as in Supplementary file 3. DOI: https://doi.org/10.7554/eLife.38963.012\n",
    "id": 321,
    "sentiment": "data availability statement",
    "annotator": 1,
    "annotation_id": 321,
    "created_at": "2023-03-07T16:08:09.241076Z",
    "updated_at": "2023-03-07T16:08:09.241102Z",
    "lead_time": 12.377
  },
  {
    "corpusid": 236603431,
    "section": "11/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.44,
    "text": "Recently, some philosophers have provided renewed defences of the cosmological fine-tuning argument for God's existence (FTA). The ethos of these defences is well-summarized by Hawthorne and Isaacs (2018: 136), by claiming that FTA is 'as legitimate an argument as one comes across in philosophy'. This paper has called the above claim into question. As I have argued, there is no reason to think that FTA can establish even a modest claim about evidential favouring-the claim that the fine-tuning **data** favours (even slightly) the designer hypothesis over its alternatives.",
    "id": 322,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 322,
    "created_at": "2023-03-07T16:08:28.136673Z",
    "updated_at": "2023-03-07T16:08:28.136700Z",
    "lead_time": 18.711
  },
  {
    "corpusid": 235727288,
    "section": "15/46",
    "subsection": 1.0,
    "section_pos_in_pct": 0.326,
    "text": "Numerical Stability and Implementation To implement q-paths in practice, we begin by considering the log of the expression in Eq. (25), which is guaranteed to be non-negative becauseπ β,q (z) is an unnormalized density. \nlogπ β,q (z) =(34)log π 0 (z) + 1 1 − q log 1 + (1 − q) · β · ln q π 1 (z) π 0 (z) ,q-path (blue) is still q = 1 − δ for small δ > 0.\nWe focus attention on ln qπ1 (z)/π 0 (z) term, which is potentially unstable for q = 1 since it takes importance weights w =π 1 (z)/π 0 (z) as input. Since we are usually given log weights in practice, we consider the identity mapping w = exp(log w) and reparameterize q = 1 − 1 ρ to obtain\nln q (exp log w) = 1 1 − q (exp log w) 1−q − 1 (35) = ρ (exp log w) 1 ρ − 1 (36) = ρ exp{ 1 ρ log w} − 1 .(37)\nThis suggests q should be chosen such that the exponential doesn't overflow or underflow, which can be accomplished by setting ρ on the order of\nρ = max i | log w i |.(38)\nwhere i indexes a set of particles {z i }. This choice is reminiscent of the log-sum-exp trick and ensures | 1 ρ log w| ≤ 1. In Fig. 6, we explore the impact of changing the scale of log w on the numerical stability of q-paths. For the case of inferring global model parameters over N i.i.d. **data** points p(D) = N n=1 p(x n ), we can see that the scale of the unnormalized densitiesπ 1 (θ, D) = p(θ) N n=1 p(x n |θ) differs based on the number of datapoints, where increasing N decreases the magnitude of log w = logπ 1 (θ, D) with π 0 (θ) = p(θ).",
    "id": 323,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 323,
    "created_at": "2023-03-07T16:08:35.795239Z",
    "updated_at": "2023-03-07T16:08:35.795267Z",
    "lead_time": 7.476
  },
  {
    "corpusid": 251500724,
    "section": "8/26",
    "subsection": 1.0,
    "section_pos_in_pct": 0.308,
    "text": "The fingerprint of the transition appears very clearly in Figure 3 as the sudden, dramatic increase in the viscoelastic moduli below a critical temperature. It is worth noting that the sample shows a Newtonian rheological behavior at the loading temperature of 70 °C. This Newtonian behavior at 70 °C was confirmed by steady-state measurements in Couette geometry, which are not reported here for brevity. The corner point of Figure 3a, where the complex viscosity is reported, indicates that the sol-gel transition begins at a temperature of about 36 °C. At a slightly lower temperature (about 32 °C, see Figure 3b), the elastic modulus overcomes the loss modulus, suggesting that the material changes its behavior from liquid-like to solid-like. The **data** in Figure 3 determine a first, rheology-related process indication, as far as the extrusion temperature is concerned. According to the kC's viscoelastic response upon cooling, an excessively high extrusion temperature would result in the production of a purely viscous, low-viscosity, structureless filament, with obvious negative consequences on the structural consistency of the construct. Conversely, an excessively low temperature would shift the kC material into the solid-like region, thus making the generation of a smooth, continuous filament impossible.",
    "id": 324,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 324,
    "created_at": "2023-03-07T16:10:03.396914Z",
    "updated_at": "2023-03-07T16:10:03.396943Z",
    "lead_time": 87.421
  },
  {
    "corpusid": 119479526,
    "section": "8/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.276,
    "text": "In this section we employ the Bayesian framework and methods introduced in the previous sections for the development of a thermodynamic property model for the alpha, beta and liquid phases of Hafnium. In Section 3.1 we give an overview of Hafnium. In Section 3.2 we discuss how the **data** was collected and any corrections which were applied. In Section 3.3 we present our analysis methods and in Section 3.4 the model selection criteria are presented. Our main results are given in Section 3.5.",
    "id": 325,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 325,
    "created_at": "2023-03-07T16:10:18.971483Z",
    "updated_at": "2023-03-07T16:10:18.971557Z",
    "lead_time": 15.393
  },
  {
    "corpusid": 117119864,
    "section": "48/83",
    "subsection": 1.0,
    "section_pos_in_pct": 0.578,
    "text": "Interest in the LL was greatly intensified by Anderson's proposal [127] that the ARPES lineshapes and certain other properties of the superconducting cuprates signaled in a general way the possibility of LL behavior in quasi 2-d systems, and by pioneering PES studies [128][129][130] of quasi-1-d metals that found power law onsets to EF rather than a Fermi edge. A complication for interpreting the PES **data** is that below a transition temperature TCDW the quasi-1-d metals studied display static charge density wave (CDW) formation which gaps the quasi-1-d FS and produces an insulator. Especially in 1-d, strong CDW fluctuations involving electron-phonon interactions above TCDW can cause the PES lineshape to have NFL behavior that mimics that of the LL, in particular that there can be a pseudogap [131][132][133] such that the weight at EF is greatly suppressed. This situation has motivated ARPES activity [17] on low-dimensional non-cuprate materials with the intent of elucidating CDW behavior, of searching for a paradigm having the distinctive features of the LL lineshape, and of seeing connections to cuprate spectra. The finding of the FL paradigm TiTe2 discussed in section 3.2 above was part of this general program, which has now progressed also to success with the other two goals, as described next.",
    "id": 326,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 326,
    "created_at": "2023-03-07T16:10:30.066297Z",
    "updated_at": "2023-03-07T16:10:30.066327Z",
    "lead_time": 10.91
  },
  {
    "corpusid": 16212589,
    "section": "3/6",
    "subsection": 7.0,
    "section_pos_in_pct": 0.5,
    "text": "Although our measurements for these Na § channels extend the measurable range and show a higher limiting slope, they do not contradict previous estimates. As indicated above, our **data** also show a slope of six charges in the range of 0.1 to 0.001, where most previous estimates of limiting slope were derived. For skeletal muscle Na + channels, however, this range is not sufficiently negative to reach the true limit.",
    "id": 327,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 327,
    "created_at": "2023-03-07T16:10:36.760729Z",
    "updated_at": "2023-03-07T16:10:36.760763Z",
    "lead_time": 6.508
  },
  {
    "corpusid": 52141668,
    "section": "6/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.375,
    "text": "Using the three classifiers, we obtained the best result using Random Forest with 80.29% accuracy and 74.12% for F-measure as can be seen in Table  2. The imbalanced dataset made most of the **data** to be classified into the majority class, which is the   In order to handle the imbalance **data** problem, we also try to apply two resampling techniques, Synthetic Minority Over-sampling Technique (SMOTE) for increasing the minority classes and Random Under-sampling (RUS) for decreasing the majority classes in training data. By applying the two resampling techniques, we manage to increase the ability of the classifiers in detecting deception. However, it also decreases the ability in detecting truth as well. This causes the F-measure score for each classifier to decrease as can be seen in Table  3.",
    "id": 328,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 328,
    "created_at": "2023-03-07T16:10:42.283278Z",
    "updated_at": "2023-03-07T16:10:42.283304Z",
    "lead_time": 5.336
  },
  {
    "corpusid": 247627894,
    "section": "9/15",
    "subsection": 1.0,
    "section_pos_in_pct": 0.6,
    "text": "By the Neyman-Pearson Lemma and the Likelihood-Ratio Trick, the test statistic with the optimal power for a test of H 0 : p(x|θ 0 ) and H 1 : p mix (x|S c ) can be found by optimizing the binary cross-entropy:\nL BXE (φ|S c , θ 0 ) = −E (y,x∼p(x|θ 0 ),p mix (x|Sc) [(y log s φ (x) + (1 − y) log(1 − s φ (x))],(8)\nwhere samples from θ 0 are labeled y = 0 and samples from the mixture of alternatives are labelled y = 1. To optimize the global power from Equation 6 in a likelihood-free way we can now average over S c and θ 0 to define a global loss L(φ)\nL(φ) = E Sc,θ 0 [L BXE (φ|S c , θ 0 )](9)\ngiven choice of densities p(θ alt |S c ), p(S c |θ 0 ), p(θ 0 ). The final optimization procedure is simple: for each minibatch, sample θ 0 and a random set of alternatives from θ alt from a random surface S c (θ 0 ), evaluate the neural network on **data** x ∼ p(x|θ i ) and compute the average of the binary cross-entropy losses for each alternative, where samples from θ 0 are labeled y = 0 and any samples from any θ alt are labelled y = 1. With such a procedure and average loss definition, the parameters φ can be optimized via standard stochastic gradient descent. The algorithm is summarized in Algorithm 1.",
    "id": 329,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 329,
    "created_at": "2023-03-07T16:10:47.449638Z",
    "updated_at": "2023-03-07T16:10:47.449671Z",
    "lead_time": 4.974
  },
  {
    "corpusid": 119144624,
    "section": "13/21",
    "subsection": 1.0,
    "section_pos_in_pct": 0.619,
    "text": "In this section we discuss the compatibility of the flat family constructed in Section 9 (out of the **data** of a valuation with a finitely generated value semigroup) with a torus action. We will show that the family can be constructed so that the compatibility conditions in Section 6 are satisfied and hence the integrable system from Section 10 descends to the quotient by the torus.",
    "id": 330,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 330,
    "created_at": "2023-03-07T16:10:55.147801Z",
    "updated_at": "2023-03-07T16:10:55.147823Z",
    "lead_time": 7.467
  },
  {
    "corpusid": 207778550,
    "section": "12/48",
    "subsection": 2.0,
    "section_pos_in_pct": 0.25,
    "text": "Clue 3: It should be investigated whether the input **data** points tend to scatter such that the number of these **data** points seen to lie significantly outside the uncertainty bands defined by the population standard deviation (irrespective of the uncertainties assigned by the original experimenters) is larger (or smaller) than should be expected from applicable statistical criteria. For example, if the **data** are assumed to be normally distributed, as is generally the default assumption, then (as mentioned earlier) approximately two-thirds of the points should lie within the above-mentioned uncertainty bands. Excessive numbers of **data** points (or too few of them) seen to lie outside these limits would suggest that a problem exists with the assumption that these **data** are normally distributed. Then, evaluation by conventional methods, e.g., the least squares method, that assume normally distributed **data** could generate misleading results. Some of the rogue **data** points actually may be discrepant for physical reasons that transcend statistics, and this could lead to inappropriate reliance on statistical interpretation. Furthermore, most statistical rules become meaningful only when fairly large numbers of **data** points are involved.",
    "id": 331,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 331,
    "created_at": "2023-03-07T16:21:00.970466Z",
    "updated_at": "2023-03-07T16:21:00.970496Z",
    "lead_time": 605.603
  },
  {
    "corpusid": 216162633,
    "section": "6/6",
    "subsection": 1.0,
    "section_pos_in_pct": 1.0,
    "text": "6. Examples 6.1. Generalized Boussinesq equation. We consider the equation (GB) * . We remember that hypothesis (H0) holds with c = min{m, α1 α2 }. Also, we assumed that the source terms f and the corresponding potential operator F do not have any particular form but they satisfy (H1). The solution in the sense of Definition 2.1 holds, see [29,25,33], then nonexistence of global solutions is due to blow-up. By Theorem 3.1, if the initial **data** are such that u 0 2 * + α 2 u 0 2 2 > 0, (u 0 , u 1 ) * + α 2 (u 0 , u 1 ) 2 > 0, and the initial energy\nE 0 = 1 2 u 1 2 * + α 2 u 1 2 2 + α 3 ∇u 0 2 2 + m 2 u 0 2 * + α 1 u 0 2 2 − F (u 0 ),\nis such that E 0 ∈ I δ = (α δ , β δ ), given in Theorem 3.1, then the corresponding solution is not global and blows up in finite time. Moreover, by Corollary 1, for every positive initial energy E 0 , there exists initial **data** such that imply the nonexistence of global solutions in the norm of H.",
    "id": 332,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 332,
    "created_at": "2023-03-07T16:21:05.111782Z",
    "updated_at": "2023-03-07T16:21:05.111807Z",
    "lead_time": 3.901
  },
  {
    "corpusid": 19033986,
    "section": "1/28",
    "subsection": 4.0,
    "section_pos_in_pct": 0.036,
    "text": "In this paper we study DM that is consistent with all **data** including the preliminary PAMELA electron measurement, most recent FERMI electron plus positron measurements and the preliminary diffuse γ measurements. We then make predictions for the additional FERMI diffuse γ ray measurements and the future PAMELA positron fraction. In fig. 1 we demonstrate a sample good fit of the PAMELA and FERMI **data** and its prediction for the ICS spectrum that is an outcome of our analysis. The ICS flux in this example is representative of most models that fit PAMELA, FERMI and HESS. This paper is organized as follows. In Section 2 we describe the various final states in our DM calculations. We relate these final states to various models of DM including annihilating, decaying, and long lived intermediate particles. In Section 3 we outline the calculation for the contribution to the diffuse gamma ray background from ICS and synchrotron. We point out certain relations between the calculation of ICS and diffusion and bring up certain issues not discussed previously in the literature. We discuss a semi-analytic approximation to Inverse Compton scattering that illuminates some basic physical features. The reader might want to jump directly to Section 4, where we fit the various models to the existing cosmic ray data, identifying the parameter space viable for explaining PAMELA after the recent FERMI and HESS results. In view of the strong restrictions we make precise predictions for the upcoming diffuse gamma ray measurement by FERMI and comment on whether this will be a sufficient measurement to test the DM explanation. In the appendices, we describe some of the details of our approach to calculating neutrino fluxes and fitting procedure.",
    "id": 333,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 333,
    "created_at": "2023-03-07T16:21:14.387435Z",
    "updated_at": "2023-03-07T16:21:14.387462Z",
    "lead_time": 9.079
  },
  {
    "corpusid": 5648146,
    "section": "10/18",
    "subsection": 2.0,
    "section_pos_in_pct": 0.556,
    "text": "In this section, the **data** set of joints and COP in AP from a typical subject is evaluated by the determinism test and stationarity test.  Figures 2(a), 2(c), 2(e), and 2(g). This clearly confirms the deterministic nature of human balance system. The average cross-prediction error for all possible combinations of and is given in Figures 2(b), 2(d), 2(f), and 2(h). The average values of all are 0.1839, 0.2215, 0.1398, and 0.8951 (for the time series of hip, knee, ankle, and COP in AP, resp.). Since each maximal cross-prediction error is not significantly larger than the average, the studied time series are clearly stationary.",
    "id": 334,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 334,
    "created_at": "2023-03-07T16:21:21.113735Z",
    "updated_at": "2023-03-07T16:21:21.113767Z",
    "lead_time": 6.526
  },
  {
    "corpusid": 119125001,
    "section": "2/32",
    "subsection": 2.0,
    "section_pos_in_pct": 0.062,
    "text": "Many researchers in dispersive partial differential equations have recently examined whether blow-up behaviour, such as the norm-inflation described above, occurs for generic or only exceptional sets of rough initial data. To quantify this, one is quickly lead to random initial data. Indeed, one natural form of rough initial **data** is pu 0`f ω 0 , u 1`f ω 1 q, where the functions pu 0 , u 1 q P 9 H 1ˆL2 are regular and deterministic, while the functions pf ω 0 , f ω 1 q P H sˆH s´1 are rough and random. An analogue of Theorem 1.1 in this case would imply the stability of the scattering mechanism under a perturbation by noise. The literature on random dispersive partial differential equations is vast. We refer the interested reader to the survey [6], and mention the related works [2,4,8,9,11,12,14,15,16,38,39,41,42,44]. In the following discussion, we focus on the Wiener randomization [3,38] of a function f P H s pR d q. Let ϕ P C 8 pR d q be a smooth and symmetric function satisfying ϕ| r´3{8,3{8s d \" 1, ϕ| R d zr´5{8,5{8s d \" 0, and ř kPZ d ϕpξ´kq \" 1 for all ξ P R d . We then define the associated operator P k by y P k f pξq :\" ϕpξ´kq p f pξq .",
    "id": 335,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 335,
    "created_at": "2023-03-07T16:21:33.340706Z",
    "updated_at": "2023-03-07T16:21:33.340732Z",
    "lead_time": 12.043
  },
  {
    "corpusid": 119247546,
    "section": "38/46",
    "subsection": 2.0,
    "section_pos_in_pct": 0.826,
    "text": "The case 8 9 m 2 < q 2 < m 2 will not be considered here. Instead, we concentrate our attention on the extremal case for which one can prove the following: Proposition 3. Assume q 2 = m 2 . Then there exists r ∈ (r + , r ) such that for r * ∈ (r , r ), the solutions to equation (49) with initial **data** given by (52a)-(52b) satisfy ω > 0 forτ ∈ [0,τ H + ].",
    "id": 336,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 336,
    "created_at": "2023-03-07T16:21:45.540378Z",
    "updated_at": "2023-03-07T16:21:45.540408Z",
    "lead_time": 11.974
  },
  {
    "corpusid": 119634392,
    "section": "1/9",
    "subsection": 1.0,
    "section_pos_in_pct": 0.111,
    "text": "where u : I × R n → R n for some time strip I = [0, T ), ν > 0 is a constant due to the viscosity of the fluid, p : I × R n → R n denotes the fluid pressure, and u 0 : R n → R n . The requisite differential operators are defined by △ = n i=1\n∂ 2 ∂ 2 x i and ∇ = ∂ ∂x i\n, ..., ∂ ∂x n . In dimension n = 2, local and global existence of solutions to the Navier-Stokes equation are well known (see [9]; for a more modern reference, see Chapter 17 of [15]). For dimension n ≥ 3, the problem is significantly more complicated. There is a robust collection of local existence results, including [5], in which Kato proves the existence of local solutions to the Navier-Stokes equation with initial **data** in L n (R n ); [7], where Kato and Ponce solve the equation with initial **data** in the Sobolev space H n/p−1,p (R n ); and [8], where Koch and Tataru establish local existence with initial **data** in the space BMO −1 (R n ) (for a more complete accounting of local existence theory for the Navier-Stokes equation, see [10]). In all of these local results, if the initial datum is assumed to be sufficiently small, then the local solution can be extended to a global solution. However, the issue of global existence of solutions to the Navier-Stokes equation in dimension n ≥ 3 for arbitrary initial **data** is one of the most challenging open problems remaining in analysis.",
    "id": 337,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 337,
    "created_at": "2023-03-07T16:21:58.903608Z",
    "updated_at": "2023-03-07T16:21:58.903642Z",
    "lead_time": 13.152
  },
  {
    "corpusid": 38256305,
    "section": "5/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.102,
    "text": "Satellite Remote Sensing (RS) **data** and Geographical Information System (GIS) software have a huge potential and wide application in assessing and categorizing natural hazards [37][38][39][40]. GIS integrated remote sensing **data** and geospatial analysis can be used to infer factors related to the occurrence of major earthquake shocks and/or earthquake-induced secondary effects. The secondary factors include lithology (loose, unconsolidated sedimentary cover), faults, steeper slopes (landslide susceptibility) or areas with a higher groundwater table [39].",
    "id": 338,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 338,
    "created_at": "2023-03-07T16:22:05.527686Z",
    "updated_at": "2023-03-07T16:22:05.527709Z",
    "lead_time": 6.433
  },
  {
    "corpusid": 29602952,
    "section": "9/19",
    "subsection": 2.0,
    "section_pos_in_pct": 0.474,
    "text": "We have generated 1000 **data** sets from the nonignorable selection model. From Crow's data, we have obtained the distribution of the ten family sizes 1, 2, . . . , 10. The frequencies of the family sizes are 9, 24, 16, 13, 9, 2, 4, 1, 1, 1. Thus, using the table method, we draw 100 family sizes for each of the 1000 simulated **data** sets. Now, noting that p(a k ,r k |p, π, θ)=p(a k |r k , π)p(r k |p, π, θ),",
    "id": 339,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 339,
    "created_at": "2023-03-07T16:22:24.202475Z",
    "updated_at": "2023-03-07T16:22:24.202502Z",
    "lead_time": 18.465
  },
  {
    "corpusid": 236766189,
    "section": "5/65",
    "subsection": 1.0,
    "section_pos_in_pct": 0.077,
    "text": "The first step of the proposed methodology comprises the estimation of the seismic hazard in the location of the building through the use of seismic hazard curves: These curves show the probability that an earthquake intensity level is exceeded during a predetermined time period. The Peak Ground Acceleration (PGA) was chosen as the earthquake intensity measure (IM) in this study to facilitate a comparison of the analytically derived results with **data** obtained from experimental campaigns conducted using the chosen RC frame building that are based on the excitation of the structure with different PGA levels (Fardis 1996;Dolšek 2008Dolšek , 2010. Furthermore, the choice of PGA as an IM facilitates a comparison of this study with the analytically derived, PGA-based fragility curves developed for RC frame buildings of varying compliance to seismic code provisions, such as the ones determined by Kappos et al. (2006) and the PGA-based fragility curves considering ageing effects on RC frame buildings (Couto et al. 2020). Kostinakis et al. (2015), Cantagallo et al. (2012), Yakut and Yılmaz (2008) and Iervolino et al. (2018), among others, demonstrated the high correlation of the spectral acceleration at the fundamental period of the structure with the maximum and average interstorey drifts of RC buildings, indicating that such period-dependent intensity measure could be more appropriate. However, Cantagallo et al. (2012) suggest the use of the cracked (secant) period of the structure to select a spectral acceleration as an intensity measure: determining such a period for an existing building is not easy in practice, considering the variations from the as-designed state due to construction, use and ageing. Within this frame, the use of a vibration-period-independent IM in this study is a good compromise between simplicity, computational efficiency and accuracy. Two different locations have been chosen to represent the effect of two different seismic hazard levels on the seismic risk of existing RC buildings: Sion, Switzerland and Athens, Greece. The seismic hazard curves for both regions are determined using the seismic hazard platform of the European Facilities for Earthquake Hazard and Risk (EFEHR 2017) and are shown in Fig. 4. ",
    "id": 340,
    "annotator": 1,
    "annotation_id": 340,
    "created_at": "2023-03-07T16:23:41.045675Z",
    "updated_at": "2023-03-07T16:23:41.045701Z",
    "lead_time": 76.645
  },
  {
    "corpusid": 25183918,
    "section": "3/7",
    "subsection": 3.0,
    "section_pos_in_pct": 0.429,
    "text": "Although our linguistic heuristics works slightly better for **data** parsed by Charniak' parser, the incorrect SCF cue rate after applying heuristics remains at about the same level for the two different parsers we used.",
    "id": 341,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 341,
    "created_at": "2023-03-07T16:23:52.695820Z",
    "updated_at": "2023-03-07T16:23:52.695857Z",
    "lead_time": 11.403
  },
  {
    "corpusid": 38256305,
    "section": "9/49",
    "subsection": 2.0,
    "section_pos_in_pct": 0.184,
    "text": "Morphometric properties of an area can influence to a great deal local site effects during earthquakes and earthquake-related secondary effects. When searching for areas susceptible to soil amplification, liquefaction or compaction, the so-called causative or preparatory factors have to be taken into account. These include height level, slope gradient, terrain curvature, lithologic conditions and fault zones. Morphometric factors can be extracted from DEM **data** that might be of importance for the detection of local site conditions influencing earthquake ground motion: Height level maps help to search for topographic depressions covered by almost recently-formed sediments, which are usually linked with higher groundwater tables. Extracting the lowest height level of an area makes visible areas with high groundwater tables. In case of stronger earthquakes, such areas have often shown the highest earthquake damage intensity, compaction and liquefaction occurrence. From the DEM data, flat areas with no curvature of the terrain and low to no slope gradients were extracted.",
    "id": 342,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 342,
    "created_at": "2023-03-07T16:24:13.379125Z",
    "updated_at": "2023-03-07T16:24:13.379159Z",
    "lead_time": 20.476
  },
  {
    "corpusid": 119479526,
    "section": "7/29",
    "subsection": 3.0,
    "section_pos_in_pct": 0.241,
    "text": "We select these non-overlapping regions of measurement for specific heat and enthalpy to demonstrate that the Bayesian framework can capture robust representations of both quantities when **data** does not cover the entire temperature range for each quantity. We generate synthetic **data** for the enthalpy, H and specific heat, C p , via Eq. (6a) and Eq. (6b), respectively,\nH (T ) − H (298.15K) = 3Rθ e θ / T − 1 + a T 2 2 + b T 3 3 (6a) C p (T ) = d dT H (T ) = 3R θ / T 2 e θ / T e θ / T − 1 2 + aT + bT 2 ,(6b)\nwhere R is the gas constant and the true model parameters are defined as follows: θ = 150K, a = 6 × 10 −3 J/mol.K, b = 7 × 10 −7 J/mol.K 2 and c = −5 × 10 3 J/mol. We add Gaussian noise with a standard error of 5000J/mol. to the enthalpy values. We add Gaussian noise to the specific heat values that scales linearly with the magnitude to a maximum of 3J/mol.K at 75K. Note that the model form of the specific heat in Eq. (6b) was proposed at the Ringberg workshop in 1995 to be effective across the entire temperature range [29]. In this case, the first term employs the Einstein model to account for low-temperature behavior, though the Debye or other models may be substituted [30].",
    "id": 343,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 343,
    "created_at": "2023-03-07T16:24:23.411933Z",
    "updated_at": "2023-03-07T16:24:23.411970Z",
    "lead_time": 9.836
  },
  {
    "corpusid": 53380409,
    "section": "64/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.538,
    "text": "\nnew heavy resonances decaying into different pairings of W, Z, or Higgs bosons, as well as directly into leptons, are presented using a **data** sample corresponding to 36.1 fb −1 of pp collisions at ffiffi ffi s p ¼ 13 TeV collected during 2015 and 2016 with the ATLAS detector at the CERN Large Hadron\n",
    "id": 344,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 344,
    "created_at": "2023-03-07T16:24:28.033472Z",
    "updated_at": "2023-03-07T16:24:28.033503Z",
    "lead_time": 4.43
  },
  {
    "corpusid": 19033986,
    "section": "12/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.429,
    "text": "although DM present thanks to a baryon-like asymmetry could decay only into anti-leptons. Fig. 5b shows that DM decays can fit the PAMELA, FERMI and HESS e ± excesses as well as DM decays, with some minor differences, mostly due to the fact that only in the DM annihilation case a sizable amount of lower-energy e ± can reach us from the Galactic Center, giving rise to a smoother e ± energy spectrum. Decay modes of fermionic DM into W ± µ ∓ or W ± τ ∓ provide good fits to e ± observations, but together with ap excess which is strongly disfavored by PAMELAp observations [2]. Fig. 10 and 11 show that DM interpretations of the PAMELA, FERMI and HESS e ± **data** in terms of leptonic DM decays are compatible with all the constraints we considered, the strongest one being relative to ICS from the '10 • ÷ 20 • ' region observed by FERMI. Other regions (so far observed only by EGRET, with a problematic energy calibration) offer possibly stronger constraints. In the near future FERMI will release **data** in other regions that can be used to constrain this scenario. As discussed in section 3, one can easily extrapolate the fluxes to other regions using the approximations we have employed.",
    "id": 345,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 345,
    "created_at": "2023-03-07T16:25:22.326329Z",
    "updated_at": "2023-03-07T16:25:22.326357Z",
    "lead_time": 54.102
  },
  {
    "corpusid": 119125001,
    "section": "1/32",
    "subsection": 3.0,
    "section_pos_in_pct": 0.031,
    "text": "Here, W ptqpu0 , u1 q \" cospt|∇|qu0`psinpt|∇|q{|∇|qu1 denotes the solution to the linear wave equation with initial **data** pu0 , u1 q.",
    "id": 346,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 346,
    "created_at": "2023-03-07T16:25:30.821533Z",
    "updated_at": "2023-03-07T16:25:30.821559Z",
    "lead_time": 8.282
  },
  {
    "corpusid": 119479526,
    "section": "3/29",
    "subsection": 2.0,
    "section_pos_in_pct": 0.103,
    "text": "Likelihood is the denominator of Eq. (1) and represents the probability of the **data** given the model. The marginal Likelihood has the desirable qualities of rewarding models that match the **data** well and penalizing models that are overly complex (i.e. have too many degrees of freedom or parameters). The marginal Likelihood is given by,\nP (D|M ) = Ω Θ P (D|Θ, M) P (Θ|M ) dΘ,(2)\nwhere Ω Θ represents the complete parameter space. Note that by integrating over the parameters in the model we are able to compare Bayesian Evidences for two distinct models in a meaningful way, even if they contain different numbers of parameters. This ratio of the Bayesian Evidence for two models is called the Bayes Factor and is given by\nR = P (M A |D) P (M B |D) = P (D|M A ) P (M A ) P (D|M B ) P (M B ) ,(3)\nwhere M A and M B are the two models under consideration [15]. Kass and Raftery [21] provide a commonly used guide to interpret the Bayes factor, in which a value in the range 3 − 20 indicates a positive strength of evidence for the preference of one model versus another and a factor in the range 20 − 150 represents strong evidence to prefer one model over another. Unfortunately because of the integral over parameter space Eq. (2) is notoriously difficult to evaluate. Only with recent advances in sampling techniques has the model Evidence become an appealing option for model selection. We briefly discuss some selected sampling approaches in the following section.",
    "id": 347,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 347,
    "created_at": "2023-03-07T16:25:37.507295Z",
    "updated_at": "2023-03-07T16:25:37.507387Z",
    "lead_time": 6.456
  },
  {
    "corpusid": 55648852,
    "section": "4/40",
    "subsection": 1.0,
    "section_pos_in_pct": 0.1,
    "text": "On-land broadband (BB) portable seismic stations were provided by the GIPP, Germany , in this volume]. They are composed of an Earth-Data PR6-24 recorder and Nanometrics Trillium Compact seismometer ( Figure 3). The PR6-24 is a **data** logger for stand-alone use with three-channel digitizer with a 140dB dynamic range at 100 sps of sampling rate. The seismometer is a Trillium Compact, a broadband three-component sensor with a flat response that ranges from 120 sec to 100 Hz. Its three symmetric triaxial sensors are resonance-free up 200 Hz. These stations were powered with solar panels connected to a 75Ah 12 V batteries.",
    "id": 348,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 348,
    "created_at": "2023-03-07T16:25:49.171672Z",
    "updated_at": "2023-03-07T16:25:49.171695Z",
    "lead_time": 11.436
  },
  {
    "corpusid": 119415579,
    "section": "23/41",
    "subsection": 2.0,
    "section_pos_in_pct": 0.561,
    "text": "A clear increase in the incidence rate of mCP stars with increasing mass was identified in Paper I (mCP stars account for ≈ 3 per cent of MS stars with M ≈ 1.5 M and ≈ 10 per cent of MS stars with 3.0 < M/M < 3.8). The Monte Carlo simulation involving the Zorec & Royer (2012) **data** did not reveal an increase in Bc with decreasing M , which might have otherwise explained the increased rarity of lower mass mCP stars. We conclude that, regardless of whether Bc exists, this particular property of mCP stars is likely a product of additional factors such as the increase in subsurface convection zone depth with decreasing mass.",
    "id": 349,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 349,
    "created_at": "2023-03-07T16:25:53.346380Z",
    "updated_at": "2023-03-07T16:25:53.346408Z",
    "lead_time": 3.934
  },
  {
    "corpusid": 118927342,
    "section": "9/10",
    "subsection": 2.0,
    "section_pos_in_pct": 0.9,
    "text": "\nB(B − → Λpπ + π − ) 3.7 +1.2 −0.5 ± 0.1 ± 0.9 5.9 ± 1.110 6 B(B − → ΛpK + K − ) 3.0 +1.1 −0.5 ± 0.1 ± 0.7 --10 6 B(B 0 → ppπ + π − ) 3.0 +0.5 −0.3 ± 0.3 ± 0.7 3.0 ± 0.310 6 B(B 0 → ppπ ± K ∓ ) 6.6 ± 0.5 ± 0.0 ± 2.3 6.6 ± 0.5 included the **data** to constrain the non-factorizable effects, which results in δN ef",
    "id": 350,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 350,
    "created_at": "2023-03-07T16:25:55.912317Z",
    "updated_at": "2023-03-07T16:25:55.912345Z",
    "lead_time": 2.353
  },
  {
    "corpusid": 17671315,
    "section": "5/32",
    "subsection": 2.0,
    "section_pos_in_pct": 0.156,
    "text": "Since the radar/target distance is provided by GPS with a good accuracy, geometrical parameters of targets, including their depths, are not of an interest here. The main goal of the publication [24] was to calculate ratios R of dielectric constants\nR = ε r (target) ε r (bckgr) ,(1)\nwhere ε r (target) is the dielectric constant of the background medium. If ε r (bckgr) is known, then (1) enables one to calculate ε r (target). If a target is located above the ground, then ε r (bckgr) = ε r (air) = 1. Unfortunately, dielectric constants were not measured during the **data** collection process. Therefore, the authors of [24] had no choice but to rely on tables of dielectric constants [32, 33] when evaluating the accuracy of results. In our mathematical  \nR = max [0,1] R(x) if R(x) ≥ 1, ∀x ∈ (0, 1), min [0,1] R(x) if R(x) < 1, ∀x ∈ (0, 1).(2)\nHence, in (1) we actually have R := R, and by (2) ε r (target) = Rε r (bckgr).",
    "id": 351,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 351,
    "created_at": "2023-03-07T16:26:02.241908Z",
    "updated_at": "2023-03-07T16:26:02.241934Z",
    "lead_time": 6.13
  },
  {
    "corpusid": 55381955,
    "section": "6/12",
    "subsection": 3.0,
    "section_pos_in_pct": 0.5,
    "text": "The best fit parameters we find in order to reproduce the ARGO **data** with our model are ESN = 10 51 erg, ℜ = 1/15 yr −1 , EM ∼ = 507 TeV and ξCR ∼ = 5.2%. While it is clear that the spectrum of light nuclei (H+He) is easily accounted for, it is equally clear that the all-particle spectrum knee cannot be reproduced within a simple approach such as the one discussed here.    [26,28], with k = 9, ESN = 10 51 erg, ℜ = 1/15 yr −1 , EM ∼ = 507 TeV and ξCR ∼ = 5.2%.",
    "id": 352,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 352,
    "created_at": "2023-03-07T16:26:11.965486Z",
    "updated_at": "2023-03-07T16:26:11.965514Z",
    "lead_time": 9.532
  },
  {
    "corpusid": 238834501,
    "section": "1/19",
    "subsection": 3.0,
    "section_pos_in_pct": 0.053,
    "text": "We use the lasso as a regression method with subset selection (for both the 'average' and the 'smoothing' type); the predictors are selected by including an ℓ 1 penalty on the vector of regression weights during the loss minimization, which effectively sets many of the weights to zero 54 . For comparison, we evaluate unweighted averages over N days similar to the eclipse day ('N-averages' from now on), as well as standard smoothing procedures (local regression, in both linear 5 and quadratic variants). Similarity between predictor and target days for the N-average is assessed as the pairwise Euclidean distance in the space spanned by times t • . Uncertainties for all methods can be quantified empirically by estimating the values for all t ∈ t • on all days d ∈ d • and subtracting the corresponding actual observations. This gives an error distribution for each time t ∈ t • . For the smoothing regression-the only method where the quantity of interest to us is actually the target-care needs to be taken to separate training and test data. To this end, we use four-fold cross-validation, so that approximately 1 year of **data** is in the test set at any time, and we calculate the error for a day only if it is in a test set. Figures 1 and 2 show the regression-based estimates and those obtained from comparison methods, respectively, for air temperature on the day of the eclipse at a few selected stations in and around La Serena. The box plots show the temperature depression T of the observation relative to the reference estimate. The plot elements representing uncertainty (shaded bands, boxes, whiskers and fliers) are constructed by adding the error distributions computed as described above to the point estimate. The regression methods' uncertainty is much reduced relative to that of the comparison methods; some but not all estimates of T fall outside of 95% of the calculated error distributions.",
    "id": 353,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 353,
    "created_at": "2023-03-07T16:26:24.884981Z",
    "updated_at": "2023-03-07T16:26:24.885011Z",
    "lead_time": 12.723
  },
  {
    "corpusid": 118551842,
    "section": "1/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.143,
    "text": "The present is a period of dramatic evolution of the study of stellar structure and evolution, in large part owing to the possibilities offered by the rapidly expanding **data** on stellar oscillations. The resulting frequencies are expected to provide stringent constraints on stellar properties, including the structure of stellar interiors, and hence challenge our understanding of stellar internal physics and stellar evolution. An obvious requirement for this is that observations are made of a sufficiently broad sample of stars, with sufficient sensitivity and of sufficient duration to secure the required frequency precision. Such observations are now resulting from the CoRoT and Kepler space missions (e.g., Baglin et al. 2006;Borucki et al. 2010), and further improvements are promised by dedicated ground-based facilities for radialvelocity observations (e.g., Grundahl et al. 2009). However, this is not all: we must also be able to analyse the basic observations to extract reliable frequencies, as well as to identify the observed modes. Also, stellar modelling must be sufficiently reliable and accurate that the assumptions about stellar internal physics are faithfully reflected in the frequencies computed from the resulting models. Although substantial progress is being made both in the **data** analysis and the modelling, it is probably fair to say that further efforts are required in these directions.",
    "id": 354,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 354,
    "created_at": "2023-03-07T16:26:35.091190Z",
    "updated_at": "2023-03-07T16:26:35.091224Z",
    "lead_time": 10.0
  },
  {
    "corpusid": 17837291,
    "section": "22/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.423,
    "text": "The Euler **data** Q\nTheorem 7.1. (i) deg α A d ≤ −2.\n(ii) If for each d the class b T (V d ) has homogeneous degree the same as the degree of M 0,0 (d, X), then in the nonequivariant limit we have\nX e −H·t/α A d = α −3 (2 − d · t)K d X HG[A](t) − e −H·t/α Ω = α −3 (2Φ − t i ∂Φ ∂t i ).\nProof: Earlier we have proved that\nA d = i * 0 Q v d = ev ! ρ * b T (V d ) α(α − c 1 (L)) ,\nwhere L = L d is the line bundle on M 0,1 (d, X) whose fiber at a point (f, C; x) is the tangent line at x.",
    "id": 355,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 355,
    "created_at": "2023-03-07T16:26:38.834185Z",
    "updated_at": "2023-03-07T16:26:38.834214Z",
    "lead_time": 3.5
  },
  {
    "corpusid": 209991952,
    "section": "14/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.609,
    "text": "A search for supersymmetry in events with same-sign leptons and jets is presented. The analysis is performed with pp collision **data** collected at a centre-of-mass energy of √ s = 13 TeV between 2015 and 2018 with the ATLAS detector at the LHC, corresponding to an integrated luminosity of 139 fb −1 . Five signal regions are defined to provide sensitivity to a broad range of supersymmetric processes. No significant excess over the yields expected from SM processes is observed, and model-independent limits on the cross-section of possible BSM signal contributions to the signal regions are reported.",
    "id": 356,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 356,
    "created_at": "2023-03-07T16:26:43.539264Z",
    "updated_at": "2023-03-07T16:26:43.539389Z",
    "lead_time": 4.468
  },
  {
    "corpusid": 118490678,
    "section": "4/15",
    "subsection": 2.0,
    "section_pos_in_pct": 0.267,
    "text": "The charm quark spectrum that was input as a normalized form f c (p T ) = (1.0 + 3.185p 2 T ) −2.7 /0.354 is extracted by fitting the STAR spectra of D 0 (D 0 ) and electron. The fitted spectra in Fig. 1 are slightly softer than the STAR **data** considering no bottom contributions. The solid lines are from the STAR σ NN cc , while the dashed lines, all lower than the data, correspond to the PHENIX σ NN cc = 0.622 mb [31]. Hereafter, the STAR σ NN cc 1.4 mb is used in all calculations at 200 GeV. The p T dependencies of the charm baryon-to-meson ratios Fig. 2. It is clear to see that the charm baryon enhancement in intermediate p T range is very prominent, which is similar to that of the p/π ratio. Note that the peak of\nΛ c +Λ c D 0 +D 0 , Λ c +Λ c D + +D − , and Λ c +Λ c D + s +D − s are plotted inΛ c +Λ c D + +D − is even higher than Λ c +Λ c D + s +D − s . This is because the yield of D + s (D − s ) is larger than D + (D − ) around p T = 3\nGeV/c due to the decay effect. The shapes of the p T dependencies of these ratios are dependent on the CO/RE mechanism, decay effect, and the charm quark spectrum obtained from fitting **data** shown in Fig. 1. Comparing with the **data** of p/π + [34] in the same Au+Au collisions, one can see that all peaks broaden and shift to the right because essentially the spectrum of the charm quark is much harder than that of the u(d) quark.",
    "id": 357,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 357,
    "created_at": "2023-03-07T16:26:56.052920Z",
    "updated_at": "2023-03-07T16:26:56.052951Z",
    "lead_time": 12.269
  },
  {
    "corpusid": 119419652,
    "section": "3/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.167,
    "text": "Jet and dijet cross-section measurements allow for a good test of perturbative QCD (pQCD) in p-p collisions. The ATLAS collaboration performed such doubly differential measurements using a total integrated luminosity of up to 3.2 fb −1 at a centre-of-mass energy of √ s = 13 TeV [4]. Reconstructed jets were formed using the anti−k T algorithm with a distance parameter R = 0.4 [5], and using EM-scale calorimeter clusters as the inputs. The inclusive-jet results cover a wide kinematic range in jet transverse momentum (p T ) from 100 GeV to 3.5 TeV and for several separate ranges of jet rapidity up to |y| = 3.0. In the case of the dijet cross-section measurements, the results are presented as a function of dijet invariant mass (m jj ) in the range 300 GeV < m jj < 9 TeV, and for values of the quantity y * = 1 2 |y 1 − y 2 | -half the absolute rapidity difference between the two leading jets 1 -up to y * = 3.0. The choice of bin sizes (for p T , y, m jj and y * ) was motivated by the respective detector resolution. The measured results from **data** were compared to next-and in some cases next-to-next-to-leading order (NLO and NNLO, respectively) pQCD predictions, and in all cases were found to be in good agreement with the SM predictions. The inclusive-jet and dijet results are summarized in Figure 1, where the usual convention is adopted of applying multiplicative offset factors to the results in various |y| (y * ) bins in order to allow for a better visual comparison of the shapes. Doubly and triply differential jet cross-section measurements were also performed by the CMS collaboration [6,7,8].  Figure 1: Results of the measured (a) jet and (b) dijet differential cross-section results at √ s = 13 TeV by the ATLAS collaboration [4].",
    "id": 358,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 358,
    "created_at": "2023-03-07T16:27:10.427394Z",
    "updated_at": "2023-03-07T16:27:10.427425Z",
    "lead_time": 14.136
  },
  {
    "corpusid": 55185767,
    "section": "7/16",
    "subsection": 2.0,
    "section_pos_in_pct": 0.438,
    "text": "Finally we treat the case that Y is VF. By Theorem 5.3.1 and by the already treated cases that Y = RF n and Y = VG, it is enough to treat the case that W K,x is a single open ball in K for each K and each x ∈ X K . Moreover, on these balls, we may assume that the functions like h as in (3.2.1) that appear in the build-up of f have the Jacobian property, and that all other build-up **data** of f factors through the projection W → X. But then calculating the integral is easy, and goes as follows. By the Jacobian property for h, the set h K (x, W K,x ) is an open ball, say, of valuative radius n K ∈ Z. If n K ≤ 0, then the integral of ψ(h K (x, y)) over y ∈ W K,x equals zero for each ψ in D K . If n K > 0, then ψ(h K (x, y)) is constant on W K,x , say, equal to ξ K,ψ,x , and hence the integral of ψ(h K (x, y)) over y ∈ W K,x equals the volume of the ball W K,x , say q m K K , times ξ K,ψ,x . By Lemma 4.6.2 and by the already treated cases that Y = RF n , we may suppose that h factors through the projection W → X, so that q m K K ξ K,ψ,x lies in C exp (X) by definition of C exp -functions and we are done. (1) For every definable set X, C(X) contains the characteristic function of every definable set A ⊂ X. (2) The collection is stable under integration, namely, for any X and Y and any f ∈ C(X × Y ) such that for each K and each x ∈ X K , the function y → f K (x, y) is integrable over Y K , the function\n(K, x) → y∈Y K f K (x, y)|dy|\nlies in C(X) (with our usual product measure on Y K ).",
    "id": 359,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 359,
    "created_at": "2023-03-07T16:27:15.941690Z",
    "updated_at": "2023-03-07T16:27:15.941716Z",
    "lead_time": 5.314
  },
  {
    "corpusid": 19033986,
    "section": "7/28",
    "subsection": 4.0,
    "section_pos_in_pct": 0.25,
    "text": "The only spectral feature that can allow one to discriminate the various modes lies at the high end of the spectrum between 1 and 3 TeV, where we only have the electron HESS **data** which is less precise than the FERMI data. The leptonic channels can be ordered according to the sharpness of their end-point:\n2e > 4e ∼ 2µ > 4µ ∼ 2τ > ∼ 4τ.(13)\nThe main difference with respect to the previous ATIC e + + e − **data** [5] is that a peak is no longer present, and therefore the 2e mode (namely, DM annihilations or decays into e + e − ) is now excluded independently of the photon bounds, since it predicts a too sharp end-point. All other leptonic modes we consider provide comparably good fits to the data, that therefore cannot discriminate which (combination of) modes is the correct one. We also explored the spectra produced by polarized µ or τ , similarly finding that present **data** do not allow to discriminate the various possibilities.",
    "id": 360,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 360,
    "created_at": "2023-03-07T16:27:23.125888Z",
    "updated_at": "2023-03-07T16:27:23.125920Z",
    "lead_time": 6.98
  },
  {
    "corpusid": 246863515,
    "section": "39/44",
    "subsection": 2.0,
    "section_pos_in_pct": 0.886,
    "text": "\nreports the performance of different estimators over 1000 replications of the **data** subsampling. Each replication results in a different observational dataset D O with different number of treated units n (1) O < N 1 and different number of control units n (0) O < N 0 . For evaluation we consider two criterion over the 1000 replications: the Mean Absolute Error (MAE) and the Median of Abolute Errors (MedAE).",
    "id": 361,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 361,
    "created_at": "2023-03-07T16:27:31.420897Z",
    "updated_at": "2023-03-07T16:27:31.420924Z",
    "lead_time": 2.454
  },
  {
    "corpusid": 119332876,
    "section": "7/24",
    "subsection": 1.0,
    "section_pos_in_pct": 0.292,
    "text": "Source 179 Figure 4. X-ray spectra of the Galactic stars (data points) together with power law models (black stepped lines) and multitemperature optically-thin thermal models (grey stepped lines). All spectra are shown in the observer frame. Both models and **data** have been divided by the product of effective area and Galactic transmission as a function of energy.",
    "id": 362,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 362,
    "created_at": "2023-03-07T16:27:34.006408Z",
    "updated_at": "2023-03-07T16:27:34.006434Z",
    "lead_time": 2.369
  },
  {
    "corpusid": 218971725,
    "section": "1/10",
    "subsection": 3.0,
    "section_pos_in_pct": 0.1,
    "text": "Optimal design has been considered in some previous works. Reilly and Pepe (1995) derived a closed-form expression of the optimal design for their mean-score estimator. Since the expression depends on phase-2 **data** which are not available at the design stage, Reilly (1996) suggested to estimate the expression using **data** from a further pilot study. McIsaac and Cook (2015) proposed to save the extra cost by using a multi-wave sampling. The idea is to sample wave 1 with pre-specified sampling probabilities and then combine phase-1 and wave-1 **data** to estimate design components. The later waves can then be sampled adaptively.",
    "id": 363,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 363,
    "created_at": "2023-03-07T16:27:39.846836Z",
    "updated_at": "2023-03-07T16:27:39.846862Z",
    "lead_time": 5.594
  },
  {
    "corpusid": 247596533,
    "section": "4/14",
    "subsection": 1.0,
    "section_pos_in_pct": 0.286,
    "text": "The current investigations present the numerical investigations of an infection-based fractional order nonlinear prey-predator system using the stochastic procedures of the scaled conjugate gradient and the artificial neuron networks. The stochastic computational procedure SCGNNs is applied to solve three different cases using different fractional-order values. The **data** proportions applied 75%, 10, and 15% for training, validation, and testing to solve the infection FONPPS. Ten numbers of neurons have been used to solve the nonlinear biologicalbased differential model. The numerical simulations of the infectious disease FONPPS are accomplished using the SCGNNs, while the competitive performances have been presented using the Adams-Bashforth-Moulton approach. The numerical results of the nonlinear fractional-order biological system are calculated using the computational SCGNNs to reduce the mean square error. To ratify the exactness, reliability, capability, and aptitude of the proposed SCGNNs, the numerical measures are plotted using the regression, MSE, STs, correlation, and Ehs. The identical performances designate the precision and accuracy of the proposed stochastic scheme and the AE values found in suitable ranges based on the nonlinear fractional-order biological system. The AE values, and the plots of other performances represent the dependability and consistency of the proposed approach. In future studies, the stochastic SCGNNs are pragmatic to achieve the results of the lonngren-wave systems and fractional order nonlinear systems [37][38][39][40][41][42][43][44][45].    ",
    "id": 364,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 364,
    "created_at": "2023-03-07T16:27:43.314641Z",
    "updated_at": "2023-03-07T16:27:43.314667Z",
    "lead_time": 3.216
  },
  {
    "corpusid": 236603431,
    "section": "24/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.96,
    "text": "Recently, some philosophers have provided renewed defences of the cosmological fine-tuning argument for God's existence (FTA). The ethos of these defences is well-summarized by Hawthorne and Isaacs (2018: 136), by claiming that FTA is 'as legitimate an argument as one comes across in philosophy'. This paper has called the above claim into question. As I have argued, there is no reason to think that FTA can establish even a modest claim about evidential favouring-the claim that the fine-tuning **data** favours (even slightly) the designer hypothesis over its alternatives.",
    "id": 365,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 365,
    "created_at": "2023-03-07T16:28:12.069118Z",
    "updated_at": "2023-03-07T16:28:12.069151Z",
    "lead_time": 28.529
  },
  {
    "corpusid": 119313773,
    "section": "6/21",
    "subsection": 2.0,
    "section_pos_in_pct": 0.286,
    "text": "A concrete example of how statistical noise may impede the detection of linear response breakdown from observations is shown in Figure 7 for an observable A(x) = x. Shown is the observed sample averageĀ (7) as a function of the perturbation size. The error bars are calculated from the standard deviation as calculated for the single available time series, which is the situation for scientists analyzing observations. For insufficient **data** length N = 10 5 a linear response is consistent within the available statistical significance levels (top of Figure 7). Only for significantly larger time series with **data** length N = 10 6 , does the breakdown become detectable in a statistically significant way (bottom of Figure 7).",
    "id": 366,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 366,
    "created_at": "2023-03-07T16:28:29.969231Z",
    "updated_at": "2023-03-07T16:28:29.969258Z",
    "lead_time": 17.698
  },
  {
    "corpusid": 117119864,
    "section": "23/83",
    "subsection": 1.0,
    "section_pos_in_pct": 0.277,
    "text": "The spectra presented in this paper are but a hint of what lies ahead for photoemission studies of electrons in condensed matter. Much can be done with the present capabilities of the technique and the prospects are very good for the further technical advances that are needed. The varied scenarios whereby the beautiful Fermi liquid paradigm is realized even in strongly correlated systems are being elucidated with unprecedented clarity and detail, and the appreciation for the Fermi liquid gained thereby only serves to heighten the great intrigue of exploring interesting ways in which the paradigm can fail. experimental verification of the \"dense impurity ansatz\" for a small TK cerium material. The slight decrease of the sideband and the larger decrease of the EF intensity with dilution are due to a known decrease of TK arising from volume expansion. 8. TL model spectra from theory of Ref. [126] in panels (b) to (f) compared to Li0.9Mo6O17 ARPES **data** of panel (a) and Fig. 7 (c) to show sensitivity of Tomonaga-Luttinger description of **data** to choice of ratio of velocities of holon peaks and spinon edges. Holon peak dispersion is held constant and matched to experimental peak dispersion for ease of comparison of spinon edge dispersions. The lineshapes of panel (d) and Fig. 7 (d) provide an excellent description of the data.",
    "id": 367,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 367,
    "created_at": "2023-03-07T16:30:01.716383Z",
    "updated_at": "2023-03-07T16:30:10.959751Z",
    "lead_time": 93.249
  },
  {
    "corpusid": 2275365,
    "section": "3/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.167,
    "text": "...... Suppose you are a Bayesian and you must express your \"belief\" that the killer will We make use of the Smets' claim that \"the de re and de dicto interpretations lead to the same results\" ( [34], p. 333), that is Bel(A|B) = Bel(¬B ∨ A). Hence being may possess his private view on a subject, it is only after we formalize the feeling of subjectiveness and hence ground it in the **data** that we can rely on any computer's \"opinion\". We hope we have found one such formalization in this paper. The notion of labeling developed here substitutes one aspect of subjective human behaviour -if one has found one plausible explanation, one is too lazy to look for another one. So the process of labeling may express our personal attitudes, prejudices, sympathies etc. The interpretation drops deliberately the strive for maximal objectiveness aimed at by traditional statistical analysis. Hence we think this may be a promising path for further research going beyond the DS-Theory formalism.",
    "id": 368,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 368,
    "created_at": "2023-03-07T16:30:19.542608Z",
    "updated_at": "2023-03-07T16:30:19.542632Z",
    "lead_time": 4.012
  },
  {
    "corpusid": 119479526,
    "section": "6/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.207,
    "text": "In a method introduced by Lahav et al. [27], and expanded by Ma et al. [13], the reported errors for each dataset are weighted using hyperparameters in the Bayesian analysis. In the thermodynamics literature, **data** variances are often available, though covariances are rarely reported. Consequently, we do not present methods for handling **data** covariances in this work and we assume independence of the **data** points.",
    "id": 369,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 369,
    "created_at": "2023-03-07T16:30:23.083159Z",
    "updated_at": "2023-03-07T16:30:23.083188Z",
    "lead_time": 3.338
  },
  {
    "corpusid": 87345260,
    "section": "10/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.769,
    "text": "The most stable area using I s or raw W (the second best index), was the Malvinas islands, a candidate to be the best area (Fig. 3 ). The high uncertainty in the area chosen is eliminated when the support is included in the selection of the best area. Santiago has the highest number of species and harbors the highest number of endemic species, but it was not placed as the highest priority, while Malvinas island, the second most endemic area has the highest priority. The inferences based on the Table 1 First area in the ranking proposed by Posadas et al. ( 2001 ). un-sampled **data** set might be misleading, while jack-knifi ng could help to decide which is the most supported solution.",
    "id": 370,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 370,
    "created_at": "2023-03-07T16:30:27.934873Z",
    "updated_at": "2023-03-07T16:30:27.934908Z",
    "lead_time": 4.647
  },
  {
    "corpusid": 14912634,
    "section": "13/23",
    "subsection": 2.0,
    "section_pos_in_pct": 0.565,
    "text": "In contrast, distinct power curves emerged when the power **data** were stratified by SODAR measured Figure 6. I w -stratified power curves for Turbine 1 during strongly convective (I w > 17%), convective (9% < I w < 17%), and stable or strongly stable (I w < 6%) conditions, as well as the expected power curve. Under-performance is observed during strongly convective conditions, especially for higher wind speeds. I U (figure 5(b)). Figure 5(b) shows power generation **data** segregated into stable or strongly stable (I U < 10%), convective (13% I U < 20%), or strongly convective (I U > 20%) conditions. The most significant power curve differences occurred between very stable/stable and very convective conditions for wind speeds 7.0-8.5 m s −1 . In general, Turbine 1 over-performed during stable/strongly stable conditions for 5.5-8 m s −1 wind speeds and under-performed during strongly convective conditions for all wind speeds above 5.5 m s −1 . Greatest under-performance occurred at moderate wind speeds (7.0-8.5 m s −1 ) during strongly convective conditions. For example, for a wind speed of 7.5 m s −1 , mean P norm was 40% ± 6% during strongly stable/stable conditions and 23% ± 4% during strongly convective conditions, compared to the expected P norm of 33%. Among all six turbines examined, differences in normalized power between stable/strongly stable (more power) and strongly convective (less power) conditions ranged from 10 to 20%.",
    "id": 371,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 371,
    "created_at": "2023-03-07T16:30:32.451711Z",
    "updated_at": "2023-03-07T16:30:32.451743Z",
    "lead_time": 4.316
  },
  {
    "corpusid": 118072666,
    "section": "8/13",
    "subsection": 2.0,
    "section_pos_in_pct": 0.615,
    "text": "• To establish the contribution to the systematic uncertainty from the simulation of R 2 , the number of φ mesons from BB events is estimated again without the R 2 requirement, and the same procedure is applied to the Monte Carlo. The fraction of φ mesons from B decays with R 2 < 0.25 in **data** is (78.18±0.80)% while this fraction in Monte Carlo is (78.00±0.09)%, in agreement within statistical errors. We also investigate the two decay models mentioned above for their effect on the R 2 selection. We find that the largest difference between the models and our Monte Carlo distribution is 0.5%, and we take this difference as a systematic uncertainty.",
    "id": 372,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 372,
    "created_at": "2023-03-07T16:30:36.680497Z",
    "updated_at": "2023-03-07T16:30:36.680528Z",
    "lead_time": 4.031
  },
  {
    "corpusid": 245800515,
    "section": "9/16",
    "subsection": 1.0,
    "section_pos_in_pct": 0.562,
    "text": "We will compare the convergence of the L1 scheme and CQ with our estimators. The meshes will be of the form t j = T (j/N ) k with k ≥ 1; the mesh is uniform if k = 1 and graded towards 0 if k > 1. The exact initial **data** will be used, i.e., U 0 = u 0 .",
    "id": 373,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 373,
    "created_at": "2023-03-07T16:30:42.899638Z",
    "updated_at": "2023-03-07T16:30:42.899662Z",
    "lead_time": 6.037
  },
  {
    "corpusid": 207778550,
    "section": "16/48",
    "subsection": 1.0,
    "section_pos_in_pct": 0.333,
    "text": "This method may be used when the available experimental **data** can be grouped into two or more distinct sets according to the experimental methods employed in the measurements, as mentioned above in discussing Clue 5. The USU contribution, when evaluating a single physical quantity, can be determined from the standard deviation of the various evaluated mean values obtained by analyzing the several **data** sets separately, according to the distinct measurement techniques (without regard for USU ). However, if there are only two groups of **data** to consider, based on the measurement techniques, the best estimate of the USU contribution probably should be the actual difference in the evaluated mean values for the two groups (see, e.g., Sect. V E 4). If more than one physical quantity is evaluated, similar procedures can be applied separately when considering each of the individual quantities being evaluated.",
    "id": 374,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 374,
    "created_at": "2023-03-07T16:31:35.861337Z",
    "updated_at": "2023-03-07T16:31:35.861376Z",
    "lead_time": 52.775
  },
  {
    "corpusid": 118420717,
    "section": "3/10",
    "subsection": 1.0,
    "section_pos_in_pct": 0.3,
    "text": "• We perform simulations of the solar system with various initial conditions of the giant planets and the terrestrial planets (see next section) for 265 Myr with a time step of 0.01 yr (3.6525 days). The **data** is output every 1000 yr. We use the 2nd order MVS integrator of Laskar & Robutel (2001), implemented (Brož et al., 2005) in the SWIFT integration package (Levison & Duncan, 1994). The code includes digital filtering of the orbital elements using Kaiser windows (Kaiser, 1966), based on the method of Quinn et al. (1991). Any signal with a period shorter than 667 yr is suppressed. To mimic the effect of general relativity we added a correction term as described in Nobili & Will (1986).",
    "id": 375,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 375,
    "created_at": "2023-03-07T16:31:43.258873Z",
    "updated_at": "2023-03-07T16:31:43.258948Z",
    "lead_time": 7.186
  },
  {
    "corpusid": 235829209,
    "section": "9/47",
    "subsection": 1.0,
    "section_pos_in_pct": 0.191,
    "text": "Figure 2 :\n2Prediction performance of 7 th -order polynomials surrogate model for the Hénon map in the x and p plane at a given tune ν = 0.205. Each dot represents one initial condition in the testing cluster. The red dashed line is the desired/expected value. Note the original **data** has been scaled/normalized to a range of [0, 1].\n",
    "id": 376,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 376,
    "created_at": "2023-03-07T16:31:48.864906Z",
    "updated_at": "2023-03-07T16:31:48.864934Z",
    "lead_time": 5.416
  },
  {
    "corpusid": 117119864,
    "section": "12/83",
    "subsection": 3.0,
    "section_pos_in_pct": 0.145,
    "text": "Resonant ARPES reveals that parts of the FS show easily observable 4f weight at 120K. As presented in Ref. [18] this 4f weight is found in k-space in the vicinity of the low mass parts of the FS, the small hole pockets around Z and the small electron pocket around Γ. The angle integrated 4f spectrum at and above TK obtained directly [23] or by k-summing the resonant ARPES spectra [83] has the gener al appearance of the impurity-model spectra of Figs. 2 and 3. Although this finding may be plausible in view of the general theoretical correlation of smaller mass, larger energy scale and larger spectral weight, there is presently no spectral theory of t he Anderson lattice capable of providing insight into ARPES **data** at this level of detail. The so called \"LDA + DMFT\" theory discussed in section 5.2 below is promising for further analysis of such **data** for systems where the Fermi surface is well described by the LDA.",
    "id": 377,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 377,
    "created_at": "2023-03-07T16:31:59.006611Z",
    "updated_at": "2023-03-07T16:31:59.006638Z",
    "lead_time": 9.95
  },
  {
    "corpusid": 17837291,
    "section": "34/52",
    "subsection": 2.0,
    "section_pos_in_pct": 0.654,
    "text": "Here e S 1 (·) denotes the S 1 -equivariant Euler class. As in the cases we have studied earlier, the left hand side of the above formula indicates that when V = L is a line bundle, we should compare the Euler **data** Q d = ϕ ! π * e(V d ), to the Euler **data** given by\nP d = c 1 (L),d m=0 (c 1 (L) − mα).\nWhat is left is to develope uniqueness and mirror transformations, which we are unable to achieve at this moment, though they can be easily axiomized. Now let us look at K-theory formula, which can be proved by using equivariant localization in K-theory. First following the same idea, we get the explicit formula as follows:",
    "id": 378,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 378,
    "created_at": "2023-03-07T16:32:03.543665Z",
    "updated_at": "2023-03-07T16:32:03.543699Z",
    "lead_time": 4.344
  },
  {
    "corpusid": 16542351,
    "section": "1/8",
    "subsection": 1.0,
    "section_pos_in_pct": 0.125,
    "text": "H eavy quark physi cs o ers a uni que opportuni ty to expl ore the avour structure of the Standard M odel .T he thi rd fam i l y ofquarksi sthe l essknow n. Itsup-type quark, the top,hasnotyetbeen observed,al though we are getti ng m ore and m ore stri ngent l i m i ts on i ts m ass from di rect search and radi ati ve correcti on anal ysi s. O n the other hand, the correspondi ng dow n-type quark, the b, i s bei ng studi ed w i th i ncreasi ng accuracy. Both grow i ng stati sti cs and the operati on ofdedi cated vertex detector for btaggi ng athi gh energy m achi nesal ready al l ow sa good determ i nati on ofbcoupl i ngs. Present **data** cl earl y poi ntto the b asthe I 3 = {1/2 com ponent ofa weak l eft-handed i sodoubl et [ 1] .",
    "id": 379,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 379,
    "created_at": "2023-03-07T16:32:08.719024Z",
    "updated_at": "2023-03-07T16:32:08.719055Z",
    "lead_time": 4.991
  },
  {
    "corpusid": 19033986,
    "section": "7/28",
    "subsection": 3.0,
    "section_pos_in_pct": 0.25,
    "text": "Other, less established bounds, are not included. In particular, ref. [39] finds that angular regions distinct from the one observed by FERMI and so far observed only by EGRET, provide stronger constraints. We do not use here the controversial EGRET observations. Once FERMI will present **data** corresponding to other regions, it will be easy to establish bounds with the use of our approximation described in the previous section. This is done by simply rescaling our predictions for the '10 • ÷ 20 • ' region using the new J factors for the additional (yet unknown) regions. Furthermore, we do not consider the 'WMAP haze' [40] which is a hint that a possible excess in synchrotron radiation could be due to DM. The haze has been shown to be consistent with a wide variety of DM masses and final states and therefore will not constrain the space of models compared to other measurements. It would however, be interesting to study in more detail the precise predictions for the haze for those models that can fit the rest of the data.  5 shows the χ 2 as function of the DM mass for various DM annihilation (left) or DM decay (right) modes. We find that, independently of the non-observation of an excess in thē p PAMELA data, only some leptonic modes can reproduce all data. Here HESS observations play a key role demanding that the e ± excess terminates in a sharper way than what typical of non-leptonic channels, irrespectively of the DM density profile. DM heavier than 10 TeV that annihilates or decays into light quarks still provides a reasonable fit to PAMELA and FERMI data, if fitted conservatively. However it is disfavored by the HESS e + + e − data, and presumably the photon **data** as well ( in the annihilating case).",
    "id": 380,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 380,
    "created_at": "2023-03-07T16:32:22.860517Z",
    "updated_at": "2023-03-07T16:32:22.860546Z",
    "lead_time": 13.949
  },
  {
    "corpusid": 54041858,
    "section": "5/45",
    "subsection": 1.0,
    "section_pos_in_pct": 0.111,
    "text": "The set-up for AE acquisition was such that the parametric **data** (load and strain) were acquired every second in addition to the AE waveforms. Thus, the variation in stress for a given fatigue cycle was not possible for the 1 Hz data. However, for the 0.1 Hz and 0.01 Hz tests, enough load **data** were available to discern at which point of the cycle the events were occurring. The AE analysis for the 0.1 Hz test (113-3) are presented here in detail, which showed the enhanced AE activity near failure ( Figure 2b). The features found for the 0.1 Hz test were also observed for the 1 Hz tests (147-3 and 113-9) for the most part. Although both specimens were tested at 1 Hz, the same acquisition rate for load in the software, the peak and valley events could be discerned by the timing of the event. In other words, the peak and valley events occurred approximately 0.5 s from one another. Where there were differences is discussed below.",
    "id": 381,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 381,
    "created_at": "2023-03-07T16:32:34.551185Z",
    "updated_at": "2023-03-07T16:32:34.551216Z",
    "lead_time": 11.495
  },
  {
    "corpusid": 246863515,
    "section": "11/44",
    "subsection": 3.0,
    "section_pos_in_pct": 0.25,
    "text": "Then the average long-term treatment effect is identifiable:\nτ = a∈{0,1} (−1) 1−a E h E (a, X) | G = O + E P (G = E) P (G = O | X) P (G = O) P (G = E | X) I [A = a] P (A = a | X, G = E) h (S 3 , S 2 , A, X) −h E (A, X) | G = E + E P (G = E | A = a) P (G = O | X) P (G = O | A = a) P (G = E | X) I [A = a] P (A = a | X, G = E) q (S 2 , S 1 , A, X) (Y − h (S 3 , S 2 , A, X)) | G = O\nTheorem 7 shows that even under weaker Assumptions 10 and 11, outcome and selection bridge functions can be still used to identify the average long-term treatment effect. This again has the doubly robust property in that it only requires one of the bridge functions to be correct rather than both. Compared to Theorem 3, Theorem 7 additionally incorporates the ratios P (G = O | X) /P (G = E | X) to adjust for the covariate distribution discrepancy in the two types of **data** (Assumption 10). It also uses the propensity score P (A = a | X, G = E) to account for the dependence between treatment A and covariates X in the experimental **data** (Assumption 11).",
    "id": 382,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 382,
    "created_at": "2023-03-07T16:32:37.865595Z",
    "updated_at": "2023-03-07T16:32:37.865622Z",
    "lead_time": 3.129
  },
  {
    "corpusid": 17837291,
    "section": "46/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.885,
    "text": "Theorem 6. 5 .\n5Suppose that A, B have property (i) of Theorem 5.4, and that A, B are linked. Suppose that A is an Euler **data** with deg α A d ≤ −2 for all d ≺ 0, and that there exists power series f ∈ R[[K ∨ ]], g = (g 1 , .., g m ), g j ∈ R[[K ∨ ]], all without constant term,such thate f /α HG[B](t) = Ω − Ω H · (t + g) α + O(α −2 ) (6.5)when expanded in powers of α −1 . ThenHG[A](t + g) = e f /α HG[B](t). Proof: By Theorem 6.4, f, g define two mirror transformations µ f , ν g , with HG[B](t) = e f /α HG[B](t) HG[Ã](t) = HG[A](t + g) (6.6) whereB = µ f (B),Ã = ν g (A). Now bothB,Ã have property (i) of Theorem 5.4. (See remark after Theorem 6.4.) Since deg α A d ≤ −2, HG[Ã](t) has the same asymtotic form as HG[B](t) in eqn. (6.5) mod O(α −2 ). It follows that e H·t/α HG[Ã −B](t) ≡ O(α −2 ), or equivalently deg α (Ã d −B d ) ≤ −2. ThusÃ,B satisfy condition (ii) of Theorem 5.4.\n",
    "id": 383,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 383,
    "created_at": "2023-03-07T16:32:43.012817Z",
    "updated_at": "2023-03-07T16:32:43.012841Z",
    "lead_time": 4.944
  },
  {
    "corpusid": 238834501,
    "section": "13/19",
    "subsection": 1.0,
    "section_pos_in_pct": 0.684,
    "text": "In terms of the **data** matrix X with rows corresponding to days and columns to times, the 'average' type regression estimate for d • is given by where β is an estimate for the (column) vector of regression coefficients β and Ŷ is the regression estimate. This expression corresponds to an unweighted average over N (selected) days if β contains N elements with value 1/N and otherwise zeros. Viewed as a regression, an intercept is included by adding a column of ones to X T , and the estimate β d • t • for β is obtained by minimising a loss function. We denote with the subscripts the fact that β is estimated by using the observation at d • but only the times t • (excluding the eclipse) as a target. The 'smoothing' approach consists in using the columns of X corresponding to t • as the regression's targets and all remaining columns X dt • as predictors:",
    "id": 384,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 384,
    "created_at": "2023-03-07T16:32:48.796336Z",
    "updated_at": "2023-03-07T16:32:48.796363Z",
    "lead_time": 5.544
  },
  {
    "corpusid": 119419652,
    "section": "7/18",
    "subsection": 1.0,
    "section_pos_in_pct": 0.389,
    "text": "Analyses involving top quarks -the heaviest of all fundamental particles in the SM -allow for rigorous tests of recent NNLO and NNLL pQCD predictions. An unprecedented number of top quarks have been produced at the centres of the main LHC experiments in recent years; previously statistically limited searches for rare SM processes featuring top quarks are now within reach based on the √ s = 13 TeV Run 2 **data** collected so far. For some less rare and previously measured processes, such as the differential tt cross-section, statistical limitations are now being relegated to the extremes of kinematic regions of phase space, where deviations from the SM may eventually be exhibited. One set of rare processes of particular interest involves the associated production of tt pairs with W /Z or H bosons. Moreover, with the top quark as the heaviest of all SM particles, techniques to identify top quarks with highly collimated decay products could prove vital in searches for new physics: any enhancement in the rates of boosted top quarks could provide hints of as-ofyet undiscovered BSM particles. Finally, SM processes featuring top quarks often represent the dominant source of background to many searches for BSM physics; a better understanding of the kinematics of top quark events in general will lead to improved sensitivity in such searches.",
    "id": 385,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 385,
    "created_at": "2023-03-07T16:32:53.125604Z",
    "updated_at": "2023-03-07T16:32:53.125628Z",
    "lead_time": 4.085
  },
  {
    "corpusid": 17837291,
    "section": "15/52",
    "subsection": 1.0,
    "section_pos_in_pct": 0.288,
    "text": "We shall often apply the following version:\nW d ω e κ ζ = 0 r d X i * r ω v e H ζ + H ζ ,r α . Definition 4.1. Fix an invertible class Ω ∈ H * T (X) −1 . A list P : P d ∈ H * G (W d ) −1 , d ≻ 0, is a Ω-Euler **data** if on X, Ω i * r P v d = i * 0 P v r i * 0 P v d−r\n(called Euler **data** identity) for all r d, and the W d P d · ω are polynomial in α for all ω ∈ R d . By convention we set P 0 = Ω.",
    "id": 386,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 386,
    "created_at": "2023-03-07T16:32:56.012397Z",
    "updated_at": "2023-03-07T16:32:56.012480Z",
    "lead_time": 2.647
  },
  {
    "corpusid": 209991952,
    "section": "12/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.522,
    "text": "butions of the nuisance parameters encoding the systematic uncertainties. The latter are Gaussian distributions for all sources, including statistical uncertainties arising from the limited number of preselected or opposite-sign **data** events in the estimation of the reducible background, or the limited number of simulated events. Correlations of a given nuisance parameter between the backgrounds and the signal are taken into account when relevant. Table 6 presents 95% CL upper limits on the number of BSM events, S 95 , that may contribute to the SRs. Normalising these by the integrated luminosity L of the **data** sample, they can be interpreted as upper limits on the visible BSM cross-section (σ vis ), defined as σ vis = σ prod × A × = S 95 /L, where σ prod is the production cross-section of an arbitrary BSM signal process, and A and are the corresponding fiducial acceptance and reconstruction efficiencies for the relevant SR. These limits are computed with asymptotic approximations of the probability distributions of the test statistic under the different hypotheses [84]. They were confirmed to be within 10% of an alternative computation based on pseudo-experiments. The probability of the observations being compatible with the SM-only hypothesis is quantified by the p-values displayed in table 6; the smallest, for",
    "id": 387,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 387,
    "created_at": "2023-03-07T16:33:06.154261Z",
    "updated_at": "2023-03-07T16:33:06.154362Z",
    "lead_time": 6.319
  },
  {
    "corpusid": 53380409,
    "section": "29/119",
    "subsection": 1.0,
    "section_pos_in_pct": 0.244,
    "text": "The event selection discussed in Sec. VI relies on the reconstruction of electrons, muons, jets, and missing transverse momentum (with magnitude E miss T ). Although the requirements vary for the different channels, the general algorithms are introduced below. The small differences between the efficiencies measured in **data** and MC simulation are corrected for by applying scale factors to the MC simulation so that it matches the data.",
    "id": 388,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 388,
    "created_at": "2023-03-07T16:33:12.563361Z",
    "updated_at": "2023-03-07T16:33:12.563383Z",
    "lead_time": 6.165
  },
  {
    "corpusid": 119253123,
    "section": "12/32",
    "subsection": 1.0,
    "section_pos_in_pct": 0.375,
    "text": "We multiply the contamination estimate by a data-driven coefficient to take into account the low strangeness yield in the Monte Carlo compared to **data** [24]. The coefficient is derived from a fit of the discrepancy between **data** and Monte Carlo strangeness yields in the tails of the DCA XY distribution which are predominantly populated by secondaries. The factor has a maximum value of 1.07 for tracks with p T < 0.5 GeV/c and is equal to 1 for p T > 1.5 GeV/c. This factor is included in the Contamination entry in table 3.",
    "id": 389,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 389,
    "created_at": "2023-03-07T16:33:23.792777Z",
    "updated_at": "2023-03-07T16:33:23.792806Z",
    "lead_time": 10.993
  },
  {
    "corpusid": 38256305,
    "section": "13/49",
    "subsection": 1.0,
    "section_pos_in_pct": 0.265,
    "text": "False color composite Landsat 8 images, including thermal bands (thermal bands: Band 7, wavelength: 2.11-2.29 µm; Band 10, wavelength: 10.6-11.19 µm) combined with texture enhancements, provide clear visibility of linear features, such as those seen in Figure 10. The spectral emissivity properties of larger fault zones in the thermal bands are often influenced by soil moisture. Surface water often finds easy passage through fault zones and, therefore, increases soil moisture. This, in turn, influences the emissivity pattern of thermal bands. The reservoir area is characterized by estimated V s 30 **data** ranging between 600 and 760 m/s. The dam site area is obviously not affected by higher susceptibility to soil amplification, as in the case of lower V s 30 velocities (<400 m/s) within broader valleys and depressions.",
    "id": 390,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 390,
    "created_at": "2023-03-07T16:33:35.018037Z",
    "updated_at": "2023-03-07T16:33:35.018061Z",
    "lead_time": 11.011
  },
  {
    "corpusid": 236522469,
    "section": "7/33",
    "subsection": 1.0,
    "section_pos_in_pct": 0.212,
    "text": "Reference It ranges from 0 to 1, where: 0-no correct ratings 1-perfect classification [54] Where, ACCoverall: relative compliance observed between classifiers; RACCoverall: hypothetical probability compliance using the observed **data** to calculate the probabilities of each classifier to identify each category randomly; TP: true positive; TN: true negative; FP: false positive; FN: false negative; PPV: positive predictive value; TPR: true positive rate; NPV: negative predictive value; TNR: true negative rate.\nMathews Correlation Coefficient MCC = TP × TN − FP × FN √(\nThe adjusted F-score is an index that groups all elements of the original confusion matrix and gives more weight to correctly classified patterns in correctly classified classes [52]. The Matthews Correlation Coefficient has a range from −1 to 1, and when it has a value of −1, it indicates a completely wrong binary classifier, while 1 indicates a completely correct binary classifier. This index allows you to evaluate the performance of your classification model [53], whereas similarity is a metric ranging from 0 (wrong classification) to 1 (perfect classification) calculated from the averages of the classifications of the classes of interest [54].",
    "id": 391,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 391,
    "created_at": "2023-03-07T16:33:40.212175Z",
    "updated_at": "2023-03-07T16:33:40.212202Z",
    "lead_time": 4.979
  },
  {
    "corpusid": 246863515,
    "section": "24/44",
    "subsection": 2.0,
    "section_pos_in_pct": 0.545,
    "text": "Corollary 2. Suppose Assumptions 1, 4 to 7, 10 and 11 hold and Y (a) ⊥ G | S(a), U, X. Then Equation (11) in Theorem 1, Equation (15) in Theorem 2 and Equation (16) in Theorem 3 all identify the average long-term treatment effect over the experimental **data** distribution, i.e.,\nτ E = E [Y (1) − Y (0) | G = E] ,\nIn Corollary 2, we still assume the weaker conditions in Assumptions 10 and 11. But we additionally require that the experimental and observational **data** share a common conditional distribution of the potential long-term outcome. This additional assumption ensures that the bridge functions defined in terms of the observational **data** distribution can also be used to identify the average long-term treatment effect over the experimental **data** distribution.",
    "id": 392,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 392,
    "created_at": "2023-03-07T16:35:21.975974Z",
    "updated_at": "2023-03-07T16:35:21.975996Z",
    "lead_time": 101.519
  },
  {
    "corpusid": 239039030,
    "section": "19/23",
    "subsection": 1.0,
    "section_pos_in_pct": 0.826,
    "text": "\n• PRE-PROCESS: Resize the captured image to fit the predefined input shape of the DNN model. • INFERENCE: Execute **data** analysis based on pretrained DNN model. • POST-PROCESS: Extract a set of object instances, including the bounding box location and categories prediction values, and apply NMS filter. • WRITE: Render the detection results on the screen.\n",
    "id": 393,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 393,
    "created_at": "2023-03-07T16:35:27.818782Z",
    "updated_at": "2023-03-07T16:35:27.818809Z",
    "lead_time": 5.612
  },
  {
    "corpusid": 249258070,
    "section": "97/119",
    "subsection": 2.0,
    "section_pos_in_pct": 0.815,
    "text": "Second, a significant increase was observed for violent crime calls. This finding was counter to some of the initial studies using CFS **data** to examine trends in aggravated assaults (Ashby, 2020; Mohler et al., Table 3 Interrupted time series analysis using ARIMA models. 17.53*** 2020) after the initial onset of COVID-19. This discrepancy could be due to measurement differences as our measure also included shootings in addition to different types of assaults. However, these initial studies only examined trends until April or mid-May of 2020 and were perhaps unable to account for longer term effects of the pandemic. For example, recent research found an increase in gun violence from the beginning of the pandemic through March 2021 across several U.S. states, including Texas generally and the Houston region specifically, when compared to the same time-period the year prior. The increases in gun violence were attributed to both prolonged psychological distress and increased gun sales resulting from the pandemic (Ssentongo et al., 2021). The current results also suggest that COVID-19 was associated with increased demands for police responses to violence, even after the initial wave of the virus. Third, the findings indicated that officers were engaged in more frequent self-initiated patrol activities compared to the pre-pandemic data. This increase remained after the death of George Floyd (when compared to pre-pandemic data), but at a lower rate. These observed changes to proactive patrol could also be attributed to changes in the routine activities of citizens. Again, the **data** showed that three of the reactive call types significantly decreased post-pandemic and another three reactive call types experienced no changes in frequency. As Ashby (2020) noted, a reduction in reactive CFS could create additional time for officers to focus on other tasks, such as proactivity. The current results may also lend further support to the findings from Lum, Koper, et al. (2020) demonstrating how proactivity is often manifested as generalized patrol even after the pandemic, as well as Maskály et al. (2021) results showing increases in directed or extra patrols since COVID-19. While there was initial concern that the pandemic may have necessitated a move to precautionary policing, a recent survey revealed that citizens are not supportive of reducing preventative patrol even during the pandemic . In this regard, decreases in reactive demands for police services coupled with citizen expectations of police services during the pandemic could account for this increase.",
    "id": 394,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 394,
    "created_at": "2023-03-07T16:35:59.260490Z",
    "updated_at": "2023-03-07T16:35:59.260522Z",
    "lead_time": 31.221
  },
  {
    "corpusid": 245105154,
    "section": "8/25",
    "subsection": 1.0,
    "section_pos_in_pct": 0.32,
    "text": "In this section, the researchers present the proposed algorithm. The algorithm was named spreading factor congestion-aware ADR approach and is specified in Algorithm 1. The name of the algorithm came from the fact that it uses the congestion statuses of each spreading factor in the network to determine the best one for the end device that requested the ADR. The development of this algorithm was performed to answer the question: how can a modified Adaptive Data Rate algorithm be developed to efficiently manage and reduce signal interferences in LoRaWAN? And to carry out the defined research objective (i). The proposed algorithm has seven main global variables (i.e., snrList, sfList, uplinkCnt, optimizedSF, defaultMargin, snrLimit, sfUsageIndex). These variables are explained as follows: snrList: a list of SNR values to be used in determining the best **data** rate. The list is initially empty and is updated each time the end device sends an uplink to the network server. sfList: a list of SFs that corresponds to the collected SNR values. The list is also initially empty and is updated each time the end device sends an uplink to the network server. uplinkCnt: a counter to keep track of the number of uplinks that the end device has sent to the network server since the ADR was requested. optimizedSF: the SF that will represent the optimized **data** rate. This variable stores the value that the algorithm will calculate once enough **data** has been collected. defaultMargin: the device-specific margin. This value is given in the devices' datasheets. A value of 10 is used in this experiment as it is generally the value for most devices. snrLimit: a list of limits required for each SF for the receiver to be able to demodulate the received signal. These values are used in determining the margin required for optimizing the **data** rate. sfUsageIndex: a list of stored values representing the number of devices using each SF. The values will be used to determine the SF with the fewest devices using it. This list is one of the most important pieces required by the algorithm.",
    "id": 395,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 395,
    "created_at": "2023-03-07T16:37:20.788758Z",
    "updated_at": "2023-03-07T16:37:20.788790Z",
    "lead_time": 81.315
  },
  {
    "corpusid": 146010542,
    "section": "7/17",
    "subsection": 1.0,
    "section_pos_in_pct": 0.412,
    "text": "A group of students successfully proved weaknesses and imperfections of Global Positioning System (GPS). In 2013 they hacked the GPS signal on a private yacht and distributed false position **data** to navigational equipment. As the track-pilot was active, automatic correction of course had been initiated in order to put the yacht back on route (Vaas, 2013).",
    "id": 396,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 396,
    "created_at": "2023-03-07T16:37:27.968328Z",
    "updated_at": "2023-03-07T16:37:27.968402Z",
    "lead_time": 6.989
  },
  {
    "corpusid": 236196766,
    "section": "24/47",
    "subsection": 1.0,
    "section_pos_in_pct": 0.511,
    "text": "Several Docker container images are also provided to run pre-configured versions of GrimoireLab, with all services already pre-installed. They can produce complete dashboards, with raw and enriched **data** for all repositories, just by running the container with the appropriate configuration data. They can also be used with official container images for services, via docker-compose (see the companion dataset, described in Section \"A companion package and other information\", for an example of running the toolset this way, including configuration files). Docker images for GrimoireLab are stored in DockerHub, so that they can be recovered later (for any GrimoireLab release). They are also produced from Dockerfile configuration files, publicly available from GrimoireLab repositories.",
    "id": 397,
    "sentiment": "data availability statement",
    "annotator": 1,
    "annotation_id": 397,
    "created_at": "2023-03-07T16:37:34.189999Z",
    "updated_at": "2023-03-07T16:37:54.229431Z",
    "lead_time": 13.786000000000001
  },
  {
    "corpusid": 246337567,
    "section": "3/70",
    "subsection": 2.0,
    "section_pos_in_pct": 0.043,
    "text": "LD interlinking describes the task of determining whether a named resource (an entity identified by a URI) can be linked to another named resource in order to indicate that they both describe the same thing or that they are related in some capacity (Ferrara et al. 2011). The purpose of LD interlinks is to provide additional information about an entity in order to improve **data** discovery (Kim and Hausenblas 2015).",
    "id": 398,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 398,
    "created_at": "2023-03-07T16:38:03.491032Z",
    "updated_at": "2023-03-07T16:38:03.491062Z",
    "lead_time": 7.673
  },
  {
    "corpusid": 52822277,
    "section": "5/20",
    "subsection": 1.0,
    "section_pos_in_pct": 0.25,
    "text": "• LSTM: This approach is a Long Short Term Memory Recurrent Neural Network (LSTMRNN or just LSTM) based on Summerville and Mateas (2016), recreated in Tensorflow from the information given in the paper and training **data** supplied by the authors. It takes as input a game level represented as a sequence and outputs the next tile type. We modified this approach to a bidirectional Figure 2: Examples of six final levels from our study, each pair of levels from a specific co-creative agent: Markov Chain (top), Bayes Net (middle), and LSTM (bottom). These levels were selected at random from the set of final levels, split by co-creative agent.",
    "id": 399,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 399,
    "created_at": "2023-03-07T16:38:23.536829Z",
    "updated_at": "2023-03-07T16:38:23.536858Z",
    "lead_time": 19.857
  },
  {
    "corpusid": 57697459,
    "section": "3/29",
    "subsection": 1.0,
    "section_pos_in_pct": 0.103,
    "text": "Congestion control. The TCP has a certain capacity called transfer window. If we want to send **data** from Point A to Point B we load **data** into the transfer window and wait for an acknowledgement. Point B will send an acknowledge signal telling Point A that all those packets have been received. If we're successful, then the TCP becomes optimistic in the sense that it widens the transfer window so that it can send more **data** at the same time. If the transfer failed for whatever reason, then the transfer window shortens. This produces a slower traffic. TCP makes use of sequence numbering, congestion window and retransmission timer mechanisms to achieve less congestion and reliable service. TCP sender assigns sequence number for every packet sent and expects an acknowledgement before proceeding with further **data** transfer. Congestion window is used to perform congestion control, which keeps track of the number of packets that can be sent by the sender without being acknowledged by the receiving side. Basically, congestion control window decides whether TCP sender is allowed to send packets at any particular instance. TCP accomplishes reliable **data** delivery by deploying retransmission timer mechanism which detects packet loss and retransmits them. If an acknowledgement is not received before the expiry of the retransmission timer, TCP retransmits the packet and triggers congestion control.",
    "id": 400,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 400,
    "created_at": "2023-03-07T16:38:31.820910Z",
    "updated_at": "2023-03-07T16:38:31.820936Z",
    "lead_time": 8.095
  },
  {
    "corpusid": 246337567,
    "section": "3/70",
    "subsection": 3.0,
    "section_pos_in_pct": 0.043,
    "text": "LD interlinks are also known as typed links and the linking property used to describe the relationship between two URIs is known as a link-type (Neubauer 2017). Identity Links are a specific kind of typed-link where the subject and object URI refer to the same entity (Papaleo et al. 2014). Identity links are typically expressed using the owl:sameAs property, from the Web Ontology Language 2 (OWL), and the process of creating these links is referred to as instance matching. The most common type of cross-dataset interlink on the SW are owl:sameAs links (Paris et al. 2019). This property has strict semantics and should only be used where two things are identical and share the same properties (McGuinness and van Harmelen 2004). However, these strict semantics are not always followed leading to the inference of inaccurate **data** and reducing **data** quality (De Melo 2013;Halpin et al. 2010;Jaffri et al. 2008;Paris 2018;Raad et al. 2018). These inaccuracies could be reduced by employing Relationship Links-another kind of typed link used to point to related entities in other datasets (Heath and Bizer 2011). Unlike identity links, relationship links do not have to point to exactly the same thing and can thus be used to provide background knowledge and context for an entity.",
    "id": 401,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 401,
    "created_at": "2023-03-07T16:38:49.076533Z",
    "updated_at": "2023-03-07T16:38:49.076581Z",
    "lead_time": 17.059
  },
  {
    "corpusid": 252835989,
    "section": "4/53",
    "subsection": 1.0,
    "section_pos_in_pct": 0.075,
    "text": "The complete automation of smart farming is achieved by ISO 7798-2 security specifica tions-heterogeneous swarms are used in cloud middleware to operate drones, autonomous vehicles, etc. Autonomous-vehicle secure connectivity using IoT devices and key management framework is demonstrated by Jha S. et al. [22]. The vehicular network authentication is performed by blockchain based on hash graphs that can perform thousands of transactions per second and a framework designed using batch rekeying and logical key hierarchy (LKH). The 5G cooperative autonomous connectedness and driving is presented by Bagheri H. et al. [23]. This system uses 5G-based extensive authentication protocol (EAP) supporting 3GPP and non-3GPP communication networks, independent access, and mobility management function with session management function. The autonomous communication within the P2P network is demonstrated by Rahmani L. et al. [24]. A distributed hash table for agent lookup is shared by all the communicating agents and uses public-key cryptography for secure P2P communication with end-to-end encryption. An IoT mutual authentication protocol for Things-To-Things (T2T) is presented by Lounis K. et al. [25]. The T2T protocol uses physical unclonable functions (PUFs) with dual-level-challenge response pairs for the IoT authentication. V2X communication-based efficient authentication for protection against DDOS is demonstrated by Ko T. et al. [26]. The V2X system uses a security credential management system (SCMS), which classifies multiple similar messages in different categories for authentication and uses advanced verify-on-demand (AVoD) for signature verification with threat analysis. An improved isolation forest method for autonomous-vehicles-attack detection is presented by X. Duan et al. [27]. The detection of data-tampering attack is performed here using **data** mass and scoring for anomaly detection as a part of intrusion detection. An autonomous vehicle smart-parking system with the fog-blockchain architecture is presented by Shahzad A. et al. [28]. Smart parking helps to recognize the parking location with the help of fog nodes to IoT, the proof-of-concept by lightweight blockchain and a cryptographic module is utilized. Blockchain-based autonomous vehicle platoon management in 5G is demonstrated by Wu B. et al. [29]. This real-time system improves traffic management with public-key cryptography and 5Genabled revocable attribute-based encryption (RABE) with key distribution and revocation. P2P drone communication using blockchain is presented by Kumar M.S. et al. [30]. The drone base communication uses blockchain with GPS coordinates to avoid spoofing attacks and keeps the blacklisted database.",
    "id": 402,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 402,
    "created_at": "2023-03-07T16:38:58.784275Z",
    "updated_at": "2023-03-07T16:38:58.784304Z",
    "lead_time": 9.531
  },
  {
    "corpusid": 236522469,
    "section": "5/33",
    "subsection": 2.0,
    "section_pos_in_pct": 0.152,
    "text": "The neural network learns the patterns from the input **data** by reading the input **data** set and applying different computations to it. However, the neural network does not just do this once; it learns repeatedly using the input **data** set and also the results of previous tests. Each step in learning from the input **data** set is called an epoch. That is, an epoch refers to one cycle in the entire training **data** set [48,49]. Initially, CNN were trained with a large number of epochs or steps (iterations) to ensure that the smallest loss would be within that step range. After the first training, we determined an ideal number of steps to obtain the least loss to optimize the analyses and repetitions that would be performed; this test served fundamentally to know how many epochs would be necessary for the final model fit; this step was essential because an excessive number of epochs leads the model to overfitting, i.e., in this case, it will present results with very good statistical metrics, but with erroneous identification of species.",
    "id": 403,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 403,
    "created_at": "2023-03-07T16:39:06.504525Z",
    "updated_at": "2023-03-07T16:39:06.504548Z",
    "lead_time": 7.54
  },
  {
    "corpusid": 12790199,
    "section": "9/21",
    "subsection": 4.0,
    "section_pos_in_pct": 0.429,
    "text": "Translation processes convert interaction history **data** to another form, such as a conversion from visual 2-D **data** to 3-D coordinates for the arm to target, or from continuous color information to a discrete color category.",
    "id": 404,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 404,
    "created_at": "2023-03-07T16:39:11.832460Z",
    "updated_at": "2023-03-07T16:39:11.832482Z",
    "lead_time": 5.147
  },
  {
    "corpusid": 245105154,
    "section": "7/25",
    "subsection": 8.0,
    "section_pos_in_pct": 0.28,
    "text": "The last two algorithms considered in this paper are presented by Cuomo et al. [7]. In their quest to find a way to improve the ADR, Cuomo et al. [7] exposed that in some cases, forcing an end device to use a lower **data** rate can reduce collisions in a LoRa net-work. Accordingly, they proposed two implementations of the ADR. The simpler of the two, EXPLoRa-SF, was built to reduce network collisions by segregating end devices into channels based on their distance from the gateway. The second approach, EXPLoRa-AT, was built on the functionalities of EXPLoRa-SF and included an intelligence mechanism to equalize airtimes for all traffic in different transmission channels and to enforce channel usage fairness. Both algorithms were implemented and tested using LoRaSim and compared against the native ADR algorithm in terms of throughput and **data** extraction rate. The simulation environment was a network operating in the European ISM band consisting of 500 to 2000 end devices each transmitting a 160-byte packet every 5 to 3600 s. The overall results showed superior performances of both algorithms over the native ADR.",
    "id": 405,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 405,
    "created_at": "2023-03-07T16:48:33.977519Z",
    "updated_at": "2023-03-07T16:48:33.977541Z",
    "lead_time": 561.957
  },
  {
    "corpusid": 235829209,
    "section": "1/47",
    "subsection": 1.0,
    "section_pos_in_pct": 0.021,
    "text": "Consider a different scenario: an unknown nonlinear dynamical system is encapsulated into a blackbox and only an ensemble of trajectories (input and output data) are available. Comparing actual trajectories to interpolated trajectories is one way to gauge chaos. A typical method to interpolate from known trajectories is to build a surrogate model with machine learning techniques. A surrogate model needs to be established first, then predictions can be made by evaluating trajectories with given initial conditions. This procedure is known as \"supervised learning\" [2]. To validate the model, the **data** is often randomly split into two clusters: a large training set and a small testing set. A model is then constructed from the training set. The performance of the model, i.e., the prediction accuracy, is measured by comparing the testing **data** against its prediction. The performance of the model depends on the type and complexity of the model, the volume of training data, the algorithm used for training, etc.",
    "id": 406,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 406,
    "created_at": "2023-03-07T16:48:39.384987Z",
    "updated_at": "2023-03-07T16:48:39.385012Z",
    "lead_time": 5.175
  },
  {
    "corpusid": 2765985,
    "section": "2/7",
    "subsection": 1.0,
    "section_pos_in_pct": 0.286,
    "text": "Rio et al explains the structure and organization of the networking code of Linux kernel 2.4.20 [2]. They explain the main **data** structure, the sub-IP layer, the IP layer, and two transport layers: TCP and UDP. This paper contributes our understanding of how the CLNP module can be integrated inside the Linux kernel, just like the IP module integrated in the same Linux kernel.",
    "id": 407,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 407,
    "created_at": "2023-03-07T16:48:43.042499Z",
    "updated_at": "2023-03-07T16:48:43.042541Z",
    "lead_time": 3.451
  },
  {
    "corpusid": 98546104,
    "section": "5/11",
    "subsection": 1.0,
    "section_pos_in_pct": 0.455,
    "text": "Ž Accurate measurements with acquisition times of . Ž . 1 320 min show that the OD phase Ib of CH CCl 3 3 is isostructural with phase Ib of CCl . According to 4 the unambiguous similarity of the patterns, the phase Ib of methylchloroform was indexed and lattice parameters corresponding to a rhombohedral structure w x R were determined to be a s 14.584 A and a s 89.498 at 232.2 K. The characteristic temperatures and enthalpy changes associated with the II-Ib, Ib-L and Ia-L transitions were found to be in accord with the w x published **data** 4,17 . The transition temperatures are 224.0 \" 0.2, 241.9 \" 0.2 and 235.8 \" 0.3 K, respectively. The associated enthalpy changes are 7.38 \" 0.12, 2.31 \" 0.03 and 1.55 \" 0.03 kJ mol y1 , respectively.",
    "id": 408,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 408,
    "created_at": "2023-03-07T16:48:48.057978Z",
    "updated_at": "2023-03-07T16:48:48.058007Z",
    "lead_time": 4.803
  },
  {
    "corpusid": 52910603,
    "section": "2/27",
    "subsection": 3.0,
    "section_pos_in_pct": 0.074,
    "text": "We also tested effects of time on the slope paralleling our analysis of the IP -we split the dataset into thirds, coinciding with the design for the test of transitivity ( Finally, we looked at whether the slope was affected by inactivation just at the beginning of the session. As explained for the IP analysis, we narrowed our **data** to the first 4 trials of each offer (on average the first 29.9% of each session) and reran the two-way ANOVA test on this early session behavior. The critical interaction effect of Laser*Fiber was not significant. The main effect of Laser was significant. Further analysis revealed a significant difference in the simple effects between the laser-on and -off trials in the blocked fiber condition, but not in the patent fiber condition (see Supplementary file 2 for ANOVA results).",
    "id": 409,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 409,
    "created_at": "2023-03-07T16:48:56.404139Z",
    "updated_at": "2023-03-07T16:48:56.404167Z",
    "lead_time": 8.123
  },
  {
    "corpusid": 10465400,
    "section": "20/65",
    "subsection": 1.0,
    "section_pos_in_pct": 0.308,
    "text": "The genomic DNA was labelled using the Bioprime DNA labelling system (Invitrogen, UK). Hybridizations were performed, using SureHyb technology (Agilent, Stockport, Chesire, U.K.), with 2 μg of test genomic DNA labelled with Cy5-dCTP and 2 μg Cy3-dCTP (GE Healthcare Life Sciences, UK) with labelled C. difficile 630 genomic DNA as a common reference. The labelled DNA was purified using a MiniElute kit (Qiagen, Crawley, W. Sussex, UK) and the extent of Cy dye incorporation was measured using a nanodrop spectrophotometer. The test and control DNA were combined in a final volume of 39 μl and at a concentration of 2 μg each. To this mixture 10× Oligo aCGH/ChIP-on-Chip Blocking agent and 2× Hi-RPM hybridisation buffer (Agilent Technologies, U.K.) were added. The solutions were then denatured at 95°C, and incubated at 37°C for 30 min. The microarray was hybridized overnight using a SureHyb chamber at 65°C for 24 h. Slides were washed once in pre-heated Oligo aCGH/ChIP-on-chip Wash Buffer 1 for 5 min and briefly in Oligo aCGH/ChIP-on-chip Wash Buffer 2. Microarrays were scanned using an Axon 4000b array scanner (Molecular Devices, Sunnyvale, CA, USA) and intensity fluorescence **data** acquired using GenePix Pro (Molecular Devices).",
    "id": 410,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 410,
    "created_at": "2023-03-07T16:49:07.410648Z",
    "updated_at": "2023-03-07T16:49:07.410677Z",
    "lead_time": 10.791
  },
  {
    "corpusid": 251843619,
    "section": "11/73",
    "subsection": 1.0,
    "section_pos_in_pct": 0.151,
    "text": "KM conducted the qualitative **data** analysis, which included a reading of each transcript and the field notes to become familiarised with the data. 25 We used a deductive qualitative content analysis approach to code the **data** in NVivo V.12, with the CFIR as the guiding framework to categorise **data** according to factors (constructs) influencing implementation. 23 27 The CFIR was used with the intention of mapping the identified barriers and facilitators to evidence-based implementation strategies using the Expert Recommendations for Implementing Change tool following this study. 28 To enhance trustworthiness, 29 three other members of the study team (M-JS, CMS, MO'B) collectively analysed 25% of the transcripts, along with KM, to compare coding and discuss potential discrepancies in the interpretation of the CFIR constructs and codebook. The codes/CFIR constructs were summarised and organised as 'facilitators' or 'barriers' to PC-QI implementation. The study team discussed the codes and grouped them into larger categories, where they could be distilled into broader themes and subthemes of facilitators and barriers until **data** saturation was reached and no new themes were observed in the data. 25 ",
    "id": 411,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 411,
    "created_at": "2023-03-07T16:49:27.124111Z",
    "updated_at": "2023-03-07T16:49:27.124148Z",
    "lead_time": 19.495
  },
  {
    "corpusid": 5784667,
    "section": "18/27",
    "subsection": 2.0,
    "section_pos_in_pct": 0.667,
    "text": "The multiplex single base extension (SBE) reaction was performed as described by Geppert et al (26) Capillary electrophoresis of the SBE fragments was run on an ABI Prism Genetic Analyzer (3130xl; Applied Biosystems) and the **data** were analyzed using GeneMapper ® ID Analysis software v3.2.",
    "id": 412,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 412,
    "created_at": "2023-03-07T16:49:32.468238Z",
    "updated_at": "2023-03-07T16:49:32.468263Z",
    "lead_time": 5.129
  },
  {
    "corpusid": 215725489,
    "section": "7/13",
    "subsection": 1.0,
    "section_pos_in_pct": 0.538,
    "text": "In order to compare between diet and guano datasets, Principle Component Analysis (PCA) was undertaken using R function prcomp using default settings [32]. PCA plots comparing principle component 1 (PC1) and principle component 2 (PC2) for the diet dataset and the guano dataset were created using R and **data** was coloured by 1) species, 2) dietary guild and 3) size of bat. To assign dietary guilds, diets as determined from the literature (S2 Table, Fig 1) were sorted using complete-linkage clustering using the hclust package in R, to group diets by the average composition for each species. This allowed us to delimit the **data** into 7 dietary guilds (M. alcathoe was excluded from later analysis, so was not assigned to a guild. To assign size categories, the minimum and maximum weight for each bat species were gathered from the Bat Conservation Trust (http://www.bats.org.uk/pages/uk_bats.html#Resident) and plotted to visualise size groupings (please see S1 Fig).",
    "id": 413,
    "sentiment": "data availability statement",
    "annotator": 1,
    "annotation_id": 413,
    "created_at": "2023-03-07T16:50:18.463806Z",
    "updated_at": "2023-03-07T16:50:18.463832Z",
    "lead_time": 45.793
  },
  {
    "corpusid": 196643668,
    "section": "5/34",
    "subsection": 2.0,
    "section_pos_in_pct": 0.147,
    "text": "Removing all but the high quality G:C to A:T SNPs removed 51% of the remaining SNPs. The count of likely EMS-induced changes (high quality G:C to A:T) ranged from 9,668 changes in 10-2 to only 2,635 in the low coverage sequence **data** from 12-2 (Table   2). An average of 6,914 high-quality G:C to A:T SNPs were identified per sample.",
    "id": 414,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 414,
    "created_at": "2023-03-07T16:50:37.182329Z",
    "updated_at": "2023-03-07T16:50:37.182359Z",
    "lead_time": 18.493
  },
  {
    "corpusid": 219480444,
    "section": "4/14",
    "subsection": 1.0,
    "section_pos_in_pct": 0.286,
    "text": "Using the survey **data** of 315 respondents chosen randomly in many cities and regencies in North Sulawesi province, the findings suggest that not everyone has the same access to benefit programs as many of them never experience some of the programs. The findings also indicate that BPJS health insurance holders tend to have less good lives as compared to non-BPJS health insurance holders. Those who have BPJS health insurance may come from a poor community whose lives are no better than a rich community. Although the rich community is also a member of BPJS health insurance, since the launch of BPJS health insurance in 2014, the local government has encouraged the poor community to have BPJS health insurance. This encouragement increases the chunk of the poor community in the BPJS health insurance program; therefore, those who have BPJS health insurance have a less good life. Meanwhile, being the holder does not statistically affect the ability to meet daily needs. On the other hand, spending on electricity reduces the ability to meet daily needs but not necessarily make lives better or worse.",
    "id": 415,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 415,
    "created_at": "2023-03-07T16:50:43.748057Z",
    "updated_at": "2023-03-07T16:50:43.748086Z",
    "lead_time": 6.377
  },
  {
    "corpusid": 55240402,
    "section": "21/47",
    "subsection": 1.0,
    "section_pos_in_pct": 0.447,
    "text": "For all experiments, the moths were reared in the laboratory on an artificial diet (Greene et al. 1976) until reaching the adult stage. After the start of oviposition in the laboratory (age 3 d), the moths were released into the cages at the beginning of the scotophase at a density of 100 pairs (100 males and 100 females) per cage and maintained for 3 d for egg-deposition on the plants. All detected egg masses were removed 72 h after the release of the moths and taken to the laboratory, where the number of eggs and egg position on the plant (bottom, middle, and upper sector of the canopy) were evaluated. Canopy sectors were identified by measuring plants of each species and equally dividing them into 3 parts. Therefore, the size of each part is different for each host species. Even though some of the host species might be small at the time of oviposition, canopy division **data** is able to provide further insight into this biological parameter. Understanding the oviposition preference for each part of the plant canopy is important for growers to help them find eggs in the field. This is crucial to forecast pest outbreaks as well as to devise necessary biological control strategies such as the release of egg parasitoids.",
    "id": 416,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 416,
    "created_at": "2023-03-07T16:50:51.056887Z",
    "updated_at": "2023-03-07T16:50:51.056916Z",
    "lead_time": 7.115
  },
  {
    "corpusid": 232089418,
    "section": "2/15",
    "subsection": 1.0,
    "section_pos_in_pct": 0.133,
    "text": "Microarray analysis. RNA expression profiling was performed using the Agilent human lncRNA microarray V.2.0 platform (GPL18109; Agilent Technologies, Inc.). Quantile normalization and subsequent **data** processing were performed using Agilent Gene Spring Software 11.5 (Agilent Technologies, Inc.). Heatmaps representing differentially regulated genes were generated using Cluster software (version 3.0, http://www. clustersoft.com/). The microarray analysis was performed by Beijing Genomics Institute/HuaDa-Shenzhen. The lncRNAs were differentially expressed on the basis of the criteria of log2FC>1 or log2FC<-1, and P<0.05. The heatmap between the glioma tumor tissues and controls (3 vs. 3) was drawn based on the same criteria.",
    "id": 417,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 417,
    "created_at": "2023-03-07T16:50:59.702136Z",
    "updated_at": "2023-03-07T16:50:59.702159Z",
    "lead_time": 8.464
  },
  {
    "corpusid": 8177260,
    "section": "6/10",
    "subsection": 2.0,
    "section_pos_in_pct": 0.6,
    "text": "Statistical analysis. Between-group comparisons of normally distributed measurements were assessed by Student's t-test. One-way analysis of variance was used to compare more than two **data** groups and Dunnett's post-test was used to compare each group with a control (untreated) group. Two-way analysis of variance was used to compare multiple **data** groups affected by two independent variables, with a Bonferroni correction to compare groups with each other. Differences were considered statistically significant at Po0.05.\n",
    "id": 418,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 418,
    "created_at": "2023-03-07T16:51:07.260266Z",
    "updated_at": "2023-03-07T16:51:07.260301Z",
    "lead_time": 7.352
  },
  {
    "corpusid": 18075776,
    "section": "12/15",
    "subsection": 2.0,
    "section_pos_in_pct": 0.8,
    "text": "While it is clear that Ghanaian women of today (and yesterday) are expected to work and generate income, the **data** reviewed so far does not adequately speak to the state of Ghanaian women in terms of autonomy outside the financial sphere especially given the proverbs about dependence on men. In order to explore this issue further, we examine **data** on decision-making abilities.",
    "id": 419,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 419,
    "created_at": "2023-03-07T16:51:14.520703Z",
    "updated_at": "2023-03-07T16:56:27.782248Z",
    "lead_time": 9.639
  },
  {
    "corpusid": 119080499,
    "section": "14/30",
    "subsection": 1.0,
    "section_pos_in_pct": 0.467,
    "text": "The localized nature of the vacancy state in LaH x is consistent with the temperature dependence of the d.c. resistivity **data** at room temperatures, which has a temperature depedence consistent with variable range hopping [11]. Localized states are a prerequisite for variable range hopping. In the next section we will discuss the electronic band width of the vacancy states.",
    "id": 420,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 420,
    "created_at": "2023-03-07T16:51:19.783788Z",
    "updated_at": "2023-03-07T16:51:19.783813Z",
    "lead_time": 5.062
  },
  {
    "corpusid": 52910603,
    "section": "15/27",
    "subsection": 1.0,
    "section_pos_in_pct": 0.556,
    "text": "The average choice behavior across sessions ( Figure 2B) was computed by subtracting the log indifference point from the log of the offer ratios for each session. The relative offer ratios were then binned into the intervals shown in Figure 2B. A probit regression was then performed on the aligned and averaged choice **data** for visual comparison. To test the effect of inactivation of medial OFC on the indifference point and the inverse slope, a fully balanced design was implemented with fibertype (blocked/patent) and laser (on/off) as the factors in a two-way ANOVA. The number of repetitions of the design (n = 50) was used to match our previous paper of inactivation of lateral OFC . As noted above, some sessions (n = 4) were dropped due to IPs exceeding a 6:1 ratio. Repetitions of the design were also used as a blocking factor due to the wide range of subjective indifference points for each rat and pellet-pair. This block factor had a main effect of p<0.01 in all cases. This test was repeated on a subset of the sessions in which rats showed a significant preference for one of the pellets. In order to determine the threshold for indifference points sufficiently far from the 1:1, trials from sessions with the blocked fiber were randomly split into two separate groups. A bootstrap on the difference between the log of the indifference point for each group was used to determine a significance threshold (a = 0.05) for indifference point shifts.",
    "id": 421,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 421,
    "created_at": "2023-03-07T16:52:15.092453Z",
    "updated_at": "2023-03-07T16:52:15.092488Z",
    "lead_time": 51.593
  },
  {
    "corpusid": 248361238,
    "section": "6/33",
    "subsection": 1.0,
    "section_pos_in_pct": 0.182,
    "text": "Clean **data** (127.29 Gb in total) were obtained from 18 libraries (2 treatments × 3 biological replicates × 3 developmental stages). According to the paraffin sectioning results, flower buds of either the control or GA 3 -treated group collected at 40, 80, and 140 DABT were in stage 3, 4, and 5, respectively (Fig. 2). Hence, we named these two sets of samples CK3-5 and GA3-5. Overall, the Q30 of all clean reads was >91%, with a GC content of 46.66-52.37. Detailed statistics of the clean reads are listed in Supplementary Data Table S1. Lacking a reference genome sequence of P. callosum, we de novo assembled the total clean reads, thus obtaining 157 463 unigenes having an average length and N50 length of 811.51 and 1282 bp, respectively (Supplementary Data Table S2). Principal component analysis (PCA) was performed for all samples spanning the flower developmental stages (Supplementary Data Fig. S3a). Except for stage 3, the other samples from different biological replicates clustered separately in the PCA biplot, following their distinct developmental stages.",
    "id": 422,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 422,
    "created_at": "2023-03-07T16:52:44.667835Z",
    "updated_at": "2023-03-07T16:52:44.667870Z",
    "lead_time": 29.366
  },
  {
    "corpusid": 134734280,
    "section": "4/12",
    "subsection": 1.0,
    "section_pos_in_pct": 0.333,
    "text": "The message from this study is clear. p-XRF's role in characterising coarse wares such as Impasto and Cooking wares begins after their fabrics have been defined, either macroscopically or petrographically, p-XRF's considerable attributes then come into play providing in the field rapid analyses of the cut (sawn) surface of large numbers of sherds; this can be viewed as a 'screening' procedure generating chemical **data** which is scrutinised in the light of the fabric classification to make useful, if broadly based statements about identity, for instance precisely those elucidated in this studyvolcanic vs. non-volcanic or local vs. non-local. The chemical classification should not be expected to be amenable to a more detailed level of interpretation, paralleling the outcome of the other p-XRF  investigation at San Vincenzo on soils from different excavated contexts (Di Renzoni, et al., 2016). For sure, there is room for the analytical protocol to be refined beyond that described in the present study in order to improve accuracy, to allow additional elements to be included in the **data** set and to calibrate with respect to the corresponding **data** obtained by destructive analysis such as benchtop WDXRF (Jones & Campbell, in press). We suggest that p-XRF should not be habitually compared to ICP/NAA and repeatedly be found wanting in its inability to cover all elements and /or display equivalent levels of accuracy for all elements. At the same time p-XRF should not develop a 'scatter gun' reputation, generating one-off **data** which cannot be reused or can only be used for internalin-house -purposes. It is time to look boldly at its greatest attribute, that is, that of non-destructive, in situ, analysis aimed at analysing large quantities of sherds; equally important is our ability to formulate appropriate questions that it can answer, satisfactorily and conclusively. In the case of impasto ware there is already a good understanding of the origin and production of these wares; so the need now is to process large quantities of newly excavated material, and to assess whether they are imported or not. This study has shown that this type of assessment (local vs non local) is indeed feasible and effective. Coarse wares are amongst the most common fabrics found in many archaeological sites so the implication of the results of this study goes beyond the remit of our work on Stromboli.  \n",
    "id": 423,
    "sentiment": "maybe",
    "annotator": 1,
    "annotation_id": 423,
    "created_at": "2023-03-07T16:53:08.684067Z",
    "updated_at": "2023-03-07T16:53:08.684093Z",
    "lead_time": 23.818
  },
  {
    "corpusid": 2111254,
    "section": "7/20",
    "subsection": 3.0,
    "section_pos_in_pct": 0.35,
    "text": "Both our results and experimental **data** suggest Sok2 has an extensive interaction with HOG pathway, and may be a novel transcription factor in this pathway.",
    "id": 424,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 424,
    "created_at": "2023-03-07T16:53:13.234338Z",
    "updated_at": "2023-03-07T16:53:13.234372Z",
    "lead_time": 4.345
  },
  {
    "corpusid": 26536498,
    "section": "4/8",
    "subsection": 1.0,
    "section_pos_in_pct": 0.5,
    "text": "The downturned wing and thorax disruption phenotypes of parkin mutant flies are caused by the degeneration of the IFMs (21,30). Therefore, we investigated whether DmGSTO1 prevented muscle degeneration in park 1 mutants. As determined by histological analysis of thoracic IFMs, the integrity of IFMs in dorsal longitudinal muscles, which regulate adult wing posture, was clearly disrupted in park 1 mutants (Fig. 2E). Although DmGSTO1 null mutants showed a normal muscle phenotype, park 1 /DmGSTO1 null double mutants showed dramatically enhanced degeneration of IFMs compared with park 1 single mutants (Fig. 2E). Furthermore, overexpression of DmGSTO1A using a ubiquitous driver, Tub-Gal4, suppressed the degeneration of IFMs in park 1 mutants. Overexpression of DmGSTO1A in park 1 mutants resulted in regular and compact muscle tissues in the dorsal longitudinal IFMs, which were similar to those of wild type flies except for occasional vacuoles (Fig. 2E). These **data** suggest that DmGSTO1 expression partially rescues the morphological defects and muscle degeneration in park 1 mutants.",
    "id": 425,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 425,
    "created_at": "2023-03-07T16:53:20.498130Z",
    "updated_at": "2023-03-07T16:53:20.498155Z",
    "lead_time": 7.069
  },
  {
    "corpusid": 232291095,
    "section": "16/24",
    "subsection": 2.0,
    "section_pos_in_pct": 0.667,
    "text": "Our study has several strengths. The health system covers a large, diverse geographic region within the New York metropolitan area. A single record system for all hospital sites and shared institutional protocols for all obstetrical units allowed uniformity of **data** collection. Unlike other large registry studies, 45,46 which only include suspected or confirmed cases of COVID-19, our study also includes all patients who had a negative test result for the virus under a universal testing protocol. Providing adequate context, with more granular **data** that are not geographically disaggregated, is essential when evaluating disparities. 47 Data in a vacuum can lead to erroneous and dangerous conclusions that reinforce stereotypes.",
    "id": 426,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 426,
    "created_at": "2023-03-07T16:53:26.368531Z",
    "updated_at": "2023-03-07T16:53:26.368559Z",
    "lead_time": 5.634
  },
  {
    "corpusid": 252664998,
    "section": "21/28",
    "subsection": 1.0,
    "section_pos_in_pct": 0.75,
    "text": "Johannes Tröger conceptualized this work; he drafted the manuscript and edited the final version. Ebru Baykara contributed to the overall interpretation of the work and drafting of the manuscript. Elisa Mallick, Simona Schäfer, Louisa Schwed, and Mario Mina implemented the biomarker, analyzed the speech, conducted the statistical work, as well as drafted the methods and results sections of this article. Daphne ter Huurne and Nina Possemis acquired parts of the data, contributed to the clinical interpretation of the results, and revised the document. Jian Zhao oversaw the design of the V3 framework validation pipeline from a regulatory standpoint and revised the document. Nicklas Linz contributed to the overall concept of this research and revised the manuscript. Inez Ramakers is responsible for the DeepSpA study and **data** acquisition, drafted DeepSpA relevant parts of the document, and revised the manuscript. Craig Ritchie is the principal investigator of SPeAk, responsible for the concept and **data** acquisition, drafted SPeAk relevant parts of the document, and revised the manuscript.",
    "id": 427,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 427,
    "created_at": "2023-03-07T16:53:34.609267Z",
    "updated_at": "2023-03-07T16:53:34.609290Z",
    "lead_time": 8.01
  },
  {
    "corpusid": 249932676,
    "section": "15/33",
    "subsection": 1.0,
    "section_pos_in_pct": 0.455,
    "text": "Collecting **data** from the same source (employees) may cause common method bias. Measures were taken to reduce the possible common method bias during **data** collection, such as using leader-member pairing mode and multiple sources. Specifically, independent variables and dependent variables were collected separately, by having leaders instead of employees report on employee innovative behavior. Furthermore, this study also used Harman's single-factor test to examine the possible common method deviations. The results showed that the most covariance explained by one factor was 20.7%, less than the cutoff value of 50%. The cumulative can explain 71.40% of the variance. As no single factor explained large variance, common method bias was not a potential problem in our study.",
    "id": 428,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 428,
    "created_at": "2023-03-07T16:53:51.734573Z",
    "updated_at": "2023-03-07T16:53:51.734598Z",
    "lead_time": 16.899
  },
  {
    "corpusid": 150005643,
    "section": "11/12",
    "subsection": 1.0,
    "section_pos_in_pct": 0.917,
    "text": "This approach that taught p-o, m-o, p-m and their interrelationships was also effective in teaching students in grades 4 and above with specific learning disabilities, such as dyslexia with or without co-occurring dysgraphia (impaired handwriting) . Both behavioral and brain imaging **data** before and after instruction for children in grades 4 to 9 who met evidencebased criteria for dyslexia, characterized by spelling as well as reading disability, showed significant gains in spelling achievement and brain normalization during spelling tasks (Berninger & Richards, 2010). This occurred after receiving instruction in p, o, and m awareness and their interconnections (e.g., through word sorts, Bear, Ivernezzi, Templeton, & Johnston, 2015;see Berninger et al., 2008, Study 1) and/or orthographic patterns in word-specific spellings (see Berninger et al., 2008, Study 2). In both studies, instructional activities also facilitated transfer of ideas and word concepts through spelling to composing. Thus, pom instruction can improve idea expression in written language (Bahr et al., 2009;Nagy et al., 2014).",
    "id": 429,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 429,
    "created_at": "2023-03-07T16:54:00.595530Z",
    "updated_at": "2023-03-07T16:54:00.595565Z",
    "lead_time": 8.622
  },
  {
    "corpusid": 25460449,
    "section": "1/31",
    "subsection": 1.0,
    "section_pos_in_pct": 0.032,
    "text": "The function of the SG-SSPN subcomplex is to stabilize ␣-DG at the sarcolemma Durbeej et al., 2000;Holt et al., 1998;Straub et al., 1998), thereby completing the linkage between the extracellular matrix and the F-actin cytoskeleton (Ervasti and Campbell, 1993). Disruption of this linkage renders the sarcolemma susceptible to contractioninduced damage (Petrof et al., 1993). SSPN is absent from the sarcolemma of patients with sarcoglycan-deficient AR-LGMD, indicating that membrane targeting and stability of SSPN is dependent on the SGs (Crosbie et al., 1999). Sarcospan is not restored to the sarcolemma even in AR-LGMD cases with partial SG deficiency where three out of four The dystrophin-glycoprotein complex spans the muscle plasma membrane and provides a mechanical linkage between laminin in the extracellular matrix and actin in the intracellular cytoskeleton. Within the dystrophinglycoprotein complex, the sarcoglycans and sarcospan constitute a subcomplex of transmembrane proteins that stabilize ␣-dystroglycan, a receptor for laminin and other components of the extracellular matrix. In order to elucidate the function of sarcospan, we generated transgenic mice that overexpress sarcospan in skeletal muscle. Sarcospan transgenic mice with moderate (tenfold) levels of sarcospan overexpression exhibit a severe phenotype that is similar to mouse models of laminindeficient congenital muscular dystrophy (MD). Sarcospan transgenic mice display severe kyphosis and die prematurely between 6 and 10 weeks of age. Histological analysis reveals that sarcospan expression causes muscle pathology marked by increased muscle fiber degeneration and/or regeneration. Sarcospan transgenic muscle does not display sarcolemma damage, which is distinct from dystrophin-and sarcoglycan-deficient muscular dystrophies. We show that sarcospan clusters the sarcoglycans into insoluble protein aggregates and causes destabilization of ␣-dystroglycan. Evidence is provided to demonstrate abnormal extracellular matrix assembly, which represents a probable pathological mechanism for the severe and lethal dystrophic phenotype. Taken together, these **data** suggest that sarcospan plays an important mechanical role in stabilizing the dystrophin-glycoprotein complex.",
    "id": 430,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 430,
    "created_at": "2023-03-07T16:54:25.571283Z",
    "updated_at": "2023-03-07T16:54:25.571307Z",
    "lead_time": 24.749
  },
  {
    "corpusid": 224886410,
    "section": "9/35",
    "subsection": 1.0,
    "section_pos_in_pct": 0.257,
    "text": "Variations of color parameters or their ratios reflect complex forming processes in the interplay of dust sedimentation, pedogenesis and reworking and their interpretation is not always straightforward. Sprafke (2016) uses color **data** to enhance field descriptions objectively, with stratigraphic units classified with master horizons (A, B, C and combinations) and their subordinate characteristics (FAO, 2006). Pedological subdivisions of LPS (Bronger, 1976(Bronger, , 2008Schirmer, 2000Schirmer, , 2016 are reasonable, as visible changes in the stratigraphy of loess profiles are mainly related to post-depositional alteration of the deposits. Numerical color values support the precise separation of stratigraphic units and the classification of the units in relation to each other based on changes in certain color components (Sprafke, 2016). Color-based stratigraphy supports thorough field documentation but remains entirely descriptive and does not discriminate between in situ and reworked material.",
    "id": 431,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 431,
    "created_at": "2023-03-07T16:54:32.395287Z",
    "updated_at": "2023-03-07T16:54:32.395309Z",
    "lead_time": 6.573
  },
  {
    "corpusid": 236684590,
    "section": "1/15",
    "subsection": 2.0,
    "section_pos_in_pct": 0.067,
    "text": "To determine the values of the lower escarpment angles in (19) for the first time conducted ridges experimental studies composed of sediments with the average particle diameter of d=0.23 mm. Analyzing of experiments results involving materials of field studies and experimental **data** of other authors, he concluded that the angle of the ridges lower escarpment in changing their forms until antidune does not depend on the flow hydraulic characteristics and it is determined by the value of the relative density of noncohesive grounds: = 7.08 0.19 1 (1)",
    "id": 432,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 432,
    "created_at": "2023-03-07T16:54:44.204584Z",
    "updated_at": "2023-03-07T16:54:44.204607Z",
    "lead_time": 11.569
  },
  {
    "corpusid": 25460449,
    "section": "6/31",
    "subsection": 3.0,
    "section_pos_in_pct": 0.194,
    "text": "One function of the SG-SSPN subcomplex is to stabilize ␣-DG at the sarcolemma (Holt et al., 1998). By examining extraction of ␣-DG with SDS, we show that disruption of proper SG-SSPN protein interactions within the sarcolemma of SSPN-Tg muscle results in destabilization of ␣-DG. The role of ␣-DG in organization of the extracellular matrix has been well-established (Brown et al., 1999;Henry and Campbell, 1998;Henry et al., 2001;Kanagawa et al., 2005). We now show that SSPN overexpression disrupts formation of the extracellular matrix, as revealed by uncoupling of perlecan, laminin and collagen VI co-localization and as seen by electron microscopy. Laminins represent one of the major components of the extracellular matrix and mutations in the LAMA2 gene, which encodes laminin-␣2 chain, cause congenital MD (Allamand et al., 1997;Helbling-Leclerc et al., 1995;Miyagoe-Suzuki et al., 2000). Although the DGC is normally expressed in mouse models with defects in laminin (dy/dy, dy w /dy w , dy 3k /dy 3k , dy Pas /dy Pas , dy 2j /dy 2j ; dy = Lama2), the linkage between the extracellular matrix and the cytoskeleton is disrupted (Besse et al., 2003;Guo et al., 2003;Kuang et al., 1998;Miyagoe et al., 1997;Sunada et al., 1994;Xu et al., 1994). We provide **data** to demonstrate that the extracellular matrix, including laminin, exhibits abnormal structure in phenotypic SSPN-Tg muscle. Taken together, our **data** support a necessary role for SSPN in organizing proteins within the DGC and in assembly of the extracellular matrix.",
    "id": 433,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 433,
    "created_at": "2023-03-07T16:54:56.914630Z",
    "updated_at": "2023-03-07T16:54:56.914657Z",
    "lead_time": 12.478
  },
  {
    "corpusid": 55240402,
    "section": "21/47",
    "subsection": 1.0,
    "section_pos_in_pct": 0.447,
    "text": "For all experiments, the moths were reared in the laboratory on an artificial diet (Greene et al. 1976) until reaching the adult stage. After the start of oviposition in the laboratory (age 3 d), the moths were released into the cages at the beginning of the scotophase at a density of 100 pairs (100 males and 100 females) per cage and maintained for 3 d for egg-deposition on the plants. All detected egg masses were removed 72 h after the release of the moths and taken to the laboratory, where the number of eggs and egg position on the plant (bottom, middle, and upper sector of the canopy) were evaluated. Canopy sectors were identified by measuring plants of each species and equally dividing them into 3 parts. Therefore, the size of each part is different for each host species. Even though some of the host species might be small at the time of oviposition, canopy division **data** is able to provide further insight into this biological parameter. Understanding the oviposition preference for each part of the plant canopy is important for growers to help them find eggs in the field. This is crucial to forecast pest outbreaks as well as to devise necessary biological control strategies such as the release of egg parasitoids.",
    "id": 434,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 434,
    "created_at": "2023-03-07T16:55:10.425101Z",
    "updated_at": "2023-03-07T16:55:10.425127Z",
    "lead_time": 13.291
  },
  {
    "corpusid": 7894599,
    "section": "15/26",
    "subsection": 3.0,
    "section_pos_in_pct": 0.577,
    "text": "To further verify that Na + depletion induces an oxidative stress response of P. gingivalis, we carried out comparative analyses of small metabolites formed in parent and rprY mutant cells grown (2 hr) in the presence and absence of Na + . The **data** was analyzed in several ways including differences between parent and mutants strains in normal medium; in Na + -depleted medium; and metabolites that responded to Na + stress in parent or mutant. The p-values obtained from Two-Way ANOVA statistical analyses indicated whether differences were due to the effect of the rprY mutation or to Na + stress or both. Accordingly, as seen in Table 3, certain significant effects appeared to relate to the sodium status, e.g., in Na + stress, higher methionine sulfoxide, lower ketosphingosine, and more of certain dipeptides. Other effects appeared more related to RprY function than to Na + , e.g., in the mutant, higher undecanoate and lower thiamine. While it is difficult to make close connections of these changes with physiology (see Discussion), we here observe only that this initial analysis of a large number of metabolites from biosynthesis, catabolism and central metabolism reveals perturbations in relatively few of them in the four situations examined. Finally, we compared growth of a P. gingivalis oxyR mutant with that of the parent strain in Na + -depleted and normal media. As observed previously, the parent strain was able to grow in the Na +depleted medium while the oxyR mutant could not (Fig. 5), similar to the rprY mutant (Fig. 2). Since OxyR is a master regulator of genes involved in oxidative stress in Gram-negative bacteria [18], we infer that Na + depletion causes oxidative stress in the P. gingivalis cells.",
    "id": 435,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 435,
    "created_at": "2023-03-07T16:55:53.435883Z",
    "updated_at": "2023-03-07T16:55:53.435907Z",
    "lead_time": 42.802
  },
  {
    "corpusid": 2111254,
    "section": "9/20",
    "subsection": 2.0,
    "section_pos_in_pct": 0.45,
    "text": "Deciphering the signal transduction in normal cells and cancer cells is an essential step in curing cancer. In spite of its importance, understanding of the human signaling pathways is very limited. In fact, predicting the regulatory relationship between kinases/ phosphatases and transcription factors remains an extremely difficult problem. The methodology proposed in this study with hetero-regulatory similarity score and the hetero-regulatory module attempts to solve this problem. Our results demonstrate that signal transduction can be accurately recapitulated by a multilevel analysis of large-scale datasets. Although this study is conducted and tested in the model organism S.cerevisiae, we suppose that this method can be easily exploited in other organisms when the **data** becomes available. Currently, the binding specificity of many transcription factors has been studied through ChIP-chip and ChIP-seq experiments in human. With the recent development in RNAi technology, the construction of kinase/phosphatase single mutation cell lines and genome-wide measurement of gene expression level in these cell lines will be straightforward. Thus, our approach serves as a promising tool for the discovery of signaling pathways in human.",
    "id": 436,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 436,
    "created_at": "2023-03-07T16:56:03.105241Z",
    "updated_at": "2023-03-07T16:56:03.105271Z",
    "lead_time": 9.432
  },
  {
    "corpusid": 134689280,
    "section": "6/20",
    "subsection": 3.0,
    "section_pos_in_pct": 0.3,
    "text": "First, what appears to be a truncation is not necessarily truncated. The features could be intentionally constructed, i.e. actually preserved and originally finished that way. This is arguably the most important distinction: do we see the representation of the finished article thanks to decent preservation or is the shape of this feature a representation of something broken? If the latter, there are still many options. Was the feature destroyed and, if so, when, by whom, and how? Did it deteriorate over time and, if so, by gradual dilapidation of the original feature after disuse or due to other site formation processes? Was it damaged by modification, reuse, or reappropriation in the ancient or recent past? Did it suffer from decay of perishable building materials or decay due to the perishing of originally incorporated natural elements (such as trees and plants)? Was (part of) the feature removed by either animals or humans after disuse or abandonment? Without a symbology for line ends, when conditions of archaeological recording allow for it, the end user will once again rely on rules of thumb to carry out the conjectures. Fortunately this can be done in critical and archaeologically knowledgeable and sensible ways (see Vis, under review, 2014b). Once the metadata of the project as well as spatial **data** and analogous information from historic and cultural proximity have been exhausted, one can still apply visual and morphological contrast when constructing complementary data, document the applied rules of thumb, and mark up **data** for easy separation of these conjectures from retentions of originally acquired spatial data.",
    "id": 437,
    "sentiment": "not",
    "annotator": 1,
    "annotation_id": 437,
    "created_at": "2023-03-07T16:56:11.708642Z",
    "updated_at": "2023-03-07T16:56:11.708671Z",
    "lead_time": 8.385
  }
]