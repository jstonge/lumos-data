corpusid,section,subsection,section_pos_in_pct,text
53380409,58/119,1.0,0.487,"The combination of the individual channels proceeds with a simultaneous analysis of the signal discriminants across all of the channels. For each signal model being tested, only the channels sensitive to that hypothesis are included in the combination. The statistical treatment of the **data** is based on the ROOFIT [61], ROOSTATS [62], and HISTFACTORY [63] **data** modeling and handling toolkits. Results are calculated in two different signal parametrization paradigms, corresponding to one-dimensional upper limits on the cross section times branching fraction (σ × B) and two-dimensional limits on coupling strengths. The statistical treatment of each case is described here."
221635250,11/22,1.0,0.5,"Raw mobility spectra for positively charged THABr clusters taken with the Perez-LT-3 o DMA, with the blower running at the lowest (3200 rpm, gray) and the highest (9000 rpm, black) settings tested. The aerosol flow was between 1.3 and 1.5 L/min. Fig. 4 shows raw mobility spectra for positively charged THABr clusters at the lowest and the highest settings of the blower used, showing a drastic increase in resolution as the flow rate goes up. The bipolar source used produces dominantly singly charged ions (Fernandez de la Mora & Barrios, 2017), so the ordering of the peaks in the spectrum reveals directly their composition: monomer (bare cation), dimer, trimer, etc, from left to right. The upper spectrum taken at 3200 rpm resolves up to 18 clusters. The lower spectrum taken at 9000 rpm goes only up to the octamer. Note that the noise level is about 0.1 fA, as expected for this electrometer. As a result of the rather low signal, the zero drift of the electrometer is not negligible. It was corrected by assuming a linear variation of the baseline with the voltage, such that the horizontal regions between isolated peaks would be indeed flat. As an example, in the **data** shown at 9000 rpm (lower continuous black line) the last datum that reads approximately 0 fA when corrected would have read 0.4 fA, and the flat theoretically horizontal regions between peaks 1-6, if not corrected, would have had an unphysical finite upward slope. Uncorrected **data** are shown also for reference as the dotted line falling slightly above the corrected continuous line for 9000 rpm. Spectra were taken at the pump settings and flow rates indicated in Table 3. The peaks for the various clusters in these spectra taken at several pump speeds were fitted to Gaussian curves to obtain the resolving powers shown in Fig. 5. The figure includes the continuous gray line C V 1/2 scaling with peak voltage as theoretically expected from diffusive broadening (Rosell et al., 1996). The coefficient C (1.55 V − 1/2 ) used is not obtained by any rigorous theoretical analysis, but empirically as a low-voltage fit (gray line in Fig. 5) to the upper envelope of the data:"
53380409,10/119,1.0,0.084,"The combination of the individual channels proceeds with a simultaneous analysis of the signal discriminants across all of the channels. For each signal model being tested, only the channels sensitive to that hypothesis are included in the combination. The statistical treatment of the **data** is based on the ROOFIT [61], ROOSTATS [62], and HISTFACTORY [63] **data** modeling and handling toolkits. Results are calculated in two different signal parametrization paradigms, corresponding to one-dimensional upper limits on the cross section times branching fraction (σ × B) and two-dimensional limits on coupling strengths. The statistical treatment of each case is described here."
17671315,18/32,2.0,0.562,"Since the differentiation of noisy **data** is an unstable procedure andf (t − λ) is the kernel of the integral equation (56), we calculate the derivativef (t) in a special way. For each point t k := z k , k = 1, ..., M z − 1 we approximate the derivativef (t k ) aŝ
f (t k ) ≈f (t k+1 ) −f (t k−1 ) 2h z .(59)
Next, we extend this approximation as the discrete even function for t k = −z k , k = 1, ..., M − 1. Even though (59) seems to be a non-regularizing procedure, it works quite well for our goal, since in the discrete integration in (56) the right hand side of (59) is actually multiplied by h z /2."
2423505,10/23,1.0,0.435,"Often, the simplifying assumptions of the previous section doesn't hold. This section discuss method that use prior probabilities estimated on the source **data** to regularize the model. We first cover priors in the bayesian sense, and then some examples of discriminative methods. Ei("
236684590,11/15,1.0,0.733,"As a result of statistical processing of our laboratory **data** and the Main canal diversion (MCD) spatial **data** model (Figure 4) using correlation obtained the following relation to calculate the magnitude of ridge lower escarpments with the magnitude of the correlation coefficient η ху =0,83: The analysis of the graphical relationship in Figure 4a shows that the ridge lower escarpment coefficient (m K ) is inversely proportional to the velocities ratio ϑ/ϑ 0 , i.e., with a flow rate increase, the ridge lower escarpment becomes steeper. Within values of the lower escarpment coefficient m K =0.1÷1.0, the ridge is washed away."
118927342,4/10,1.0,0.4,"Finally, we remark that we cannot explain the **data** of B(B 0 s → ppK ± π ∓ , ppK + K − ) = (1.5 ± 0.7, 4.6 ± 0.6) × 10 −6 measured by the LHCb [9] due to the lack of the information for the transition form factors ofB 0 s → (K + π − , K + K − ). This calls for the theoretical and experimental studies of the three-body mesonicB 0 s decays that could proceed with theB 0 s → M 1 M 2 transitions, such as theB 0 s → D * − s π + K 0 ,B 0 s → D * 0 π + K − (K + K − ) and B 0 s → ρ − π + K 0 decays with one of the mesons to be a vector one, in order to extract both (h, w − ) in Eq. (6). On the other hand, the observedB 0 s → D 0 K + π − andB 0 s → D 0 K + K − decays [20] are also important as they relate to w − . "
119351763,8/10,1.0,0.8,"Also plotted in Fig. 5, where they are labelled Ha1 and Ha0, are smooth curves representing H a1 and H a0 , the analytic approximations to H m obtained by differentiating the asymptotic equations (6) and (8)  (The corresponding equation for H ✷ = −µ −1 dW ✷ /dǫ is not plotted in Fig. 5: it is practically indistinguishable from H a1 over the appropriate range 0.5 ≤ m ≤ 1.) Both Equations (10) and (11) provide a good fit to dW m /dm over their appropriate **data** ranges, 0 ≤ m ≤ 0.5 for H a0 and 0.5 ≤ m ≤ 1 for H a1 . As one would expect for such asymptotic approximations, the fits are best towards the limits m = 0 and m = 1. Unlike the expressions (6) and (8), from which they are derived, equations (10) and (11) do not lead to coincident values at m = 0.5. That expressions (6) and (8) should yield the same value for W m at m = 0.5 is a remarkable coincidence; that their slopes dW m /dm should also agree there, is scarcely to be expected! In fact the discontinuity between them is in remarkably good agreement with the corresponding step in the **data** for dW m /dm in the vicinity of m = 0.5."
38256305,20/49,1.0,0.408,"There is a strong need to create and improve the systematic and standardized inventory of areas like Koyna that are susceptible to seismic hazards. These hazards can be amplified by local site conditions pertaining to higher ground motions or can be related to secondary effects, such as landslides, liquefaction or compaction. The present study has demonstrated the feasibility of remote sensing and GIS creating a basic, yet standardized dataset, with relatively low financial resources. By using open-source **data** and web gateways to acquire comprehensive geologic, geophysic and geomorphologic information that influences local site conditions in the case of stronger earthquakes."
17671315,26/32,1.0,0.812,"Figure 4 :
4a) The graph of the function ε r (x) for which the **data** are computationally simulated via solution of the forward problem
"
208006088,4/26,1.0,0.154,"Our unifying Mixing Model Hierarchy begins with Linear Mixing Models (LMMs), a simple class of MOGP models typically characterised by low-rank covariance structure. An LMM decomposes a signal f (t) comprising p outputs into a fixed basis h 1 , . . . , h m ∈ R p whose coefficients x 1 (t), . . . , x m (t) are time-varying and modelled independently with Gaussian processes:
f (t) = h 1 x 1 (t) + . . . + h m x m (t) = h 1 · · · h m H x 1 (t) · · · x m (t) T x(t) = Hx(t).
The noisy signal y(t) is, then, generated by adding N (0, Λ)-distributed noise to f (t). Intuitively, this means that the p-dimensional **data** lives in a ""pancake"" around the m-dimensional column space of H, where typically m p."
118522044,28/41,1.0,0.683,"If we consider the transverse susceptibility χ yy , following the argument of the previous paragraphs, we interpret the diagonal entry χāā(ω) of the transport matrix (6.10) as the transverse susceptibility χ yy . The numerical **data** for χ (static) yy are plotted in Figure 10 for c = 1 10 and c = − 3 10 . It is interesting to note that, unlike Re χ "
17837291,1/52,2.0,0.019,"Step 3. Linking theorem. The construction above is functorial, so that if X comes equipped with a torus T action, then the entire construction becomes G = S 1 × T equivariant and not just S 1 equivariant. In particular, the Euler **data** identity is an identity of G-equivariant classes on W d . Our problem is to first compute the G-equivariant classes Q d on W d satisfying the Euler **data** identity, and with the property that A d ∼ α −2 . Note that the restrictions Q d | p to the T fixed points p in X 0 ⊂ W d are polynomial functions on the Lie algebra of G. Suppose that X is a balloon manifold. Then it can be shown that (with a nondegeneracy assumption on e G (X 0 /W d )) the classes Q d are uniquely determined by the values of the Q d | p , when α is some scalar multiple of a weight on the tangent space T p X. These values of Q d | p can be computed explicitly by exploiting the structure of a balloon manifold."
14912634,10/23,1.0,0.435,"To discern the appropriate U for power curves, 10 min power **data** P norm are plotted as a function of (a) nacelle cup anemometer 80 m U, (b) nacelle cup anemometer 'true-flux' equivalent U, (c) SODAR 80 m U, and (d) SODAR 'true-flux' equivalent U for a typical summer day in figure 3. The uncertainty induced by a non-co-located SODAR wind speed in the power curves can be seen in figures 3(c) and (d). When compared with the manufacturer's power curve, the SODAR-based power curves have lower Pearson's coefficient (r) values (r = 0.88-0.89) than the nacelle-based (r = 0.94-0.95). Furthermore, a small improvement (in terms of a higher r value and lower standard deviation of residuals) is evident from using the nacelle-adjusted 'true-flux' equivalent wind speed instead of the nacelle hub-height U ( figure 3(b)). Though small, these differences suggest that the nacelle-adjusted 'true-flux' equivalent wind speed generates the most accurate power curves at this site."
207778550,27/48,1.0,0.562,"ORSphere Caliban a a discarded, see text especially since all criticality measurements involve using almost exactly the same technique? If so, this would cause one to doubt that integral **data** are always the most accurate. This is a valid question that merits further exploration. In this section, we examine this possibility of USU existing in integral benchmark **data** by exploring a relatively simple situation. It is well understood that it is difficult to identify sources of discrepancies and estimate uncertainties in ∆k eff ≡ C−E comparisons for complicated integral benchmarks that incorporate many types of materials and reaction processes. Therefore, we have selected eighteen bare, HEU criticality benchmarks from the ICSBEP catalog that were found to be suitable for this exercise, owing to their relative simplicity and very limited number of materials involved. Being ICSBEP benchmarks for critical assemblies, the experimental values are usually given as a k eff close to unity with an uncertainty u. The given benchmark uncertainty includes both experimental uncertainty as well approximations of the computational model of the benchmark. The selected list, using identification codes from the ICSBEP catalog, is given in Table IX, and ∆k eff comparison results are shown in Fig. 9."
119247546,37/46,1.0,0.804,"The analysis of solutions of equation (49) with for r < r * follows closely the discussion of [13]. It is given here for the sake of completeness. The key observation is that the solution to (49) admits the representation
ω(τ ) = (τ ) ω * + 1 − ς(τ ) (τ ) ζ ,(55)
where (τ ) and ς(τ ) are the solutions to the auxiliary problems − (β 2 +χ) = 0, (0) = 1, (0) = 0,
ς − (β 2 +χ)ς = −1, ς(0) = 1, ς (0) = 0.
That ω as given by (55) is indeed a solution of (49) with the right initial **data** can be verified by directed evaluation. As r * > r , it follows from the observation after equation (50) that β 2 +χ > 0 along the conformal curves under consideration."
229722484,27/39,1.0,0.692,"Figure 10 .
10Transferring of **data** combining Bluetooth and WiFi.
"
246863515,28/44,1.0,0.636,"3. The external validity (S 3 (a) , S 2 (a) , U ) ⊥ G | S 1 , X and overlap
p (S 1 , U, X | A = a, G = E) p (S 1 , U, X | A = a, G = O) < ∞, almost surely. 4. The sequential structure (Y (a), S 3 (a)) ⊥ S 1 | S 2 (a), U, X, G = O.
Note that Assumption 14 consider the most general setting: we allow the treatment assignments in the observational and experimental **data** to depend on pre-treatment outcomes S 1 , and also allow the distribution of S 1 to be different on the two datasets. Now we extend our identification strategy to this setting."
52275756,9/51,1.0,0.176,"The stepwise linear regression (SLR) model is often used to assess the linear relationship between multiple independent variables and it can be used for variable screening and avoided collinearity for soil nutrient prediction [16]. Stepwise regression can also be used for variable screening. SLR can remove the weakly significant variables while retaining those with high contribution rate. In this study, the SLR model was used to identify the optimal combination of input PCs with targets. The linear fitting relationships between hyperspectral auxiliary variables and soil nutrients were also extracted. The SLR model was performed in MATLAB 2013b software. The formula of SLR model (Equation (1)) is defined as follows [32]:
y = b + a 1 x 1 + · · · + a k x k ,(1)
where y is the estimation value of SLR for soil nutrients, b is a regression constant, a 1 , . . . , a k are regression coefficients and x 1 , . . . , x k are the input PCs converted from hyperspectral variables. The support vector machine (SVM) model is a supervised learning method that was used to solve the regression problem in this study. The SVM method can identify the separating optimal hyperplane in multi-dimensional spatial **data** and seeks to minimize the error of all training samples. The SVM model overcomes the limitation of neural networks, which tend to rely on local optimal solutions [33]. Therefore, the SVM model is well suited to soil nutrient prediction with multi-dimensional variables."
119253123,5/32,2.0,0.156,"Corrected **data** are compared to three Monte Carlo models: Pythia 6.4 (tune Perugia-0), Pythia 8.1 (tune 1 [15]) and Phojet 1.12."
54041858,6/45,5.0,0.133,"To understand the intensity and progression of peak and valley events, an energy analysis was performed. For each event the average energy was determined by taking the average of the energies of the two outer sensors. Figure 4 shows the AE event energy versus time for the entire test that were added in sequence to produce Figure 3. Each **data** point represents a single AE event from a discrete source. Significant AE activity occurs at the beginning of the test, primarily in the first few cycles. For most of the test, little AE occurs until the very end of the test. The energy ranges from 0.1 to 50 V 2 /µs until near the end of the test where energies less than 0.01 V 2 /µs were recorded. Since the valley events were only observed at the end of the test (201,990 s), the peak and valley events were plotted separately after 200,000 s in Figure 4b,c, respectively. There were 2156 valley events compared to 1367 events over this period. Valley events were also much more prevalent prior to about 206,000 s. It is also obvious that the valley events tended to be of higher energy than the peak events though both peak and valley showed increasing energy events as time progressed. The peak events contained low and high energies (Figure 4b), including nearly all the very low energy events at the end of the test."
233705419,17/53,2.0,0.321,"The overview of the model shown in Figure 6 illustrates the **data** set used, the target, and influencing parameters, as well as the quality of the model."
119247546,19/46,1.0,0.413,"In order to write the conformal curve equations (24a)-(24b) for the Reissner-Nordström metric, it is noticed that the metric l in the warped product line element (23) is given by
l = D(r)dt ⊗ dt − D −1 (r)dr ⊗ dr. Equations (24a)-(24b) implyt + ∂rD(r) D(r)r t = 1 D(r) βr ,(32a)r − ∂rD(r) 2D(r)r 2 + D(r)∂rD(r) 2t 2 = D(r)βt ,(32b)
where consistent with the notation of section 3.2 we have setr ≡ r(τ ),t ≡ t(τ ). Initial **data** for these equations is prescribed by observing the discussion of Section 4.1, and by requiringẋ to be given initially by the unit normal toS. It follows that
t * = 0, r * > r + ,t * = 1 √ D * ,r * = 0, (b t ) * = 0, (b r ) * = − 2 r * ,(33)whereb t ≡ b , ∂ t ,b r ≡ b , ∂ r .
Notice that r * =r * , t * =t * . As a consequence of the symmetry of the hypersurfaceS with respect to the bifurcation sphere at r * = r + , it is only necessary to consider the case r * > r + . The equations (32a)-(32b) can be decoupled by making use of thẽ g-normalisation condition
D(r)t 2 − 1 D(r)r 2 = 1.(34)
Solving the latter for t ≥ 0 and substituting into (32b), one obtains that
r + 1 2 ∂rD(r) − β D(r) +r 2 = 0.(35)
This equation can be integrated once to yield
D(r) +r 2 − βr = γ,
where γ is a constant given in terms of the initial **data** by
γ = − D * . It follows thatr = ± (γ + βr) 2 − D(r),(36)
with the sign depending on the value of r * ."
119583676,10/16,1.0,0.625,"Theorem 1 . 2 .
12In dimension 2, given any initial **data** in H T n , the Calabi flow exists for all time in H T n and the curvature is uniformly bounded along the flow.
"
207778550,20/48,4.0,0.417,"Note that the selected fifteen experimental values do not include any outliers and their evaluation generated a χ 2 /df that is very close to one. Dividing the **data** points into three groups, according to the measurement technique, gives statistically coherent results in spite of the fact that large differences between mean values derived using different methods were observed [81,82]."
19033986,3/28,1.0,0.107,"Electrons generated by DM lose essentially all their energy via Inverse Compton scattering, e ± γ → e ± γ , on ambient light with average energy E γ ∼ eV. Such scatterings give rise to photons with larger energy E γ ∼ E γ (E e /m e ) 2 ∼ 10 GeV, which is in the energy range being probed by FERMI. As discussed below, this DM ICS γ flux is only marginally affected by astrophysical and DM distribution uncertainties. The reasons for this can be traced back to two observations: (i) Far away from the Galactic Center, the DM uncertainties are relatively mild. (ii) As we will see in Section 4, all DM models that fit the **data** predict roughly the same e ± spectrum, as it is now mostly fixed by the new measurements (given the new FERMI and HESS results). Thereby the DM ICS spectrum is well predicted. As already illustrated in fig. 1 it is not much below the first FERMI diffuse γ-ray data, released for energies ≤ 10 GeV in a specific angular region. Therefore, if the e ± excess is due to DM, FERMI is expected to observe an associated γ excess which is not sensitive to the specific DM model or DM density profile. Whether such an excess is seen or not, will decisively implicate on the DM (or any other mechanisms that produces e ± in a spherical region away from the galactic plane) interpretation of the measured excesses. Alternative scenarios involve e ± generated locally (e.g. by a powerful pulsar) or along the galactic plane (e.g. by supernovae)."
117119864,40/83,4.0,0.482,"Another important point concerns the bulk sensitivity of the CeRu2Si2 ARPES **data** of Ref. [18]. These spectra were not measured at the high photon energies that enhance bulk sensitivity and which are generally necessary [23] for bulk sensitive Ce 4f studies. However it was found [18,78] that some cleaved surfaces yield 4f spectra at low photon energies, e.g. in RESPES at the Ce 4d edge, that are essentially the same as those at high photon energy, e.g. RESPES at the Ce 3d edge. Such surfaces were used to obtain the **data** of Ref. [18]. The explanation [83] probably lies in the particular crystal structure of CeRu2Si2, which admits of two cleavages, one that exposes Ce at the top layer and another that places Ce in a buried layer. The latter situation is expected to enable bulk sensitive studies even at lower photon energies."
126349038,1/28,4.0,0.036,"Flow-field velocities were measured using a towed underwater TPIV system at 10 y-z planes at x¼ 0.06, 0.1, 0.12, 0.2, 0.3, 0.4, 0.6, 0.8, 0.935, and 1.0, as shown in Figs. 1(a) and 1(b). The TPIV resolution was 1 mm in all directions (D x ¼ 0.00033L) and the **data** rate was 13 Hz. The **data** were postprocessed to obtain averaged velocity field (U,V,W), TKE and axial vorticity x x . The averaged flow field **data** were obtained by ensemble averaging over ten repeat runs of 20 s. For b ¼ 0 deg, small static drift asymmetry b asym adjustments were made for improved flow field center plane symmetry, i.e., b asym ¼ 1 deg for x ¼ 0.06-0.2, b asym ¼ 0.5 deg for x ¼ 0.3 and 0.4, and b asym ¼ 0 deg for x ¼ 0.6-1. 2 The Nomenclature provides the definitions of solution and N-version V&V. The references provide the detailed derivations and discussions. Tomographic particle image velocimetry 3D volumes were analyzed along the primary vortices at all 10 y-z planes, namely SDV and FBKV for b ¼ 0, and sonar dome (SDTV) and bilge keel (BKTV) tip vortices for b ¼ 10 deg and 20 deg. Bhushan et al. [15] DES and URANS vortex structure predictions were used to determine the regions of interest for the TPIV measurements. The **data** were post processed to obtain the vorticity components (x x, x y, x z ), second and third invariants of the deformation-rate tensor Q and k 2 in addition to the mean and turbulent flow variables obtained for the planar data. Unsteady analysis was performed for b ¼ 20 deg forces and moment, and velocity and Q at six points along primary vortices using fast Fourier transform (FFT) of their time histories."
249258070,91/119,2.0,0.765,"Collectively, there were ten categories of police reactive and proactive activities coded from the data, and these categories make up the dependent variables for the current study. Police reactivity to CFS were coded into seven different categories using an approach consistent with previous research utilizing CFS **data** (see Wu & Lum, 2017). Violent crimes included CFS involving reported shootings, robbery, and several types of assaults. Property offenses was a category comprised of burglary, theft, and forgery type offenses. Disorder incidents consisted of an array of disturbances, including general, family, and noise CFS. Suspicious incidents included calls where an alarm was sounded (e.g., vehicle alarm set off) and reports involving a suspicious person or vehicle. The trafficrelated activities category comprised CFS for traffic accidents, driving while intoxicated (DWI), and other traffic-related issues (i.e., road rage). Service-related activities included calls to assist first-responders, including other officers, fire fighters, or emergency medical services. Lastly, the non-crime events category consisted primarily of assisting specialized units with transporting individuals and responding to silent 911 calls."
202178736,6/16,1.0,0.375,"The rigid rough square footing of size 100 X 100 mm was placed at the middle of the surface of the unsaturated sample. The footing was fixed at the chosen angle with the loading piston. The displacement transducer was connected with the piston for the measurement of the displacement [R-Displacement {Fig. 5(a)}] during loading. Two external, vertical and horizontal displacement transducer were also placed precisely at the middle of the footing. These transducers were placed perfectly in vertical and horizontal plane at the center of the footing as shown in Fig. 5(a). The footing settlements were measured with a resolution equal to 0.001 mm and the accuracy of the applied load [R-Force] was equal to 0.05 %. A National Instrument (NI) **data** logging system was used to collect all the experimental data. The noisy signal received from load and displacement transducers of the device was filtered using third order Zero-Phase Low Pass Butterworth filter in MATLAB. To obtain a smooth signal after filtering, the unwanted transients were eliminated by using Median Filtering technique available in MATLAB. The loading piston was connected with the footing at an angle of 15 degrees with vertical Fig. 5(a). This provides an angular loading to the footing of 15 degrees to impose a coupled cyclic excitation. A force controlled coupled harmonic excitation from Equation 1 is imposed to the footing at a constant amplitude of 0.25kN, with the frequency of 0.1Hz with respective static load as shown in Fig. 5(b).
[ ] ( ) t A F Force R st cyclic ω cos * + = −(1)
Where, Fst = Static Load A = Amplitude = 0.25kN ω = Angular Frequency = 2*π*f f = frequency = 0.1Hz t = Total time of loading = 0 to 5 x 10 5 sec for 50000 cycles."
207778550,34/48,1.0,0.708,"Maximum likelihood estimation is an established method in statistics that is used to infer unknown parameters of a probability distribution from data. Assume a probability distribution L(D | θ) that gives the likelihood for the realization of any potential measurement D under a specific choice of values for the distribution parameters θ. In maximum likelihood estimation, the values of θ are chosen to maximize the likelihood for the actually observed **data** D obs . This estimation technique is general and not bound to a specific choice of probability distribution."
53380409,111/119,1.0,0.933,"A combination of results from searches for heavy resonance production in various bosonic and leptonic final states is presented. The **data** were collected with the ATLAS detector at the LHC in pp collisions at ffiffi ffi s p ¼ 13 TeV and correspond to an integrated luminosity of 36.1 fb −1 . While previous combination efforts included only the decays of heavy resonances into VV and VH, the combination presented here also includes decays into lepton-antilepton final states. Compared to the individual analyses, the combined results strengthen the constraints on physics beyond the Standard Model and allow the constraints to be expressed in terms of the couplings to quarks, leptons, or bosons. The relative sensitivities of the different approaches are compared, including bosonic and leptonic final states or different production mechanisms such as quark-antiquark annihilation/gluon-gluon fusion vs vectorboson fusion."
17837291,30/52,1.0,0.577,"The linear sigma model is
W d = N d (P(n)) = N d 1 ,1 × N d 2 ,2 × N d 3 ,2 . The equivariant
Euler class, after taking nonequivariant limit with respect to the T action, is given by Once we have the hypergeometric series, the corresponding Picard-Fuchs equation can be easily written down as given in [27]. Here we denote by O i (l) the pullback of O(l) from the ith factor of X = P 1 × P 1 . Our bundle V has rk V + − rk V − = n − 3 = −1. Thus we can apply our formula (8.4) with
e G (X 0 /W d ) = d 1 m=1 (H 1 − mα) 2 d 2 m=1 (H 2 − mα) 3 d 3 m=1 (H 3 − mα) 3 .
x = 0. We put Ω = 1 H 1 H 2 . The Euler **data** P in eqn. (8.4) that compute the K d is now given by:
P d = 2d 1 −1 k=1 (−2κ 1 + kα) × 2d 2 −1 k=1 (−2κ 2 + kα).
The corresponding equivariant Euler class, after taking the nonequivariant limit with respext to the T -action is
e G (X 0 /W d ) = d 1 m=1 (H 1 − mα) 2 d 2 m=1 (H 2 − mα) 2 .
Again one can immediately write down the hypergeometric series as well as the corresponding Picard-Fuchs equation by using our mirror principle."
118072666,4/13,2.0,0.308,"We next subtract the combinatorial background to extract the number of φ mesons. This background is estimated by fitting the mass distribution in sideband regions well away from both the signal and the KK threshold. The ranges 0.989 < m KK < 1.002 GeV/c 2 and 1.04 < m KK < 1.1 GeV/c 2 were chosen and the function
(m KK − 2m K ) a · (b + c · m KK + d · m 2
KK + e · m 3 KK ) was used. This function provides a good description of the phase space in the vicinity of a threshold; a fit to the combinatoric m KK spectrum (removing true φ mesons) in Monte Carlo gives χ 2 = 104 for 95 degrees of freedom. Figure 1 shows the m KK distributions of the onresonance and luminosity-scaled off-resonance **data** samples for all φ momentum bins combined; the fitted combinatorial background shapes are overlaid. The on-(off-) resonance fit has χ 2 = 86 (75) for 61 degrees of freedom. With estimates of the combinatorial background shape, the signal is extracted by subtracting the background. The resulting signals are shown in the lower part of Figure 1. In the on-resonance sample, we observe (2.349±0.007)×10 5 φ candidates, and the corresponding number in the off-resonance sample is (1.95 ± 0.02) × 10 4 . These plots and numbers are representative only; they are not used in the signal extraction."
54041858,8/45,2.0,0.178,"From the determination of first peak times of arrival, the location was determined from:
Location = (x/2)(∆t/∆t x )(1)
where x is the length between the outer sensors (25 mm). The location of each event after 130,000 s is plotted in Figure 7a versus the time of the test where the value 0 is the center of the gage section. Each **data** point represents a single event and the width of the **data** point is proportioned to the average AE energy of the given event. There are scattered and infrequent peak events for the over 70,000 s (~20 h) leading up to the heightened period of AE activity just prior to failure. The events at the end of the test are so dense that the period after 200,000 s (box in Figure 7a) are plotted in Figure 7b,c for the valley and peak events, respectively. Most of the peak and all of the valley events are concentrated in the 0 to +1.5 mm location from the center of the gage, which is where failure took place. Whatever occurred just prior to failure would not correspond to distributed matrix cracking along the gage length. Therefore, one would expect to observe localized damage near the fracture surface within an approximate 2 mm length or less of the composite, as shown below."
251913584,8/31,1.0,0.258,"The main limitation of logistic regression is that it can only be used to predict discrete functions. Thus, the dependent variable of Logistic Regression is restricted to the discrete set. This restriction is prohibitive to predicting continuous **data** (Al Shamali, 2015)."
53380409,62/119,1.0,0.521,"The search channels included here provide access to several coupling strengths of heavy resonances to SM particles as described by Eq. (1) in the context of the HVT model. Specifically, the **data** constrain the coupling strength to both the quarks and bosons in the VV and VH channels, whereas constraints are placed on both the quark and lepton couplings in the leptonic channels. These constraints are shown in Figs. 9-11, where the first and second include a shaded area denoting a region where the limits are not valid because resonances would have a width greater than 5% of their mass. This is a region where the resonance width would exceed the discriminating variable's resolution in the search, and the assumed narrow-width approximation breaks down. Figures 10 and 11 include constraints on heavy resonances with masses of 3, 4, and 5 TeV from precision electroweak (EW) measurements [66], which already exclude this aforementioned region for the relevant contours shown. The EW constraints are only overlaid on the final plots for each part of the combination."
19033986,22/28,1.0,0.786,"Fig.
Fig. 5 shows the χ 2 as function of the DM mass for various DM annihilation (left) or DM decay (right) modes. We find that, independently of the non-observation of an excess in thē p PAMELA data, only some leptonic modes can reproduce all data. Here HESS observations play a key role demanding that the e ± excess terminates in a sharper way than what typical of non-leptonic channels, irrespectively of the DM density profile. DM heavier than 10 TeV that annihilates or decays into light quarks still provides a reasonable fit to PAMELA and FERMI data, if fitted conservatively. However it is disfavored by the HESS e + + e − data, and presumably the photon **data** as well ( in the annihilating case). The only spectral feature that can allow one to discriminate the various modes lies at the high end of the spectrum between 1 and 3 TeV, where we only have the electron HESS **data** which is less precise than the FERMI data. The leptonic channels can be ordered according to the sharpness of their end-point:
"
2423505,11/23,1.0,0.478,"Bayesian Priors
x 2 ) = 1 ni T t =1 Pr(i | xt )x 2 t .(10)
This is the same as the expectation step in the EM algorithm. Finally, these new sufficient statistics from the training **data** are used to update the old UBM sufficient statistics for mixture i to create the adapted parameters for mixture i (Fig. 3b) with the equations:
wi = α w i ni /T + (1 − α w i )wi γ (11) µi = α m i Ei(x) + (1 − α m i )µi (12) σ 2 i = α v i Ei(x 2 ) + (1 − α v i )(σ 2 i + µ 2 i ) −μ 2 i .(13)
The adaptation coefficients controlling the balance between old and new estimates are {α w i , α m i , α v i } for the weights, means and variances, respectively. The scale factor, γ , is computed over all adapted mixture weights to ensure they sum to unity. Note that the sufficient statistics, not the derived parameters, such as the variance, are being adapted."
119313773,3/21,1.0,0.143,"The detectability of breakdown of linear response is linked to the amount of available data. As N → ∞, a breakdown will always become detectable at any specified significance level α. Conversely, if the mismatch Eq between the true response of the dynamical system and the linear response is too small and there is an insufficient amount of **data** available, the actual response will be swamped by the sampling noise, and one will not be able to detect the breakdown of linear response with a reasonable significance level."
119479526,16/29,1.0,0.552,"In this Section, we demonstrate the approach described in Sec. 2.4 to rescale the errors of datasets and accommodate systematic errors. We use three synthetic **data** sets of 10 points each. We generate Set A via Eq. (14) and assign error bars equal to the standard deviation of the ε standard error parameter (equal to 0.05). In this example we will assume that the errors for Set A are accurate. The points in Set B follow the same trend, but with a 0.2 standard error (4 times larger than for Set A). The reported errors in Set B underestimate the standard error by a factor of 4. We generate Set C with an intercept 0.5 less than the true value and a 0.05 standard error (also reported as 0.05). We then perform the analysis with and without the use of hyperparameters. In the method without hyperparameters, we directly use the reported standard errors in the Likelihood definition (we do not need to employ a parameter for the **data** variance as in Eq."
24865762,1/34,4.0,0.029,"In Sect. VII, we introduce the class of MSMs tailored on stochastic Stuart-Landau oscillators, aimed at the modeling of the aforementioned reduction coordinates corresponding here to the projection of the dataset onto the DAH modes; see Sectns. VII A and VII B. These elemental models-the multilayer Stuart-Landau models (MSLMs)-are stacked per frequency, only coupled at different frequencies by the same noise realization; see Sect. VII C. In Sect. VIII, we finally show the flexibility of the DAH-MSLM modeling approach, by its ability to provide skilled stochastic inverse models for **data** issued either from the nonlinear chaotic Lorenz 96 model, or a stochastic heat equation driven by a space-time white noise. Concluding remarks discuss then in Sect. IX directions for future research."
253708011,1/6,2.0,0.167,"In phenomenological estimates (see Refs. [23,24] and references therein), the enhancement of the intrinsic nuclear Schiff moment, Ŝ , is predicted to scale with the quadrupole β 2 and the square of the octupole β 3 deformations, as well as inversely with the energy difference ∆E P D between the parity doublet Table 1 Data for selected odd-mass nuclei in the A ≈ 224 mass region exhibiting parity-doublet candidates. The table presents the isotope, its halflife T 1/2,gs , spin and parity of the ground state J π 0 , energy difference to the lowest-lying possible parity-doublet partner ∆E P D , and the lifetime T 1/2,ul of the upper level. Data are taken from the NNDC **data** base [27] and Ref. [28]."
207778550,31/48,2.0,0.646,"Early neutron capture activation measurements by Stavisskii and Tolstikov [111] and Tolstikovet al. [112] with sealed 232 Th oxide samples are shown in Fig. 12. As later measurements were made of this cross section with metal samples, it became clear that cross sections derived from these earlier **data** were significantly larger than those obtained with metal samples. Not all measured experimental **data** sets are shown in Fig. 12. Although the authors tried to avoid water absorption in their samples, it was found later that the increase of the capture cross section could be explained qualitatively by the presence of water in those samples. Initially this difference might have been considered to be due to an USU effect. However, in this case, the physical origin of the discrepancy became known but no correction for the neutron multiple scattering due to hydrogen and subsequent capture in 232 Th, could be made. Therefore, a large uncertainty (not an USU component) should be assigned to these **data** if they are to be considered for evaluation purposes. An equivalent (and perhaps best) alternative is to discard these uncorrected **data** and not use them for **data** evaluation."
216162633,6/6,3.0,1.0,"In [15], existence and uniqueness is proved in the phase space H = (H 1 0 (Ω) ∩ H 2 (Ω)) × H(Ω) for a set of functions g and φ, satisfying several conditions. In particular, g satisfies (H1), that is, (g(u), u) − θG(u) ≥ 0, u ∈ H 1 0 (Ω) ∩ H 2 (Ω), θ > 2, where G is the potential of g, and a typical form of φ is φ( ∇u 2 2 ) = c 1 ∇u 2 2 + c 2 ∇u 2q 2 with c j > 0, j = 1, 2 and q ≥ 1. In this case, if c 1 = c 2 = 1 for simplicity, f (u) = g(u) + ∇u 2 2q ∆u, has the potential F (u) = G(u) − 1 2(q + 1) ∇u 2q 2 , and satisfies (H1) with θ ≥ r ≥ 2(q + 1) ≥ 4, that is, the nonlinearity of the source term g is stronger than the one of φ. In contrast, for a Timoshenko equation where these two nonlinearities appear, if θ < 2(q + 1) holds then all the solutions are global and uniformly bounded in the phase space H defined above. Furthermore, all the solutions are attracted by the set of equilibria as times goes to infinity, see [13]. The blow up result showed in [15] holds if the initial **data** are such that
0 < E 0 < 1 2 u 0 2 2 (u 0 , u 1 ) 2 − 2δ r − 2 u 0 2 2 2 , (u 0 , u 1 ) 2 > 0.
Note that this upper bound of E 0 is smaller than
1 2 |(u 0 , u 1 ) 2 | 2 u 0 2 2 + 2 δ r − 2 2 u 0 2 2 ,
which is of the type (17). And also, it is smaller than β δ in Theorem 3.1.
"
252111050,9/21,1.0,0.429,"Effect of Turbo boost: It can be noticed that in the exclusive setting, more measurements were made than in the shared setting. The scatter plots of algorithms in Figure 6b and 6c show multi-modal distribution of measurements (especially bimodal with two clusters of **data** points at the two ends of the distribution). This is because the processor operated at multiple frequency levels due to turbo boost settings, thereby resulting in significantly different execution times for the same algorithm. As the measurements of algorithms were sufficiently shuffled, the probability that a particular algorithm executes in just one frequency mode-thereby resulting in a biased comparison-is minimized. However, for the quantile ranges we considered (from Table III), the algorithms alg4, alg3, alg0, alg1, alg2 obtain the same mean rank scores (see exclusive mode in Figure 5b) even though the relative shifts among their distributions can be visually observed in Figure 6c. In order to compare algorithms based on the measurements taken during the fast frequency modes of the processor (i.e., measurements towards the left end of the distribution), we modify the quantiles set in Procedure 4 and consider the following ranges: [(q 5 , q 50 ), (q 15 , q 45 ), (q 20 , q 40 ), (q 25 , q 35 )] and recalculate the mean ranks. The results are shown in Figure 7a. Now, alg5 obtains the best rank. The relative shifts among the algorithms based on the left-part of the distributions are now quantified by the mean ranks."
17228594,3/7,2.0,0.429,"At short distances we compare with the perturbative potential obtained by Schröder [17] as c(r) does not contain the string tension, and has a universal value in the L.O. It is therefore more sensitive to the sub-leading behaviour of the flux tube. In Fig. 2 we plot c(r) in units of r/r 0 . We look at a wide range of r starting from where the **data** almost touches the perturbative curves going all the way to the region where the string predictions hold. The **data** almost lie on top of each other exhibiting nice scaling behaviour as one goes to larger values of r. The β = 12.5 and β = 10 **data** come together already in the range 0.5 and 1.25 r 0 . The β = 7.5 set joins onto this at around 1.5r 0 and even the β = 5 **data** joins up at around 2.25r 0 . This points to the possibility that the continuum limit of the scale where the flux tube is well described by the Arvis curve can be obtained even on relatively coarse lattices.
V pert (r) = s pert r + g 2 C F 2π ln g 2 r + . . . with s pert = 7g 4 C F C A 64π . For SU (2) C F = 3/4, C A = 2. The perturba- tive force and c pert (r) can be computed by f pert (r) = dV pert (r) dr and c pert (r) = r 3 2 d 2 V pert (r) dr 2 . In
We estimate a temporal extent correction factor of about 0.1% for c(r) for our largest r values. This is about an order of magnitude lower than our statistical errors at such r values. Corrections due to finite spatial extents are of similar magnitude."
14912634,3/23,2.0,0.13,"The meteorological tower was equipped with cup anemometers at 50, 60 and 80 m AGL to measure wind speed at a sampling rate of 1 Hz and accuracy of 0.3 m s −1 . High resolution vertical profiles of wind speed, direction, and three-dimensional turbulence were available from a three beam, 4500 Hz Doppler mini SODAR (Model4000, Atmospheric Systems Corporation, Santa Clarita, CA, USA). The SODAR measured 3-axis wind speeds (u, v and w), with a sampling rate of 1 Hz per beam and a vertical resolution of 10 m, from 20 to 200 m AGL. The **data** were quality controlled according to accepted SODAR standards (e.g. Antoniou et al 2003) (see Wharton and Lundquist 2012)."
14912634,3/23,1.0,0.13,"Power **data** from July 2007 to June 2008 were collected at a multi-MW wind farm in western North America at an elevation near sea level. Strong land-ocean temperature differences, particularly during the summer, drive strong local southwesterly winds. The landscape is grass-covered rolling hills, with subtle elevation changes. In addition to turbine power data, meteorological **data** from an 80 m tall tower, SODAR and turbine-mounted cup anemometers were also used. A map of the wind farm and instrument locations is found in (Wharton and Lundquist 2012)."
6811986,9/20,4.0,0.45,"Theorem 7. There are a Boolean UCQ Q and a set C of IDs over a schema S for which the problem PQI(Q, C, S, V) is ExpTime-hard in **data** complexity."
207778550,27/48,5.0,0.562,"As discussed above, the observed biases in ∆k eff of spherical metallic fast benchmarks cannot be removed by any adjustments of the evaluated nuclear **data** used in the analyses. This conclusion also does not depend on the particular method or code used for the calculations of k eff (Monte Carlo, deterministic, etc.), provided that the same code is used for all the benchmark calculations. Therefore, additional factors that influence the ∆k eff outcomes need to be considered."
6811986,9/20,1.0,0.45,"Proof. We just note that PQI(Q, C, S, V) translates to unsatisfiability of the following formula:
φ PQItoGNF Q,C,S,V = ¬Q ∧ C ∧ ⋀ R∈Sv ⋀ R(ā)∈V R(ā) ∧ ∀x R(x) → ⋁ R(ā)∈V x =ā
If the constraints are in GNFO, then the formula above is also in GNFO. The finite controllability of PQI(Q, C, S, V) comes from the finite controllability of GNFO formulas (Theorem 1) Above we are using results on satisfiability of GNFO as a ""black-box"". Satisfiability tests for GNFO work by translating a satisfiability problem for a formula into a tree automaton which must be tested for non-emptiness. By a finer analysis of this translation of GNFO formulas to automata, we can see that the **data** complexity of the problem is only singly-exponential."
119479526,7/29,2.0,0.241,"Firstly, we generate 100 synthetic **data** points each for specific heat and enthalpy. The specific heats and enthalpies are equally spaced in the 1K − 75K and 300K − 1800K temperature ranges, respectively."
251711372,21/119,1.0,0.176,"The remainder of this paper is organized as follows. Section 2 reviews the relevant literature. Section 3 provides the testable research hypotheses. Section 4 presents the **data** and the event study methodology. In Section 5, we present and discuss our main findings. Section 6 concludes the paper."
17837291,1/52,3.0,0.019,"Once these values are known, it is often easy to manufacture explicit G-equivariant classesQ d with the restrictionsQ d | p having the above same values, and satisfying the Euler **data** identity. In this case, we say that the dataQ d are linked to the **data** Q d ."
15294885,23/25,1.0,0.92,"
a), the QAM symbols of the Tx have E s /N 0 from 0 to 15 [dB] and have k(f ) = 0.0, 0.2, 0.4, 0.6, 0.8, or 1.0, ∀f . In Fig. 3-(b), the QAM symbols of the Tx have E s /N 0 = 0, 5, 10 or 15 [dB] and have k(f ) from 0 to 1, ∀f . In both cases, as the amount of impropriety increases, the optimal pair of the Tx and Rx more exploits impropriety and cyclostationarity of the desired signal in suppressing the data-like interference and, consequently, the MSE performance monotonically improves. VI. CONCLUSIONS In this paper, we have considered a joint optimization of the Tx and Rx for the transmission of an improper-complex SOS **data** sequence over an additive proper-complex cyclostationary noise channel. An MSE minimization problem is formulated under the average transmit power constraint to find the jointly optimal transmit waveform of a linear modulator and the receive waveforms of a widely linear Rx. This problem is converted into an equivalent problem described in the frequency domain with the help of the VFT technique and solved by introducing the notion of the impropriety frequency function. It is shown that the optimal transmit and receive waveforms well exploit the frequency-domain second-order structure of the improper-complex SOS **data** sequence and the additive proper-complex SOCS noise. APPENDIX A. Proof of Lemma 3 Proof: Define the 2-by-2 matricesM (f ), M (f ), and K(f ), respectively, aŝ M (f ) M(f )M (f ) M (f ) * M(−f ) * , M (f ) M(f )e jφ(f ) 0 0 M(−f )e −jφ(f ) 1 2 , and K(f ) 1 k(f ) k(f ) 1 . (28) Then, we can rewriteM (f ) asM (f ) = M (f )K(f )M (f ) H . Also, define theN (f )-by-N (f ) ma-"
251719245,4/21,2.0,0.19,"We created several graph representations by varying the input parameter . In total, we obtained 5 graph representations of the same **data** set by creating a corresponding graph for = {0.70, 0.75, . . . , 0.95}. We summarized the properties of the graphs in Table 1 Table 1: Summary of network properties for graphs obtained using value-based constrution"
119415579,19/41,2.0,0.463,"In six cases (HD 3980, HD 38823, HD 108662, HD 108945, HD 137909, and HD 223640) the most probable v sin i values derived in Paper I were found to exceed the equatorial velocities (veq) calculated using Prot, and R; however, the v sin i and veq values of all six stars were found to be equal within the estimated uncertainties (i.e. they are consistent with i ≈ 90°). In these cases, we removed those MC **data** points for which v sin i > veq. The peak values of the resulting MC distributions were then used to define new, most probable v sin i values."
207778550,7/48,2.0,0.146,"What harm can be done if evaluations generate small uncertainties? Of course there is no harm, only benefits, as long as these predicted uncertainties are realistic and truly reflect the quality of the underlying **data** being evaluated. Otherwise, if the estimated uncertainties are indeed too small, there is a danger that users of these evaluated **data** will be misled into thinking that the quality of the provided information is better than it really is. This can have serious consequences for analyses of cost, reliability, and safety when these **data** are employed in modeling nuclear systems and/or in making predictions. Of course, if the evaluated uncertainties are too large relative to the uncertainties of the underlying data, this too can have important implications. Then, it may happen that costly and time consuming effort might be devoted to improving the quality of the underlying **data** when it would not be warranted if the evaluation had produced results that represented the **data** properly. Clearly, the goal of proper **data** evaluation is to produce realistic results that represent the available underlying **data** well. As mentioned above, we should be reminded of Aristotle's observation that was expressed long ago [1]."
53380409,53/119,1.0,0.445,"The event selection discussed in Sec. VI relies on the reconstruction of electrons, muons, jets, and missing transverse momentum (with magnitude E miss T ). Although the requirements vary for the different channels, the general algorithms are introduced below. The small differences between the efficiencies measured in **data** and MC simulation are corrected for by applying scale factors to the MC simulation so that it matches the data."
118072666,8/13,5.0,0.615,"We also repeat the analysis using a different **data** set. We use a smaller **data** set from the year 2000 in which the detector was operating under different conditions. This analysis yields B(B → φX) = (3.34 ± 0.07)% where the error is statistical only, entirely consistent with our primary result."
119634392,1/9,4.0,0.111,"We remark that in the first theorem, γ 2 can be arbitrarily large, but s 1 > −1, while in the second theorem γ 2 < 2n/p, but for sufficiently large γ 1 and sufficiently small γ 2 , s 1 > γ 2 − n/p − 1 can be less than −1. Thus the non-standard product estimate allows us to obtain existence results for initial **data** with lower regularity, but requires γ 2 to be small and requires the use of Besov spaces."
246863515,11/44,1.0,0.25,"In this section, we extend our identification results by relaxing Assumptions 2 and 3. In particular, we relax Assumption 3 by allowing the covariate distribution to be different in the experimental and observational data. This is an important extension because these two types of **data** are often collected from different environments, where the covariate distributions are very likely to be different. For example, because observational **data** are usually easier to collect and have larger scale than experimental data, its covariate distribution may be more representative of the entire population of interest; while experimental **data** may only correspond to a selective sub-population. Therefore, we consider the following assumption to allow for different covariate distributions in two types of data. This assumption is also called conditional external validity in Athey et al. [2020]."
119479526,7/29,5.0,0.241,"MultiNest to obtain the combined marginal Likelihood for enthalpy and specific heat. The Likelihood expression for this analysis is identical to Eq. (7), save that the parameters are not shared between the models for enthalpy and specific heat (e.g., θ Cp and θ H are distinct parameters). Secondly, we perform a combined analysis using the Likelihood given in Eq. (7) where the model parameters are shared between the enthalpy and specific heat models. In other words, the analysis only employs a single parameter each for θ, a and b. Note that we select priors for the separate analysis in the same manner as in the combined Figure 1: Specific heat data, predictions and uncertainty intervals for enthalpy (a) using only enthalpy data, and (b) through a combined analysis; and for specific heat (c) using only specific heat data, and (d) through a combined analysis. Note that in panels b (d) the inset figure shows the ratio of the analyses using only enthalpy (specific heat) **data** to the combined analysis as a % difference."
15294885,6/25,1.0,0.24,"Given a pair (B, 1/T ) of a bandwidth and a reference rate, the excess bandwidth β is defined as
β 2BT − 1 and the Nyquist interval F is defined as F f : − 1 2T ≤ f < 1 2T .
Given a pair (B, 1/T ) and a deterministic function p(t) having the continuous-time Fourier transform
(CTFT) P (ξ) ∞ −∞ p(t)e −j2πξt dt, the VFT p(f ) of p(t) is defined as a vector-valued function of f ∈ F that is equivalent to P (ξ). In particular, the kth entry of p(f ) is given by [p(f )] k P f + k−L−1 T for k = 1, 2, · · · , 2L + 1, where L ⌈β/2⌉.
Given a pair (B, 1/T ) and an SOCS random process N(t) with cycle period T having the auto-
correlation function r N (t, s), the matrix-valued PSD R N (f ) of N(t) is defined as a matrix-valued function of f ∈ F , whose (k, l)th entry is given by [R N (f )] k,l R (k−l) N (f +(l − L − 1)/T ) for k, l = 1, 2, · · · , 2L+ 1, where R (k) N (ξ) is the CTFT of r (k)
N (τ ) that is obtained by applying the Fourier series expansion to
r N (t, t − τ ), i.e., r N (t, s) = ∞ k=−∞ r (k) N (t − s)e j2πkt/T .
In using the above definitions, it is assumed that the parameter B is chosen as bandwidth in complex baseband over which the Rx can observe and process a signal and that the parameter 1/T is chosen as the symbol transmission rate of the Tx. It is also assumed that the frequency band over which the Tx can emit non-zero power is identical to the frequency band of the Rx. For a general case where these two frequency bands are different, the notion of virtual legacy Rx's and the orthogonal constraint at the virtual legacy Rx's can be employed as is done in [15] for the transmission of a proper-complex **data** sequence."
67205543,8/18,1.0,0.444,"The dimension parameters (L, W, A, P and F) of the wheat grains were given as inputs and their respective classification results of IPT were given as output to the ANN-ABC model. The **data** set of 170 wheat grains representing the overall problem space is used to train the ANN-ABC model and the remainders 30 wheat grains that are not included in the training process are utilized to test the accuracy of the model. ANN-ABC model based on MLP having one input layer with five neurons, one hidden layer with five neurons and one output layer with one neuron was constructed, as shown in (Figure.6). ""Log-sigmoid"" function is used for input and output layers while ""tangent sigmoid"" function is utilized for the hidden layer. The parameters of the ANN-ABC model used in this work are listed in "
14912634,11/23,1.0,0.478,"To examine stability-related effects on turbine power performance, 10 min power generation **data** were segregated into stability classes, based on the wind shear exponent (section 3.3.1), turbulence intensity (section 3.3.2), or turbulence kinetic energy (section 3.3.3) per the categories defined in table 1. Normalized power P norm is plotted as a function of binned nacelle 'true-flux' equivalent wind speed with separate curves for each stability class. Data points are missing in the power curves when there were too few  The nacelle and power **data** are from Turbine 1. Power curve accuracy is based on the 'best fit' metrics (Pearson r value and standard deviation of residuals) between the observations and the manufacturer expected power curve. The plots show that U equivTI nacelle produces the most accurate power curves at this site."
119052582,1/9,1.0,0.111,"In 2016, the Belle collaboration published the cross-section **data** on γ * γ → π 0 π 0 [4], so that it became possible to discuss the GDAs in comparison with actual experimental measurements. We determined the pion GDAs by analyzing the Belle data, and then gravitational form factors were evaluated for the pion from the obtained GDAs [3]. We discuss these results in this report. First, the definitions of the GPDs and GDAs are introduced in Sec. 2.1, gravitational form factors of the pion are explained in Sec. 2.2, and the differential cross section of γ * γ → π 0 π 0 is expressed by the GDAs in Sec. 2.3. Our analysis results are discussed for the GDAs and gravitational form factors in Sec. 3."
216162633,5/6,1.0,0.833,"Remark 1. It is well known that when the potential well method is applied, for E 0 < d, the qualitative behavior is determinated by the sign of I(u 0 ). In particular, the blow-up of a solution is characterized if the initial **data** are such that I(u 0 ) < 0. For any positive value of the initial energy, this is not the case. Indeed, under the assumptions of Theorem 3.1, we get from energy equation, if E 0 < β δ , and since
r−2−δ/ √ c r−δ/ √ c < λ * δ < 1, the following consequences I(u0) = 2E0 − u1 2 W P + 2F (u0) − (f (u0), u0) ≤ 2E0 − |P(u0, u1)| 2 u0 2 W P + 2F (u0) − (f (u0), u0) < 2β δ − |P(u0, u1)| 2 Ψ(u0) + 2F (u0) − (f (u0), u0) = r − 2 − δ/ √ c r − δ/ √ c Φ(u0, u1) λ * δ − |P(u0, u1)| 2 Ψ(u0, v0) + 2F (u0) − (f (u0), u0) = r − 2 − δ/ √ c r − δ/ √ c 1 λ * δ − r r − δ/ √ c Φ(u0, u1) − ((f (u0), u0) − 2F (u0) − cΨ(u0)) < 1 − r r − δ/ √ c Φ(u0, u1) − ((f (u0), u0) − 2F (u0) − cΨ(u0)).
Hence,
I(u 0 ) < − δ/ √ c r − δ/ √ c Φ(u 0 , u 1 ) − ((f (u 0 ), u 0 ) − 2F (u 0 ) − cΨ(u 0 ))(14)
Let us assume that the source term is large enough, that is,
(15) (f (u 0 ), u 0 ) − 2F (u 0 ) ≥ cΨ(u 0 ),
then, by (14),
I(u 0 ) < − δ/ √ c r − δ/ √ c Φ(u 0 , u 1 ) ≤ 0.
From (H1), (15) holds if the source term is such that
F (u 0 ) ≥ 1 r − 2 cΨ(u 0 ).
Then, in this case the inequality I(u 0 ) < 0 is a necessary condition for nonexistence of global solutions. However, it seems that the condition I(u 0 ) < 0, alone, does not imply nonexistence of global solutions for high energies, see [14,28]. Moreover, the sign of I(u 0 ) is not required in the proof of Theorem 3.1."
118072666,4/13,1.0,0.308,"We first remove the continuum background from our signal by subtracting the m KK distribution obtained in the off-resonance sample from that in the on-resonance sample, scaled by the ratio of the luminosities of the two samples. This scale factor is calculated by comparing the number of e + e − → µ + µ − events in the two samples. The center-of-mass momenta of φ candidates in the off-resonance **data** are scaled by the ratio of on/offresonance beam energies to account for the slightly different momentum spectrum of the continuum component in the on-resonance sample. This procedure explicitly accounts for all backgrounds from physics processes other than Υ (4S) production as their cross sections are almost identical at the two energies; it also accounts for beamrelated backgrounds, as the running conditions were very similar."
119125001,1/32,2.0,0.031,"The Cauchy problem for (deterministic) initial **data** in the energy space is well-understood. We summarize the relevant results in the following theorem. Theorem 1.1 (Global well-posedness and scattering [1,29,30,45,47,48,49,50,52]). Let pu 0 , u 1 q P 9 H 1 pR 4 qˆL 2 pR 4 q. Then, there exists a maximal time interval of existence I and a unique solution u : IˆR 4 Ñ R of (1) such that u P C 0 t 9 H 1"
5181686,2/10,1.0,0.2,"The estimates of the logarithms of the probability of the **data** under the models and assumptions regarding independence of allele frequencies are shown in Table 1. Under the admixture model, the smallest probability is associated with a prior K of 1 and little of the posterior probability is associated with higher K values. The distribution of members of the sample to inferred clusters is consistent with this observation. The proportion of individuals assigned to each cluster is approximately the same with little variation between ethnic groups ( Table 2). This symmetry is strongly suggestive of the absence of population structure in the AADM study sample. This is so because real population structure is associated with individuals being strongly assigned to one inferred cluster or another with the proportions assigned to each ethnic group showing asymmetry. The posterior probability under the no-admixture model also favours a K of 1. Examination of the distribution of individuals sampled to inferred clusters also shows the same strong symmetry. These consistent displays of symmetry suggest that a K of 1 is the most parsimonious model. The same conclusion was reached by examining the membership coefficients (Q). Irrespective of the value of K between the range of 2 and 6, Q is similar across the whole sample as illustrated by the bar plots in Figure 2."
129244480,23/39,1.0,0.59,"Previous studies have concluded that CO 2 gases in the Sydney Basin coals are primarily of igneous origin (Smith & Pallaser 1996;Faiz et al. 2003). Carbon isotope results in the range of -7 to -3% are interpreted as representing igneous, magmatic or 'deep external' origins (Smith & Pallaser 1996) and support findings by Baker et al (1995) that continental-scale magmatism resulted in the emplacement of CO 2 and dawsonite formation in the Bowen-Gunnedah-Sydney basin system. While this conclusion is widely accepted, isotope **data** collated in the current study (Figure 15) indicate that the range of d 13 C CO 2 is much wider than the narrow magmatic-source values and that the overall trend of the CO 2 isotope values represents a pattern of d 13 C enrichment with depth across the region. In other words, the d 13 C CO 2 range reported in the Sydney Basin is not likely to be (at least, solely) of magmatic, igneous or volcanic origin (Smith & Pallaser 1996;Faiz et al. 2003;Faiz & Hendry 2006;Thomson et al. 2008). Smith et al. (1992) suggested some other possible CO 2 sources in the region, such as oxidation of coals and 'thermal decomposition of carboxyl groups'; the former process produces isotope values of around -20%, while the latter shows values greater than 0%. Rice (1993) and Boreham et al. (2001) described shifts from more negative to more positive carbon isotope values in CO 2 and interpreted them to be due to microbial activity. Such a shift can also be due to interactions with the country rock or groundwater, which can alter the original CO 2 carbon isotope compositions (e.g. Hoefs 2009). As a result, the interpretation of CO 2 origin from carbon isotope values should be carried out in the context of the hydrogeological and biogeochemical regime (cf. Golding et al. 2013)."
119419652,8/18,1.0,0.444,"A dedicated tt differential cross-section measurement was performed by the ATLAS collaboration in the all-hadronic channel by searching for events compatible with a boosted tt final state [27]. Advantages of the all-hadronic tt channel include a high branching ratio and no neutrinos in the hard-scatter final state, allowing for a full event reconstruction. In contrast, the lack of an isolated high-p T lepton in the final state leads to a formidable source of background from multijet events which must be estimated using a data-driven approach. The analysis targets events featuring two large-radius (R = 1.0) jets and employs so-called jet grooming and top-tagging techniques to increase the relative fraction of signal events. Corrections for detector-level effects, efficiencies and overall acceptance led to a final set of distributions unfolded to particle and parton level within the fiducial volume. Figure 5b shows the resulting normalized differential cross section as a function of   the transverse momentum of the tt system, showing good overall agreement between the measured **data** and simulation. The notable exception is a slight disagreement in the MadGraph aMC@NLO prediction interfaced with Pythia8 for the parton shower and hadronization modelling; in this case the simulation predicts a harder p tt T spectrum compared with what is observed in the data. The overall event yields were seen to be somewhat lower in **data** relative to the NLO predictionsthough this is recognizably not visible in the figure shown -but are nevertheless consistent within the overall uncertainty bands."
184487142,8/47,1.0,0.17,"= maximum allowed 2 perturbation of the input T = number of steps of the attack σ = std. of Gaussian noise **data** augmentation during training and certification mtrain = number of noise samples used to estimate (6) during training mtest = number of noise samples used to estimate (6) during evaluation (♦) Given a smoothed classifier g, we use the same prediction and certification algorithms, PREDICT and CERTIFY, as [6]. Both algorithms sample base classifier predictions under Gaussian noise. PREDICT outputs the majority vote if the vote count passes a binomial hypothesis test, and abstains otherwise. CERTIFY certifies the majority vote is robust if the fraction of such votes is higher by a calculated margin than the fraction of the next most popular votes, and abstains otherwise. For details of these algorithms, we refer the reader to [6]."
216162633,2/6,1.0,0.333,"We consider the following abstract differential equation. For every initial **data** u 0 , u 1 , find t → u(t), t ≥ 0 , such that (P) P u tt + Au + δP u t = f (u), u(0) = u 0 , u t (0) = u 1 ."
119125001,2/32,3.0,0.062,"Since the translates tϕp¨´kqu k form a partition of unity, we have that
f "" ÿ kPZ d P k f ,(3)
which is called the Wiener decomposition of f . The Wiener randomization is obtained by randomizing the coefficients in (3). Let I Ď Z d by an index set such that Z d "" I 9 Ť t0u 9 Ť p´Iq. Let tX k u kPIYt0u be a sequence of symmetric, independent, and uniformly sub-gaussian random variables (see Definition 2.1). We set X´k :"" X k for all k P I, and assume that X 0 is real-valued. Then, the Wiener randomization f W is defined as
f W :"" ÿ kPZ d X k¨Pk f .(4)
The reason for introducing the set I is to preserve the real-valuedness of f . The Wiener randomization f W is a random linear combination of functions with unit-scale frequency uncertainty, and therefore resembles a random Fourier series. We then examine the random **data** Cauchy problem #´B tt u`∆u "" u 3 pt, xq P RˆR 4 u| t""0 "" u 0`f W 0 , B t u| t""0 "" u 1`f W 1 ."
119479526,22/29,1.0,0.759,"Figure 9 :
9(a) Sequences of 400 walkers from the kombine sampler over 150 iterations. Two randomly selected sequences are highlighted in black. All samples to the left of the black dashed line are discarded. (b) The marginal Likelihoods and 2-sigma standard deviations are plotted for both samplers and all models.Finally, we examine the Posterior distribution and uncertainty intervals.Figure 10summarizes the posterior distributions obtained via the kombine (left) and MultiNest (right) samplers. We plot histograms and KDE distributions for each univariate parameter distribution. We provide multivariate summaries for each pair of coefficients via scatter plots of 2000 randomly selected samples. True parameter values are indicated by black markers. We first note that the samplers produce similar posterior parameter distributions. Furthermore, regions of high density in the Posterior closely align with the true parameter values. Figure 11 displays the model prediction and 95 th percentile uncertainty intervals obtained via the kombine (left) and MultiNest (right) samplers versus the synthetic **data** points. The uncertainty intervals are obtained by randomly sampling the Posterior distributions of the model parameters and computing the 2.5 th and 97.5 th percentile levels of the resulting model predictions. The uncertainty intervals show the expected spread of model predictions from the distribution of θ 0 and θ 1 . The models fit the synthetic **data** well, and both the model predictions and uncertainty intervals are nearly indistinguishable between the two samplers. The results of
"
248366486,4/24,2.0,0.167,"At this point and regarding the identification of universal scaling behavior, we must note that our kMC results are unavoidably limited due to the finite size of the systems em- Table V and Table III vs. T for J = 1 and for A = 0.01 (circles), A = 1 (squares), and A = 10 (triangles). Lines are guides to the eye. All units are arbitrary. ployed and the statistics assessed. From a theoretical point of view, crossover effects are known to occur very frequently in kinetic roughening processes [45,46]. In the renormalization group framework [15,36], these are induced by the existence of more than one attracting fixed point (FP), i.e. universality class, for the nonequilibrium system under study. For finite parameter values, system size, and simulation times, renormalization towards the most relevant FP may be incomplete, inducing e.g. effective values for the scaling exponents that are measured, which may not coincide with any of the expected universality classes. In many cases, it is at most these effective exponents which are the ones accessible experimentally [45,46]. From the practical point of view of our present numerical simulations, we are unable to control the (subdominant) scaling corrections of the **data** due to the high correlation (in time) of the observables which we measure, and to the lack of precise theoretical predictions in the various parameter regions that we study. Without this kind of control, stronger numerical arguments regarding universal behavior can hardly be provided. In spite of these limitations, the values we report for the critical exponents show compatibility in a wide range of parameters, from a statistical point of view (differences at most less than two standard deviations). Moreover, the long simulated times gives us some confidence that the contribution of the subdominant terms can be safely neglected.  In this section we will study the height-difference correlation function.
○ ○ ○ ○ ○ □ □ □ □ □ △ △ △ △ △ ○ □ △ ○ ○ ○ ○ ○ □ □ □ □ □ △ △ △ △ △ ○ □ △
"
208006088,9/26,3.0,0.346,"Rainforest tree point process modelling. We consider a subset of the extensive rainforest **data** set credited to Hubbell et al. (2005)  In the survey area, the locations of all Trichilia tuberculata (a tree species of the Mahogany family) have been measured (see Fig. 7 in App. J). We tackle this spatial point pattern with a log-Gaussian Cox process model, which is an inhomogeneous Poisson process model for count data. The unknown intensity function λ(x) is modelled with a Gaussian process such that f (x) = log λ(x). We model locally constant intensity in subregions by discretising the region into np bins (Møller et al., 1998). This leads to having a Poisson observation model in each bin (see App. J). This model reaches posterior consistency in the limit of bin width going to zero (Tokdar and Ghosh, 2007). The accuracy thus improves with tighter binning. We use a separable Matérn-5/2 GP prior over f (x 1 , x 2 ), and discretise the area into a n × p = 200×100 grid with np = 20000 grid bins, and treat the first dimension as time."
17671315,9/32,1.0,0.281,"Also, since the time moment {t = 0} was unknown to us, we set t := 0 to be such a point on the time axis, which is one (1) nanosecond to the left from the beginning of the selected peak. Resulting superimposed pre-processed curves are shown on Fig. 5. It is clear now that the condition t ∈ (0, ∞) in (9) is not a restrictive one, because each curve of Fig. 5 represents a function with a compact support for t ∈ (0, ∞) . It is also clear from comparison of Figures 4-b) and 5 that the pre-processed **data** are far from the range of the operator A (ε r ) := u (0, t) , which should be inverted to solve CIP1. "
17895596,5/11,1.0,0.455,"The analysis of low-multipoles from SKA continuum surveys can benefit from the methods developed for the study of the CMB. Missing sky area is always a problem for low-mode measurements. For CMB studies, many methods were proposed to deal with a mask of missing **data** for both power spectrum estimation and phase recovery. For the power spectrum, one of the most used methods is MASTER (Hivon et al. 2002). It consists in first building a matrix which captures the coupling between the modes induced by the mask, and then inverting this matrix. In Figure 4 we plot the angular power spectrum of SKA galaxies for low-mulitpoles with error bars corresponding to a SKA1 continuum survey. For phase recovery, or more generally for large scale map reconstruction, many methods have been proposed based on Wiener filtering, l 2 or l 1 norm regularization, constraint realizations or diffusion [see Starck et al. (2013) and references therein]. Based on these new methodological idea, Planck **data** were analyzed with a mask removing 27% of the sky (Planck collaboration 2014c; Rassat et al. 2014). For a given observed sky area, the shape of the mask will also be important. The importance of random sampling is also described in Paykari et al. (2013), and many small missing parts, randomly distributed, will always be much better for large scale studies than a compact big missing part."
119080499,19/30,1.0,0.633,"The optical absorptivity spectra for samples of LaH x and NdH x were also measured by Peterman et al. [36] at 4.2K at near-normal incidence. From the absorption data, they deduced the real and imaginary part of the dielectric constant, and hence the optical conductivity by using Kramers-Kronig analysis. They found a relatively broad feature for LaH 2.87 . However, their samples were polycrystals and the use of Kramers-Kronig analysis might enlarge the **data** uncertainties. On the other hand, a more recent and accurate measurement reported by Griessen et al. [39] sheds new light on the optical conductivity curve. The optical transmission spectra of the insulating phase YH 3−δ were measured as functions of the photon frequencyhω and of hydrogen vacancy concentration δ. The effect of the vacancy appears to reduce the overall transmission spectra quite evenly between hω = 0.5 eV and 2 eV. In our theory, the δ-dependent conductivity σ(ω) athω < 2.8 eV mainly determined by the optical transition from the vacancy state to the conduction bounds. The transition energy from the valence bands to the vacancy state is larger because of the larger energy difference between the two states and because of the Coulomb repulsion of the doubly occupied electron states on the same vacancy site. Since the vacancy state is highly localized, the transition matrix largely depends on the density of states of the conduction bands, which is expected to lack pronounced feature."
17671315,20/32,3.0,0.625,"Step 3. Let Y = X be any other target listed in Table 1. When applying GLK to the **data** F Y (t), use the same calibration factor CF (X) . Here CF (wood stake) = 10 −5 , R GLK = 3.8; b) the function R GLK (plastic cylinder) for the case when the plastic cylinder was the calibration target. Here CF (plastic cylinder) = 1.8 · 10 −5 , R GLK (plastic cylinder) = 0.28; c) the function R GLK (bush) for the case when bush was the calibration target. Here CF (bush) = 0.6 · 10 −5 , R GLK (bush) = 6.4."
119479526,26/29,1.0,0.897,"Figure 13 :
13errors on each dataset by the best fit hyperparameter values of 0.9, 0.4 and 0.1 for hyperparameters α A , α B and α C , respectively. Figure 14 displays the histograms and KDE distributions for each parameter and scatter plots for each pair of parameters. Each true parameter value (marked by the black dashed line) falls within the high probability regions of the associated Posterior distribution. Considering the log marginal The model prediction, uncertainty intervals and synthetic **data** points are plotted versus x for the posterior distributions (a) employing the reported errors, and (b) weighing the errors using hyperparameters. Note that for a visual effect we have rescaled the errorbars in (b) by the best fit hyperparameters.
"
15394341,18/18,1.0,1.0,"Table S1 :
S1Autocorrelation input parameters. These were used for detection in synthetic **data** (except the event detection CC threshold), and in one week of CCOB.EHN data.Autocorrelation Parameter 
Value 
Time series window length 
200 samples (10 s) 
Time series window lag 
2 samples (0.1 s) 
Similarity search: near-repeat exclusion parameter 
50 samples (5 s) 
Scale factor β for MAD, for initial threshold 
5 
Event detection CC threshold 
0.818 
Near-duplicate pair and event elimination time window 
21 s 
FAST and catalog comparison time window 
19 s "
53612739,14/14,1.0,1.0,"FIG. 4 .
4|000 . . . y (as usual) and the transverse field B(t) is instantly switched from B = B 0 to B = 0(Color Online) State probabilities of all 2 6 = 64 spin configurations for each local adiabatic **data** point inFig. 3(a), ordered in binary (e.g. |010101 = 21 and |101010 = 42). The two degenerate AFM states (solid blue) are the most prevalent for all times.
"
222090354,10/21,2.0,0.476,"Proposition 4.29. Let µ ∈ AC([0, ∞); P 2 (R)) be a 2-curve of maximal slope of the energy E with respect to the weak upper gradient G, for initial **data** µ 0 ∈ Z E ∩ Y. Then,
lim t→∞ d LP (µ(t), E w ) = 0,
where E w ⊂ P(R) is the set of weak stationary states of the 2-curves of maximal slope associated to E and G."
209947757,3/18,1.0,0.167,"All blades are excited using an impact hammer with soft tip. In total, eight excitation points are positioned at two cross-sections at 3.7 m and 17.5 m, respectively. Edgewise hammer hits have been performed at the leading edge, as flapwise excitation is conducted at three discrete points in each section, i.e. on the girder, on the shell, and on the trailing edge. The cross-sections are selected with regard to a maximum of deflection appearing at the sensor positions predicted from the first ten mode shapes of the FE model. In order to identify rigid body modes, **data** is acquired using a large window size of 81.92 seconds. The processed frequency response functions are averaged from 10 hammer hits. Additionally, in the test of blade #1 an electrodynamic shaker is applied at the same points emitting broadband random excitation. It has been found that all mode shapes are sufficiently excited for identification using the modal hammer only."
222090354,5/21,2.0,0.238,"Remark 4.7. We note that in the case of general initial **data** ρ 0 ∈ P 2 (T) it is non-trivial to identify which distinct element of T ρ * is seen in the long-time limit. We refer the reader to the discussion in [GPP12, Lemma 2.2, Theorem 4.6] in which it is shown that under certain mild assumptions a distinct limit in T ρ * is selected. 
∂ t ρ = ∇ · (ρ∇(log ρ + V + W * ρ)) ρ(0) = ρ 0 ∈ P 2 (R d ) , (4.4)
where V : R d → R is a confining potential and W : R d → R is an interaction potential satisfying the following assumptions:
V ∈ C 2 (R d ) and there exists λ > 0 such that D 2 V (x) ≥ λ for all x / ∈ K ⊂ R d , compact, (V1) lim |x|→∞ D 2 V (x) = +∞,(V2)
W ∈ C 2 (R d ) even and positive."
11224784,6/14,1.0,0.429,"Although the abundances of heavy s-process elements could not be estimated, the upper limits of the abundances of Ce, Nd do demonstrate the lighter s-process elements are more enhanced even in EHe stars. It is generally acknowledged that 13 C(α, n) 16 O is the main source of neutrons to run the s-processing in the He-burning shells of intermediate-mass AGB stars. Sufficient amounts of 13 C are to be generated by slow mixing of protons into the 12 C rich intershell regions to generate neutrons. The neutron irradiation occurs in radiative conditions. The heavier the neutron flux the greater is the abundance of heavies relative to light s-process elements. Busso et al. (2001)  the distribution of the ratio of heavy s-process celements (hs) to the light s-process (ls) elements with respect to metallicity to characterize various parameters of neutron exposures during the third dredgeup phase in AGB stars eg. mass of 13 C pocket in the inter shell regions. Reddy et al. (2002) showed that the variation of the [hs/ls] with respect to metallicity in post -AGB stars (that went through third dredgeup) is characterized by a model ST/1.5 of Busso et al. (2001). 4 ) shows that the enhancements are positive and both show a similar range in their abundances. We compared the run of the ratio of [ls/hs] for RCBs and EHes with respect to the metallicity parameter [M] . The estimates for EHes are based on the upper limits for the heavy s-process elements and includes **data** from our ongoing analysis of the HST UV spectra. Estimates of U Aqr and the born again giant, Sakurai's object (during May -Oct 1996) are also included for comparison. Both the groups RCBs and EHes blend together emphasizing the similarity in their ls/hs ratios. that shown by post-AGB stars. It also, probably, suggests the s-processing in RCBs and EHes is not a result of the third dredgeup and could have happened when the stars passed through a second AGB phase (presumably)."
119354744,3/4,1.0,0.75,"In some types of **data** on multiparticle production processes one frequently encounters ambiguity concerning the question, which of the particular models used at that time is the correct one [2]. Such situation arises always when **data** contain only limited amount of information. To select this information one has apply information theory methods, which are widely known and used in other branches of science [1]. The examples shown here show that information theory ideas can be successfully used also to analyse **data** from multiparticle production processes and that in this way one gets highly model independent estimation of some quantities, in our example it was inelasticity parameter K [4]. Actually, in [21] we have attempted to fit single particle distribution without a priori introducing neither inelasticity nor fluctuations in mean multiplicity and found that it is possible only with q < 1. The reason turned out to be simple: in this case the most important factor was decreasing of the available phase space to mimic the action of the inelasticity and this can be done only with q < 1. The fit was not as good as shown here with notion of inelasticity introduced explicitly but it was not very bad either. As in [11,12] we are stressing here the connection between necessity to use nonextensive version of information theory and some intrinsic fluctuations existing in the hadronizing system. Finally, as was clearly displayed in Fig.2, our method seems to be also very useful in describing the gross features of the single particle spectra observed in heavy ion collisions. In particular, it seems that with growing energy of collision there is room for some new mechanisms, not present at more elementary nucleonic collisions. This point deserves therefore some special scrutiny in the future."
207778550,24/48,1.0,0.5,"Lower-triangle of the correlation matrix, ρ 8.0 0.5670 (28)  results within a **data** set are obtained from measurements at a different incident neutron energy."
17895596,6/11,1.0,0.545,"With the assistance of Lyman alpha data, one can model the luminosity function and evolution. With the SKA morphology **data** we expect to be able to identify different type of sources. This will allow us to study cross correlations between star forming galaxies and AGNs. We could also cross correlate with the CMB and different types of radio sources, which have different redshift distributions."
119383949,1/4,1.0,0.25,"The device investigated here consists of electrostatic gates confining a high-mobility 2DEG in a GaAs/GaAlAs heterostructure (see Fig. 1). The 2DEG was 82nm beneath the surface, its electron density was ≈ 3.47 * 10 15 m −2 , and the mobility about 100m 2 V −1 s −1 . Four metallic gates are used to define a long, narrow channel (5µm×1µm). These and two circular antidot gates are contacted individually. Details about the device are presented in [19][20][21] and the references cited therein. All measurements were taken at T ≈ 100 mK using standard low-excitation AC techniques. The dots in Fig. 2(a) show the experimental maximum positions of δG xx as functions of magnetic field B and antidot gate voltage V g [21]. The nearly equally spaced maxima and their shift to higher B for decreasing antidot diameter can be understood in analogy to the Aharonov-Bohm (AB) effect, if the AB ring is identified with cyclotron orbits around the antidots. Extracting the effective area from the experimental **data** yields a diameter between 0.76µm and 0.86µm, which is consistent with the device dimensions. The dislocations of the peak positions (see the boxes in Fig. 2), however, cannot be understood within this simple picture. They have been qualitatively reproduced in a quantum calculation by Kirczenov et al. [21]. Our objective is to decide if these dislocations and the variation of the spacings between the maxima can be understood semiclassically (which was doubted in Refs. [19,21]). For the effective one-electron model potential we follow essentially Kirczenov et al. [21] who assumed a parabolic shape V (r) = E F [r/a 0 − (1 + s)] 2 for r < a 0 (1 + s) and V (r) = 0 otherwise. Here r denotes the distance to the gate, and a 0 is the length scale over which the potential falls from E F to 0, i.e., the diffuseness of the potential. s is a dimensionless parameter modeling the depletion width around the gates. We use a 0 = 0.05µm and s = s c = 1 for the gates defining the channel throughout this paper. The depletion width s d of the antidot gates is varied between 1.5 and 2.2. According to Ref. [21], this corresponds to an effective antidot diameter of ≈ 0.35µm to 0.42µm."
17837291,1/52,1.0,0.019,"Step 2. Gluing identity. Consider the vector bundle U d := π * V d → M d (X), restricted to the fixed point components F r . A point in (C, f ) in F r is a pair (C 1 , f 1 , x 1 )×(C 2 , f 2 , x 2 ) of 1-pointed stable maps glued together at the marked points, ie. f 1 (x 1 ) = f 2 (x 2 ). From this, we get an exact sequence of bundles on F r :
0 → i * r U d → U ′ r ⊕ U ′ d−r → e * V → 0.
Here i * r U d is the restriction to F r of the bundle U d → M d (X). And U ′ r is the pullback of the bundle U r → M 0,1 (d, X) induced by V , and similarly for U ′ d−r . Taking the multiplicative characteristic class b, we get the identity on F r :
e * b(V )b(i * r U d ) = b(U ′ r )b(U ′ d−r ).
This is what we call the gluing identity. This may be translated to a similar quadratic identity, via Step 1, for Q d in the equivariant cohomology groups H * S 1 (W d ). The new identity is called the Euler **data** identity."
53380409,87/119,1.0,0.731,"A combination of results from searches for heavy resonance production in various bosonic and leptonic final states is presented. The **data** were collected with the ATLAS detector at the LHC in pp collisions at ffiffi ffi s p ¼ 13 TeV and correspond to an integrated luminosity of 36.1 fb −1 . While previous combination efforts included only the decays of heavy resonances into VV and VH, the combination presented here also includes decays into lepton-antilepton final states. Compared to the individual analyses, the combined results strengthen the constraints on physics beyond the Standard Model and allow the constraints to be expressed in terms of the couplings to quarks, leptons, or bosons. The relative sensitivities of the different approaches are compared, including bosonic and leptonic final states or different production mechanisms such as quark-antiquark annihilation/gluon-gluon fusion vs vectorboson fusion."
246863515,8/44,1.0,0.182,"Nevertheless, our setting is substantially different from the existing proximal causal inference literature. The short-term outcomes (S 1 , S 3 ) are both affected by the treatment, so they do not satisfy the proxy conditions in Miao et al. [2016]. Our identification strategies also feature a novel use of the experimental data. This is crucial in our setting but not considered in the previous proximal causal inference literature. Notably, our identification strategy in Theorem 2 relies on a new selection bridge function. This bridge function is specialized to our **data** combination setting, without any analogue in the existing proximal causal inference literature."
53380409,57/119,1.0,0.479,"The small-R jet uncertainties are relevant for most of the channels in the combination, including those with leptonic final states that contain at least one neutrino, due to the impact of those uncertainties on the E miss T measurement. The uncertainties in the jet energy scale and resolution are derived by comparing the response between the **data** and the simulation in various kinematic regions and event topologies. Additional contributions to this uncertainty come from the dependence on the pileup activity and on the flavor composition of the jets as well as the punchthrough of the energy from the calorimeter into the muon spectrometer. An uncertainty in the efficiency for jets to satisfy the JVT requirements is assessed. The small-R jet uncertainties are summarized in Table VI. For large-R jets, the uncertainties in the energy, mass, and D 2 scales are estimated by comparing the ratio of calorimeter-based to track-based jet p T measurements in dijet events between the **data** and the simulation. The uncertainties in the jet mass resolution and jet energy resolution as well as D 2 are assessed by applying additional smearing of the jet observables according to the uncertainty in their resolution measurements. A summary of the large-R jet systematic uncertainties is provided in Table VII. The flavor-tagging uncertainty is evaluated by varying the data-to-MC corrections in various kinematic regions, based on the measured tagging efficiency and mistag rates. These variations are applied separately to b-hadron jets, c-hadron jets, and light (quark or gluon) jets, leading to three uncorrelated systematic uncertainties. An additional uncertainty is included due to the extrapolation for the jets with p T beyond the kinematic reach of the **data** calibration. The flavor-tagging uncertainties are summarized in Table VIII."
207778550,48/48,1.0,1.0,"Reaction AMS **data** 
TOF **data** 
Ratio 
35 Cl(n,γ) 8.33(0.32) [118] 9.39(0.29) [122] 
0.89 (0.04) 
40 Ca(n,γ) 6.18(0.37) [119] 6.7(0.70) [123] 
0.92 (0.10) 
54 Fe(n,γ) 26.7(1.10) [120] 30.8(1.60) [124] 
0.87 (0.06) 
58 Ni(n,γ) 30.4(2.3) [121] 34.1(1.70) [125, 126] 0.89 (0.08) 
Average 
0.89 "
207778550,8/48,1.0,0.167,"It can happen that no matter how hard evaluators attempt to apply rigorous evaluation procedures faithfully, and to track down, adjust, and include all the different sources of uncertainty known to them (including their correlations) for the various experiments considered, the resulting evaluated uncertainties may still appear to be inconsistent with the input data, and thus will be perceived as unacceptable by **data** users. Most often, these evaluated uncertainties are perceived as being too small, as mentioned above."
207778550,20/48,3.0,0.417,"Clearly, an investigator needs to assess whether making an effort to identify and quantify USU contributions would be a waste of time for very practical reasons.
A. USU in 252 Cf(sf ) νtot Evaluation
The 15-point **data** set of the neutron multiplicity ν tot experimental values for 252 Cf(sf) as compiled by Axton [82] had been used for the Thermal Neutron Constants (TNC) fit [80] that involved more than 100 experimental points of various thermal quantities [93] within the IAEA Neutron Data Standards 2017 [52]. The IAEA Neutron Data Standards were adopted for the ENDF/B-VIII.0 library [53]."
119313773,10/21,2.0,0.476,"• The amount of **data** N required to detect a breakdown of linear response for a given perturbation size can be very large. For the logistic map with a given perturbation size of the order of ε = O(10 −6 ) one needs at least N = 600, 000 for a smooth observable A(x) = x. Hence, an apparent linear response seen in a given time series might be spuriously caused by an insufficient quantity of data."
69674584,14/42,1.0,0.333,"The main change introduced in Explanatory Combinatorial Lexicology by the use of Lexical Systems as structural models of lexicons -as opposed to ""textual"" dictionaries -is that each lexical unit in this approach is first and foremost characterized by its lexical cluster (see 3.1 below): the micro-system of paradigmatic and syntagmatic relations the lexical unit is at the center of. Within the framework of Lexical System lexicography, the lexical cluster of each lexical unit is systematically taken into consideration while interpreting linguistic **data** and making decision on how to model them. This methodological aspect of the work will very clearly manifest itself in the remainder of the paper."
207778550,46/48,1.0,0.958,"TABLE VI .
VIThe average relative uncertainties for each **data** set i resulting from the variance analysis proposed by Badikov et al.[35] (< σi >, < ση,i >, < σ ,i >) are compared with the ones derived from the reported uncertainties (< ui >, < uη,i > , < u ,i >). The total uncertainties are indicated without a sub-index. For the uncertainties due to systematic and random effects the sub-indices η and , respectively, are used. They are derived by taking the average of the relative values for the three energy groups multiplied by 100. The last three columns give the ratio between the values from the variance analysis and the reported ones for each component and for the total uncertainty. Uncertainty ratios that differ significantly from one indicate overestimated (> 1 in red) or underestimated (< 1 in blue) reported uncertainties.Dataset 
Reported values 
Variance analysis 
Ratio 
< σi > < ση,i > < σ ,i > < ui > < uη,i > < u ,i > total η 
Tovesson 
0.91 
0.84 
0.34 
2.38 
2.31 
0.55 
2.63 2.76 1.63 
n TOF (1) 
2.40 
1.73 
1.77 
2.60 
2.08 
1.56 
1.08 1.20 0.94 
n TOF (2) 
3.65 
2.49 
2.67 
2.66 
2.34 
1.27 
0.73 0.94 0.48 
n TOF (3) 
3.65 
3.35 
1.44 
2.43 
2.12 
1.18 
0.67 0.63 0.82 
n TOF (4) 
4.39 
3.74 
2.30 
3.04 
2.29 
2.00 
0.69 0.61 0.87 
Behrens 
1.46 
0.81 
1.21 
2.57 
2.26 
1.22 
1.76 2.79 1.01 
Difilippo 
2.48 
2.39 
0.66 
2.77 
2.20 
1.68 
1.12 0.92 2.54 
Cierjacks 
3.01 
1.09 
2.81 
2.77 
2.36 
1.45 
0.92 2.16 0.52 
Coates 
3.44 
2.92 
1.82 
3.27 
2.31 
2.32 
0.95 0.79 1.27 
Shcherbakov 2.53 
2.45 
0.64 
2.47 
2.31 
0.88 
0.98 0.94 1.37 
Lisowski 
1.54 
0.94 
1.22 
2.59 
2.28 
1.24 
1.68 2.42 1.01 "
209947757,9/18,1.0,0.5,"In every test final modal models have been determined out of several excitation runs according to meaningful quality measures. Obtained results from multiple free-free tests allow for conclusions concerning effects of the finishing and the repeatability of the manufacturing process in terms of the modal parameters. For this blade series the impact of the finish on mode shape correlation is stronger than differences resulting from the production. Regarding frequency and damping, changes caused by the finishing and deviations between the blades within the same manufacturing steps are of a similar magnitude. For blade #1 a non-linear damping behaviour in the 1 st flap mode has been identified. Though, a comprehensive flutter analysis demands in preparation the examination on non-linear behaviour in other modes of interest. Due to the high sensor density and the advantages of both test scenarios, the acquired modal **data** represents an optimal basis for updating finite element models towards structural dynamic properties."
251040404,7/15,1.0,0.467,"In this section, we evaluate the performance of the proposed symbols-wise puncturing. The LDPC codes are from  the WLAN standard [18] and are decoded by the beliefpropagation (BP) algorithm with a maximum iteration number of 12. The output probability distribution P A of CCDM can be obtained by [7, Sec. III] and obeys the Maxwell-Boltzmann (MB) distribution. We choose CCDM rate as
R dm = H(A),
where H(·) denotes the entropy function. Note that when R dm is fixed, since the LDPC code rate provided is not continuous, we may need to append some filler bits (zero bits), which are assumed to be known at both the transmitter and the receiver, to reach the length of information bits after PS encoding. We start with the throughput performance. The average throughput is calculated by
TP = tmax t=1 k t j=1 n j · Pr {E t } ,(10)
where E t denotes that the t-th transmission is successful while the first t − 1 transmissions are failed. We evaluate the throughput performance of the symbol-wise puncturing over the AWGN channel with the **data** block length k = 648 and 864 and the LDPC encoded codeword length n c = 1296. Each codeword can be modulated to n = 216 64-QAM symbols and is allowed to be transmitted a maximum of t max = 3 times. More simulation parameters are detailed in Table I. Fig. 5 and Fig. 6 show the average throughput as a function of SNR with k = 864 and 648, respectively. For comparison, we evaluate the average throughput of transmitting uniform and shaped bits with sequential puncturing. The Gaussian limit and the mutual information (MI) with uniform and shaped 64-QAM inputs are also provided. We observe that the shaped symbolwise puncturing achieves a stable shaping gain across the SNR of at least 0.6 dB compared with the uniform under the same throughput. Note that the gain remains almost a constant with the increase of retransmission times. Instead, the gain of the shaped sequential puncturing drops rapidly when retransmission is required. In other words, at the same SNR, our symbol-wise puncturing can bring higher average throughput."
238834501,11/19,1.0,0.579,"Terminology. All discussions below apply to timeseries of **data** from a single sensor, in particular screen level air temperature. We think of such a timeseries as a **data** matrix X dt indexed by a calendar date d (rows; 'date axis') and a time t (columns; 'time axis'). The time period affected by the eclipse will be given the symbol t • ; this includes the full duration of the eclipse (C1-C4), plus an additional time span beyond C4 to account for some dynamical lag in the system (see Table 1). Conversely, all times not affected by the eclipse are symbolised by t • . Analogously, the day of the eclipse will be denoted d • and all other days by d • . When we talk about an average over a number of days, we mean that the averaging is performed over the date dimension and the result is a onedimensional array indexed by times (a daily cycle). All **data** used in this study comes from the meteorological surface station network operated by the Centro de Estudios Avanzados en Zonas Áridas (CEAZA) in La Serena, Chile, from here on called the CEAZAMet station network."
251711372,81/119,1.0,0.681,"The remainder of this paper is organized as follows. Section 2 reviews the relevant literature. Section 3 provides the testable research hypotheses. Section 4 presents the **data** and the event study methodology. In Section 5, we present and discuss our main findings. Section 6 concludes the paper."
238834501,13/19,2.0,0.684,"We calculate a distribution over prediction errors by subtracting the observed value at t • for all non-eclipse days from the corresponding estimates:Ŷ   The lasso, which we apply to both regression problems, is a regularised regression whose loss function contains a ℓ 1 penalty on the vector of regression coefficients β . It is this form of the penalty that leads to the coefficients for less influential predictors to be set to zero, and thereby to subset selection 54 . The penalty is scaled by a hyperparameter, denoted α here, which controls how many predictors are culled: the larger the value of α , the more coefficients become zero, and the lower the resulting model complexity. We choose its value by approximately minimising a root mean square test error (RMSE) computed by averaging PE 2 over t • and d • . The regression estimate itself is not overly sensitive to the precise value of α , and so we simply choose a single value that is approximately optimal for a number of meteorological stations (see Table 3).
d • t = X T d • tβ d • t • Y dt • = X dt •β d • t • .
We compare the lasso estimates with estimates based on simple averaging and smoothing. For the averaging, the **data** matrix's date dimension is first ordered according to the mean square difference over times t • between each day and eclipse day. Then, the N closest days, in this Euclidean distance sense, are averaged. A lower value of N corresponds to a larger value of α (stronger regularisation and fewer non-zero coefficients). As with α , we select a value for N which approximately minimizes the average test error over all stations."
208006088,16/26,1.0,0.615,"Missing **data** is troublesome for the OLMM, because it is not possible to take away a subset of the rows of H and retain orthogonality of the columns. Suppose that **data** are missing for a fixed subset of outputs. Collect that subset of outputs into y 1 , and let y 2 be the remainder of the outputs. The issue can then be resolved by considering a multi-headed version of the OLMM:
y 1 | H 1 , x ∼ GP(H 1 x(t), δ[t − t ]Λ 1 ), y 2 | H 2 , x ∼ GP(H 2 x(t), δ[t − t ]Λ 2 ).
For k heads, if y I = (y i ) i∈I for a subset of heads I ⊆ {1, . . . , k} are observed, then
T I y I = Λ T I i∈I Λ −1 Ti T i y i , Λ −1 T I = i∈I Λ −1 Ti .
While this new model resolves the missing **data** problem, the assumption that each collection of outputs (each head) is a linear transformation of the same collection of latent processes will be overly restrictive for many contexts: it is not typically the case that our prior beliefs about the relationships between outputs have anything to do with the availability of **data** pertaining to them."
238719786,4/14,2.0,0.286,"Step 2: B(y) = F(y)*M(y) B(y) = (y 2 +y+1) (y 10 +y 9 +y 8 +y 5 +y 4 +y 2 +y+1) B(y) = (y 10 +y 9 +y 8 +y 5 +y 4 +y 2 +y+1)
Step 3: B [000010100110111]
The digital signal B [000010100110111] is the output of the BCH encoder block. This output is sent to SIE block which converts B signal into USB supported format and sends to the other FPGA (receiver) using USB physical cable. Let us suppose that there are three errors introduced into transmitted signal B while transmission. So, B signal is changes to [010010100010011]. We need to detect the error bits in the received signal and need to correct the effectedbits due to transmission errors. Otherwise error **data** will be received by other FPGA which leads to wrong **data** processing which is not desired. To overcome this problem, we introduce BCH decoder block in the receiver FPGA after the SIE block which is represented in Fig.6. As we receive the transmitted **data** using USB cable, UTMI interface collects the **data** and sends to SIE block.  J(y) = (y 13 +y 10 +y 8 + y 4 +y+1) Step 2: Here p=3"
119419652,4/18,1.0,0.222,"The CMS collaboration has performed a set of differential W +jets cross-section measurements with 2.2 fb −1 of Run 2 **data** -the first such measurements to be performed with a √ s = 13 TeV dataset [9]. The analysis strategy targeted events with a single, isolated, high-p T muon and considered jet multiplicities up to N jets = 6. Differential results unfolded to stable particle level were presented as a function of a number of kinematic observables including: the transverse momentum and absolute rapidity of the i th leading jet; the azimuthal separation between the i th leading jet and the selected reconstructed muon 2 ; the jet multiplicity; the scalar p T sum of reconstructed jets (H T ); and the separation in η-φ space between the reconstructed muon and its closest reconstructed jet. The final results as a function of H T for the N jets ≥ 1 case can be seen in Figure 2a. The measurements from **data** were compared to the predictions from a variety of simulated datasets. In all cases the results for the angular observables were found to be well described. Disagreements were observed between the **data** and a leading-order (LO) MG_aMC@NLO prediction, which underestimates the measured **data** at low H T , as well as at low to moderate jet p T ."
238834501,12/19,4.0,0.632,"A physics-based model could in principle be initialized to the state of the atmosphere at the start of the eclipse and run predictively with eclipse-unaware radiative forcing in order to provide a reference estimate. If all necessary physical parameters are measured for a specific location, this could be a relatively confined boundary layer energy balance model. However, in our case we have little knowledge of important parameters such as surface albedo, boundary layer thickness and longwave radiation. As we have previously stated, advection plays an important role in the surface energy balance, and either a crude observational estimate based on widely spaced station measurements or a full numerical weather prediction (NWP) model is needed in order to account for it. However, such a physics-based modeling exercise represents a major effort, and we devised our statistical methods primarily in order to provide a simpler alternative. It is furthermore not clear whether the results from a physical model would be less uncertain than those of a statistical one, in particular under the circumstances described for the Coquimbo Region: relatively small swings between on-and offshore winds can have major effects on local temperatures, and the horizontal resolution of NWP models is coarse compared to the complexities of the terrain. Even more importantly, adequate **data** for model initialization is not available. "
118420717,3/10,2.0,0.3,"• From the simulation the quantities α, A(t), B(t) and Γ(t) are calculated using the filtered elements. Numerical forward differentiation with four **data** points was used to computeṗ andq."
126349038,21/28,1.0,0.75,"Fig. 9
9Axial variation of flow variables in SDTV (left column) and BKTV (right column) cores obtained using S1 through S4, and mean code with P E jj bars is compared with experimental **data** with U D bars for b 5 20 deg. The SDTV onset at x 5 0.077 and BKTV onset at x 5 0.4 are shown in radial location (R) subplot.
"
17671315,10/32,2.0,0.312,"In [24] the same calibration number CF was chosen for all five targets. It is important that because of the blind case, that choice was unbiased. If we would have measured the dielectric constant of at least one target in advance, then we would choose CF in such a way that the number R in (1), which would be obtained via solving the above CIP, would be close to the measured one. However, since dielectric constants were unknown in advance, then another method of choice of CF was used in [24]. Namely, consider the Laplace transform of the function u (x, t) ,
w(x, s) = L (u (x, t)) := ∞ 0 u(x, t)e −st dt, s ≥ s = const. > 0.(10)
We call the parameter s pseudo frequency. Denote
g (s) = Lg,(11)g 0 (s) = 1 2 L (H (t − |x 0 |)) = exp (−s |x 0 |) 2s , g (s) = g (s) − g 0 (s) .
Let g sim (s) be the Laplace transform of the function depicted on Fig. 4-b). Let g bush (s) be the Laplace transform of the function, which corresponds to bush on Fig. 5. Then we multiply all experimental **data** by such a calibration number CF that values of functions g sim (s) and CF · g bush (s) would be not far from each other for s ∈ [1,5]. It was shown in [24] that CF = 10 −7 was good for this goal. Furthermore, we have discovered that the same value of CF = 10 −7 provides similar behavior for Laplace transforms of four other curves of Fig. 5. In [24] we have also varied the calibration factor CF by 20% as CF 1 = 0.8 · 10 −7 , CF 2 = 1.2 · 10 −7 . In all three cases of CF, CF 1 and CF 2 the resulting values of dielectric constants were within tabulated limits [24]. Hence, we have chosen in [24] CF = 10 −7 ."
251500724,13/26,1.0,0.5,"Figure 2 .
2The kC temperature as a function of time during the printing stages: (a) cooling down from the loading temperature to the extruding temperature (the experiment at 40 °C is shown); (b) cooling down of the generated droplet (see text) after extrusion, during the self-supporting stage. The droplet was printed at 40 °C on the printer plate kept at 25 °C. Lines correspond to the best fit of the **data** to Equation(1).
"
2275365,7/18,1.0,0.389,"An interesting alternative interpretation of the Dempster-Shafer Theory was found within the framework of the rough set theory [32], [10]. The difference between our approach and the one based on rough sets lies first of all in the ""ideological"" background: We assume that the ""decision attribute"" is set-valued whereas the rough-set approach assumes it to be single-valued. This could have been overcome by some tricks which will not be explained in detail here.But the combination step is here essential: If we assume that the **data** sets for forming knowledge of these two experts are exhaustive, then it can never occur that these opinions are contradictory. But the MTE rule of combination uses the normalization factor for dealing with cases like this.  (
E 1 → D → E 2 E 1 ← D ← E 2 E 1 ← D → E 2 E 1 → D ← E 2Prob P (ω) ω E 1 (ω) = e 11 ∧ E 2 (ω) = e 22 ) = = ( Prob P (ω) ω E 1 (ω) = e 11 ) · ( Prob P (ω) ω E 2 (ω) = e 22 ) > 0 However, it is impossible that ( Prob P (ω) ω E 1 (ω) = e 11 ∧ E 2 (ω) = e 22 ) > 0 because we have
to do with experts who may provide us possibly with information not specific enough, but will never provide us with contradictory information. We conclude that unconditional independence of experts is impossible."
17671315,24/32,1.0,0.75,"Figure 2 :
2The schematic diagram of the **data** collection by the Forward Looking Radar model (4)-(8) of section 3 R is the function of the spatial variable x, i.e. R = R (x) := ε r (x) , x ∈ (0, 1) . Hence, after computing R (x) we took the value R to compute the dielectric constant of the target ε r (target), where
"
118072666,2/13,1.0,0.154,"The **data** used in this analysis were collected by the BABAR detector at the PEP-II storage ring. We use 17.6 fb −1 of **data** taken at the Υ (4S) resonance (onresonance) and 4.1 fb −1 of **data** taken at a center-of-mass energy 20 MeV below the BB threshold (off-resonance). The latter sample is used for the subtraction of the non-BB component (continuum) in the on-resonance data. These **data** samples were taken between January and May 2002. Additional data, consisting of 3.5 fb −1 of on- ‡ Also with Università della Basilicata, Potenza, Italy § Also with IFIC, Instituto de Física Corpuscular, CSIC-Universidad de Valencia, Valencia, Spain ¶ Deceased resonance **data** and 1 fb −1 of off-resonance **data** taken under different running conditions, are used for verification of the result."
15394341,1/18,1.0,0.056,"Continuous **data** time gaps Detection on synthetic **data** Reference code: Autocorrelation Near-repeat exclusion of similar pairs Postprocessing and thresholding Fig. S1. Illustration of comparison between many-to-many search methods for similar pairs of seismic events. Fig. S2. Twenty-second catalog earthquake waveforms, ordered by event time in 1 week of continuous **data** from CCOB.EHN (bandpass, 4 to 10 Hz). Fig. S3. Catalog events missed by FAST, detected by autocorrelation. Fig. S4. Twenty-second new (uncataloged) earthquake waveforms detected by FAST, ordered by event time in 1 week of continuous **data** from CCOB.EHN (bandpass, 4 to 10 Hz); FAST found a total of 68 new events. Fig. S5. FAST detection errors. Fig. S6. Example of uncataloged earthquake detected by FAST, missed by autocorrelation. Fig. S7. Histogram of similar fingerprint pairs output from FAST. Fig. S8. Schematic illustration of FAST output as a similarity matrix for one channel of continuous seismic data. Fig. S9. CC and Jaccard similarity for two similar earthquakes. Fig. S10. Theoretical probability of a successful search as a function of Jaccard similarity. Fig. S11. Synthetic **data** generation. Fig. S12. Hypothetical precision-recall curves from three different algorithms.  "
249258070,71/119,4.0,0.597,"The breakdown of self-initiated activities across these three units were 64.2% for patrol, 17.5% for special crime units, and 18.3% for the differential response team. Unfortunately, the HPD CFS **data** do not provide detailed information about the nature of self-initiated activities, such as offense or incident type for patrol officers. Activities performed by specialized crime investigation and DRT officers might be deduced approximately using the division information. But to further breakdown the activity to specific units produces small frequencies of specific offenses, which would have been insufficient to conduct meaningful analysis. For example, there were only 338 self-initiated activities recorded under gang division, and 165 self-initiated activities recorded under robbery division during the study period. The **data** do include some information on dispositional outcomes of police self-initiated activities. The most frequent type of action for self-initiated activities is ""collecting information"" for each of the three types of police units. The disposition of ""collecting information"" is used in instances where the incident is resolved without needing to generate an offense report or referral for further investigation. Some examples are if the officer resolves the issue by mediation, the complaint is a civil issue rather than criminal, or if it is to provide further information about an offense report that already exists. Other self-initiated actions include making an offense or supplementary report, making an arrest, issuing tickets, etc."
207778550,14/48,4.0,0.292,"USU are additional uncertainties that need to be included in the input database in order to be able to perform a reliable evaluation. Introduction of USU contributions after an evaluation has been performed is not comparable to introducing them in the input **data** prior to an evaluation, with the sole exception being the evaluation of a single physical quantity [78]. When USU contributions are included in the input data, their impact will be reflected in the output evaluated results. This is the proper way that USU should be treated in an evaluation."
119253123,12/32,2.0,0.375,"We correct for leading-track misidentification with a data-driven procedure. Starting from the measured distributions, for each event the track loss due to inefficiency is applied a second time to the **data** (having been applied the first time naturally by the detector) by rejecting tracks randomly. If the leading track is considered reconstructed it is used as before to define the different regions. Otherwise the sub-leading track is used. Since the tracking inefficiency is quite small (about 20%) applying it on the reconstructed **data** a second time does not alter the results significantly. To verify this statement we compared our results with a two step procedure. In this case the inefficiency is applied two times on measured data, half of its value at a time. The correction factor obtained in this way is compatible with the one step procedure. Furthermore, the data-driven procedure has been tested on simulated **data** where the true leading particle is known. We observed a discrepancy between the two methods, especially at low leading-track p T values, which is taken into account in the systematic error. The maximum leading-track misidentification correction is 8% on the final distributions."
236522469,25/33,1.0,0.758,"Figure 7 .
7Location of trees from field **data** and species recognition model.
"
235829209,30/47,1.0,0.638,"Two different surrogate models were used in our studies. A 7 th -order polynomial was used to study the Hénon map. In the examples provided by two storage ring lattices, a four-layer 4 × 12 × 12 × 4 ANN as shown in Fig. 6 was used. The  and processing time. In principle, a more complicated model can achieve predictions with greater accuracy, but would need more **data** and time to train. For these applications, high absolute prediction accuracy is not critical because only a relative accuracy is needed to direct the optimizer to converge. For example, a three-layer ANN was also tested and confirmed functional for the lattice optimization. However, its statistical fluctuations were larger than the one used in the four-layer ANN and therefore were not the focus of the investigation. Although, at the early stages of optimization, a course, simple-structured model could be used to quickly narrow down the ranges of searching parameters. This would be much more computationally efficient. Then another, more complex model can be deployed for a finer, more accurate and precise search. Besides the application of an ANN and a polynomial regression, another surrogate model using the support vector regression [30] was also confirmed to be able to see the existence of correlations. Other surrogate models should be functional as well with this technique, although they were not tested in this investigation."
54041858,5/45,2.0,0.111,"The AE **data** were separated into events which occurred near the peak of the cycle and events which occurred near the valley of the cycle. Figure 3a shows the cumulative AE energy **data** for the Typically, for these types of composites, most of the AE energy occurs on initial loading and dissipates as matrix crack formation decreases or ends with increasing stress. Consequently, the increase in AE energy accumulation is only moderate near failure. The rationale for this is that large transverse matrix cracks which occur over the entire gage section create considerable energy since they are of large area and high modulus. When the crack forms and propagates, considerable surface energy is created from hundreds of cracks (several per mm in multiple plies) during the loading of the specimen. Failure in a typical tensile test for these types of composites would mostly involve fiber failure, which are of significantly smaller cross-sectional area and are only located within a narrow region of the gage section. A striking feature for some of the fatigue specimens in this study is that several of the specimens showed more AE energy cumulated at the end of the test than the entire stress history that precedes it. However, two of the tests did show only a minor increase in AE activity at failure as is typical of these types of composites when tested to failure in tension. It is apparent that the specimens which showed dramatic increase in AE at failure were tested at higher frequencies (two 1 Hz and one 0.1 Hz), whereas the two specimens that did not show this behavior were at lower frequencies (one 0.1 and one 0.01 Hz). To understand what was causing the difference in AE behavior, the methodology for AE analysis described above was employed with some microstructural analysis to correlate AE activity with the damage mechanisms that developed during the tests."
233705419,21/53,2.0,0.396,"The overview of the model shown in Figure 6 illustrates the **data** set used, the target, and influencing parameters, as well as the quality of the model. "
17895596,3/11,1.0,0.273,"SKA continuum surveys at low frequencies (< 1 GHz) should be ideal to probe the cosmic radio dipole already in the Early Science phase for two reasons. First, it is not necessary to cover the full area of the 3π surveys, since a sparse sampling spread out over all of the accessible sky should be sufficient for a first estimate. And second, a focus on low frequencies and bright sources will pick primarily AGNs which have a much higher mean redshift than the SFG. Figure 1 illustrates the accuracy that we can hope to achieve for a measurement of the radio dipole based on a linear estimator (Crawford 2009;Rubart & Schwarz 2013). Our estimates are based on differential number counts from surveys in small and deep fields and simulations (Wilman et al. 2008). Our expectations for all-sky continuum surveys are summarized in Table 1. We find that the cosmic radio dipole can be measured at high statistical significance, even taking realistic **data** cuts into account (e.g. masking the galaxy and very bright extragalactic sources, or morphology, spectral index or flux cuts)."
119419652,11/18,1.0,0.611,"Recent developments in analytical methods have introduced jet substructure observables accurate beyond leading-logarithm accuracy. One recent ATLAS publication based on 32.9 fb −1 of √ s = 13 TeV **data** presents a first measurement involving the jet soft-drop mass -one such substructure variable -using dijet events from p-p collisions [38]. The analysis features a reclustering of the constituents of anti−k T jets based on a designated soft-drop mass procedure. This procedure is based on an iterative condition which is governed primarily by the choice of two parameters: z cut and β , which relate to p T and angular thresholds in the context of gluon radiation. The final results correspond to normalized differential cross-section measurements, presented as a function of the logarithm of the dimensionless quantity ρ 2 , where ρ is the ratio of the invariant mass of the jet following the soft-drop procedure and the transverse momentum of the ungroomed jet (ρ = m soft drop /p ungroomed T ). The inclusion of p ungroomed T in the denominator counters an undesireable p T dependence. The distribution shown in Figure 7b, corresponding to the particular choice of β = 0, can be contrasted with other such distributions (for β = 1, 2) in the original publication. An unfolding procedure is used to correct for detector-level effects. The evolution of the distributions with β and z cut highlights particular regions of the quantity log 10 ρ 2 with larger levels of disagreement between **data** and predictions. Such disagreements between the measured **data** and either precise QCD calculations or leading-logorithm Monte Carlo simulations will prove beneficial in improving our understanding of the collinear QCD regime."
232380196,23/28,2.0,0.821,"Implementation details for iNaturalist 2018 For iNaturalist 2018, following most of the existing work, we use ResNet-50 [10] as backbone network. The **data** augmentation is similar to that used in long-tailed CIFAR datasets except that random cropping with size 224 × 224 is used. To fit two NVIDIA 2080Ti GPUs, we use a batch size of 100 for both SC and PSC based hybrid networks. The networks are trained for 100 epochs using SGD with momentum 0.9 and weight decay 1 × 10 −4 . The initial learning rate is 0.05, which is decayed by a factor of 10 at epoch 60 and epoch 80. Motivated by the fact that iNaturalist has a large number of classes which can make classifier learning more difficult, we assign higher weighting to the classifier learning branch by using a linearly decayed weighting factor α, i.e., α = 1 − T /T max . The temperature τ is set to be 0.1 for both SC and PSC loss functions. For SC loss function, the number of positive samples for each anchor is fixed to 2."
118490678,2/15,1.0,0.133,"We determine the parameters λ s and Nd/N d by fitting the two hadron ratios K + /π + (or K − /π − ) and p/p. The other hadron ratios can be obtained accordingly with QCM. The parameter Nd/N d , a measure of the net baryon number in QGP, [3] Note that the combination picture or CO/RE mechanism is the same in all coalescence/recombination/combination models, but the methods to implement the mechanism are different in detail. is taken as 0.859, 0.915, and 0.927 for √ s NN = 62.4, 130, and 200 GeV respectively. Likewise, λ s is 0.52, 0.48, and 0.48 for the three RHIC energies, and this is consistent with the **data** fitting in Ref. [29]. Similar to the same reference, we suppose λ s is saturated in AA reactions above 200 GeV, that is, QGP created at LHC also has λ s = 0.48. Assuming the parameters V c /P c and D c /O c are universal in relativistic AA reactions, we use 3.0 and 0.5 as their default values in QCM at various energies, respectively. The same default value of V c /P c is also used in PYTHIA."
19033986,13/28,1.0,0.464,"Similarly, the lack of an edge in the e + + e − spectrum implies that DM that dominantly annihilates or decays into 2e is now firmly excluded, and one can constrain the BR of the subdominant e ± primary channel. Assuming that DM annihilates into leptons with lepton flavor components BR e , BR µ , BR τ that sum to unity, BR e + BR µ + BR τ = 1 one can constrain the allowed regions in light of the PAMELA, FERMI and HESS **data** together with the other constraints. To clarify the situation we show the constraints on the branching fractions for three distinct cases:"
248366486,4/24,1.0,0.167,"We notice that no evidences of eventual saturation to a steady-state value [15,16] have been observed, due to the large lattice sizes of the simulated systems. Conversely, we should also remark that we have not detected any substantial time dependence of the exponent values that will be reported below at our long times. Thus, we have avoided entering the very-long time regime explored in Ref. [14] in which the precursor film has grown so wide that diffusion is no longer able to communicate its front with the liquid reservoir efficiently, and the front evolves effectively as if there were no external driving. Table III in Appendix A reports the growth exponent values obtained for both layers in this study. This table indicates that the detailed value of β depends on the physical parameters A and T . The same **data** are summarized graphically in  Table III. As a visual reference, the solid black line corresponds to w 2 (t) ∼ t 1/2 . All units are arbitrary. and supernatant layers, and does not depend on the Hamaker constant A. At low temperatures (T < 1), the growth exponent is slightly higher for the precursor layer and seems more sensitive to the value of A both for the precursor and for the supernatant layers. As a reference value in the lowtemperature regime, the kMC simulations of Ref. [14] obtained β 1/6 for the precursor layer using J = 1, A = 10, and T = 1/3, which is compatible with our results."
53612739,7/14,2.0,0.5,"The **data** in Fig. 3(b) show how the ground state probability grows during a single 2.4 ms linear, exponential, or local adiabatic ramp. The ground state population grows quickly under local adiabatic evolution since the transverse field B(t) is reduced quickly at first. In contrast, the linear ramp does not approach the paramagnetic to AFM phase transition until ∼ 2 ms, and the AFM probability is suppressed until this time. Once again, local adiabatic ramps show the largest ground state probability at each time."
207778550,31/48,4.0,0.646,"In a series of irradiations at Karlsruhe Intitute of Technology (KIT), neutron capture reactions for a 25-keV Maxwell-Boltzmann neutron energy distribution were studied, mainly for reactions that are relevant for s-process nucleosynthesis. AMS **data** obtained from such identical activations are used here, but they were converted into 30-keV MACS values for a direct comparison with TOF data. TOF measurement techniques have been well-established for several decades. In TOF, usually the prompt signature from the de-excitation of the reaction product is measured and used to generate a cross-section value. This approach involves different and also more sophisticated **data** processing procedures compared to AMS. The TOF **data** plotted here are based on measurements at ORELA ( 35 Cl, 40 Ca and 58 Ni) and n TOF/CERN ( 54 Fe and 58 Ni). In the activation reaction studies for AMS, the irradiation setup was designed such that the integrated neutron energy distribution resembled closely a Maxwellian distribution. Thus the measured cross section approximated an energy-averaged value. In TOF the energy-dependent cross sections are folded with the respective neutron flux energy-distribution for the MACS. There are no correlations between these two techniques. We find a systematic deviation between the two methods as seen in Table X. AMS **data** are systematically lower by about 11% compared to TOF **data** for these four reactions. In general, AMS measurements are normalized to reference materials which are independent from each other, and thus there is no correlation in AMS between different reactions. AMS and TOF **data** were both acquired at two different laboratories (Univ. of Vienna and TU Munich, and ORELA and n TOF, respectively). Currently, it is not known what the cause for this systematic deviation could be. The uncertainties of these ratios are generally lower than the observed deviation, leading to a potential estimate of an USU value. For instance, a numerical value for an USU contribution could be assumed to be the minimum deviation that make all measured **data** statistically consistent. If so, this could be estimated to be 5-7%."
119415579,8/41,3.0,0.195,"The periodogram calculation was followed by the application of a commonly used period search analysis described, for example, by Alecian et al. (2014). The method involves fitting the time series **data** to a function consisting of the first two or three terms in a Fourier series using a range of fixed periods (P ); plausible rotational periods are identified as those which yield the lowest χ 2 values. We adopted a 2 nd order sinusoidal fitting function given by
f (t) = C0 + C1 sin (2π[t − t0]/P + φ1)+ C2 sin (4π[t − t0]/P + φ2) (1)
where t0 is the epoch (set to zero during the period search analysis) and C0, C1, C2, φ1, and φ2 are free parameters. We defined an initial grid of period values having a step size of ∆P = 10 −4 d and spanning 0.1 P 25 d. For each P value in the grid, the best fit was derived and the associated χ 2 values were recorded. This analysis was repeated with C2 ≡ 0 (reducing Eqn. 1 to a 1 st order sinusoidal fitting function), which was frequently found to decrease the number of statistically significant periods derived from the Bz **data** sets. This is related to the fact that longitudinal field measurements of mCP stars are most sensitive to the dipole component (e.g. Eqn. 68 of Bagnulo, Innocenti & Degl'Innocenti 1996). Nevertheless, significant higher-degree contributions to Bz curves are often detected in high-precision **data** (e.g. Kochukhov et al. 2004;Kochukhov, Makaganiuk & Piskunov 2010;Silvester, Kochukhov & Wade 2015)."
119247546,24/46,2.0,0.522,"Proposition 1. The conformal curves with initial **data** given by (33) and r * > r exist for all τ ∈ [0, ∞). The curves reach future null infinity for a finite value of the parameter τ ."
209947757,4/18,1.0,0.222,"There are certain advantages of a clamped boundary condition with view to validation. First, the clamping resembles to the hub connection which applies when rotor blades are used in operation. Once derived from modal testing, modal **data** can be utilised in updating approaches [12]. Even if adjusted model parameters are selected reasonably, it is evident that independent updating based on modal **data** from differing boundary conditions brings about unequal finite element models. Having in mind the apparent resemblance of the connection, the model obtained from test rig **data** might be most suitable. Second, the clamping enables higher force input and other excitation techniques like snapback and sine sweeps. On account of higher forces, the blade undergoes larger deformations such that potential non-linear behaviour may be revealed. Knowing the modal properties at larger deformations is particularly beneficial for solving the stability problem of flutter."
207778550,13/48,1.0,0.271,"If the population standard deviation of the input **data** points (without regard to assigned uncertainties) is noticeably different from the average of the magnitudes of uncertainties assigned to these data, then either the assigned uncertainties are over-estimated or under-estimated. In the latter case, the difference may be attributable to USU. This clue is related to Clue 1."
207778550,32/48,8.0,0.667,"Methods were suggested during the course of the present investigation on how to update the Neutron Data Standards evaluation with an improved formulation of contributions from USU. These approaches will be applied in the future for this particular evaluation effort. XI: Selected absolute 238 U/ 235 U cross-section ratio **data** from 8.0 up to 15 MeV. En is the incident neutron energy. R 8/5 is the cross-section ratio. u , uη, unorm, and utot are the cross-section ratio uncorrelated, correlated, normalization, and total uncertainty components, correspondingly. u1, u2, u3, u4, and u5 are additional MERC (Medium Energy Range Correlations) uncertainty components as used in GMAP. All uncertainties are given in %."
85554746,17/19,1.0,0.895,"Figure 4 (
4b) depicts the land-based stations of the WMO's Global Observing System, which is derived from the National Meteorological and Hydrological Services of WMO Members, as well as the SASSCAL Weather Net network in Southern Africa. Overall, although long time series of meteorological **data** are crucial in the context of climate change research, meteorological observations also show a strong bias towards Southern Africa. With regards to oceanic observations, figure 4(c) illustrates that, compared to the Mediterranean Sea, the oceans surrounding the African continent are only sparsely monitored by the global oceanic observation networks identified in this study.
"
119335883,2/26,2.0,0.077,"Though both these tools and others have shortcomings and merits, they have a significant shortcoming: because they rely on election **data** only, they cannot themselves show that map-drawers actually drew a partisan map. That is, perhaps the election **data** looks suspicious, but maybe the electorate have self-sorted in a way that makes a lopsided seat share unavoidable."
24865762,4/34,1.0,0.118,"Remark IV.1 The eigenfunctions are thus phase-shifted sinusoids, for which the phase θ k relates to the phase spectrum of the **data** φ . The eigenvalues provide the power spectrum. So far, for a one-dimensional signal φ , one can recover the Fourier spectrum (power and phase) from the spectral elements of L φ . The operator representation of the Fourier spectrum via the operator L φ allows us to propose an innovative generalization for multidimensional signals (see Sect. V) turns out to be particularly useful for the inverse stochastic-dynamic modeling of spatio-temporal scalar fields; see Sectns. VII and VIII."
246863515,1/44,2.0,0.023,"In this paper, we study the identification and estimation of long-term treatment effects when both experimental and observational **data** are available. By combining these two different types of data, we hope to leverage their complementary strengths, i.e., the randomized treatment assignments in the experimental data, and the long-term outcome observations in the observational data. In particular, we aim to tackle complex persistent confounding in the observational data. That is, we allow some unmeasured confounders to have persistent effects, in the sense that they can affect not only the short-term outcomes, but also the long-term outcome. Persistent confounders are prevalent in long-term studies. For example, in the study of early childhood education and lifetime earnings, students' intelligence or capabilities can affect both short-term and long-term earnings. Our setup is summarized in the causal diagrams in Figure 1."
17671315,1/32,1.0,0.031,The goal of this publication is to compare performances of two numerical methods listed in the title on both computationally simulated and experimental data. The reason why we do not compare other methods is a practical one: such a study would require much more time and effort than the authors can afford to spend. We work with the time resolved **data** of scattering of the electrical wave field and these **data** are the same as ones in [24]. The **data** were collected in the field in the cluttered environment by the Forward Looking Radar of US Army Research Laboratory (ARL) [30]. The goal of this radar is to detect and possibly identify shallow explosives.
40772223,7/11,1.0,0.636,"The result for the beam momentum for a nominal settings of 450 GeV/c is given in Table 2. The beam momentum is 449.16 ± 0.14 GeV/c, which is −0.19 ± 0.03% lower than the nominal momentum. The accuracy on the beam momentum of 0.03% is dominated by the systematic effects. The intrinsic accuracy is 0.01−0.02%. A more detailed account of the **data** analysis and of the results is given in Reference [5]."
53648778,12/14,1.0,0.857,"Figure 7 :
7The comparison of the lattice **data** for the trace anomaly with HTL perturbation theory (red) and EQCD (green) each shown with a line (µ = 2πT ) and a band (πT to 4πT ). Data with N τ = 6 and 4 for T > 1 GeV have been rescaled by 1.4 and 1.2, respectively.
"
117119864,70/83,1.0,0.843,"Panel (a) shows resonant ARPES spectra from Ref. [18] for the heavy Fermion material URu2Si2 [31,86], obtained along the Γ-X line at a photon energy near the U 4d edge where U 5f weight is resonantly enhanced, and panel (b) shows an intensity plot of the data. Panel (c) shows a similar intensity plot of ARPES **data** along this line but for a photon energy where the 5f weight is suppressed so that presence of a d-character hole pocket centered on X is easily seen. Panels (b) and (c) show the confinement of fweight inside the d-electron hole pocket, with striking similarity to the model weight distribution of panel (e). The ARPES spectra of CeRu2Si2 discussed above display very similar f confinement to the hole pockets around the Z-point."
237649489,5/49,1.0,0.102,"The CA-Markov model is composed of CA model, Markov chain and multi-criteria evaluation (MCE) [29]. The CA model is a discrete, finite state composition of the meta-cell model. It can simulate complex dynamic systems with spatial-temporal characteristics according to certain local rules [43]. Markov chains create the transfer matrix and probability between land use types for multiple time periods in the past through spatial comparison analysis, which are the basic **data** for predicting future land use patterns. MCE refers to the selection of expansion factors to construct a land use transition suitability image collection. CA-Markov model can effectively predict future land use dynamics [44]. The prediction process equation of the CA-Markov model is shown below [45]:
Ctj + 1 = F[Ctj, N](5)
where C(t j ) and C(t j+1 ) are the states of the cell at time t j and t j+1 , respectively; F is the transition rule; N is the domain of the cellular. In this study, elevation, slope, earthquake, landslide, highway, main road, railroad and population density were selected as the driving factors affecting urban expansion. Among them, elevation, slope, earthquake and landslide are negative indicators, and the rest are positive indicators. The suitability evaluation maps for earthquakes, landslides, highways, main roads and railroads were calculated by the kernel density tool, and finally, the normalized driver maps were obtained ( Figure 5). The Kappa coefficient was applied to check the accuracy of the CA-Markov model simulation results [46]. The formula for calculating the Kappa coefficient is shown below:
Kappa = p a − p c 1 − p c (6) p a = s n (7) p c = a 1 * b 1 + a 2 * b 2 n * n(8)
where n is the total number of cell sizes in the raster; a 1 is the number of cell sizes in the real raster of the urban land; a 2 is the number of cell sizes in the non-urban land; b 1 is the number of cell sizes in the simulated raster of the urban land; b 2 is the number of cell sizes in the non-urban land; s is the number of cell sizes in the real raster and the simulated raster that correspond to each other. "
119253123,5/32,1.0,0.156,"The analysis uses two **data** sets which were taken at the center-of-mass energies of √ s = 0.9 and 7 TeV. In May 2010, ALICE recorded about 6 million good quality minimum-bias events at √ s = 0.9 TeV. The luminosity was of the order of 10 26 cm −2 s −1 and, thus, the probability for pile-up events in the same bunch crossing was negligible. The √ s = 7 TeV sample of about 25 million events was collected in April 2010 with a luminosity of 10 27 cm −2 s −1 . In this case the mean number of interactions per bunch crossing µ ranges from 0.005 to 0.04. A set of high pile-up probability runs (µ = 0.2-2) was analysed in order to study our pile-up rejection procedure and determine its related uncertainty. Those runs are excluded from the analysis."
207778550,32/48,7.0,0.667,"One example of USU effects, formulated by considering measurements of neutron-induced fission cross-section ratios of 238 U/ 235 U, highlighted the point that different USU estimation approaches result in slightly different but yet consistent evaluated mean values and uncertainties. Under/overestimation of reported uncertainties have been found. This example illustrated the subjective nature of estimating the effect of USU. However, this subjectivity does not apply only to estimating USU effects but also holds true for evaluating mean values and covariances in general. After all, these derived values depend on the evaluation techniques utilized, input **data** selected and subjective choices made on the parameters of models, corrections of experimental **data** and on methods for estimating covariances of both, model and experiment. Hence, our evaluated results are always subjective-with and without consideration of USU. However, if one neglects adding obviously necessary contributions from USU to uncertainties of input data, the evaluated mean values might be more biased and the evaluated uncertainties will be underestimated, in turn, adversely impacting application calculations."
52995678,14/29,1.0,0.483,"In reduced order modeling, the basis is generally built from Eulerian data, e.g., velocity, vorticity, or streamfunction. Thus, ROMs generally are Eulerian ROMs. Of course, these Eulerian ROMs can efficiently approximate the corresponding Eulerian fields. However, Lagrangian fields (e.g., the FTLE field) might be better approximated by Lagrangian ROMs, i.e., ROMs whose bases are built from both Eulerian and Lagrangian data. The new α-ROM and λ-ROM are Lagrangian ROMs, since the FTLE weighted inner products (23) and (25) use both Eulerian **data** (i.e., the vorticity ω) and Lagrangian **data** (i.e., the FTLE λ). The input Eulerian **data** (i.e., the vorticity field) help the resulting FTLE-ROMs yield an accurate approximation of the output Eulerian **data** (i.e., the vorticity, streamfunction, and velocity fields computed by the FTLE-ROM). On the other hand, the input Lagrangian **data** (i.e., the FTLE field) ""nudge"" the FTLE-ROMs toward an accurate approximation of the output Lagrangian **data** (i.e., the FTLE field obtained from the FTLE-ROM velocity field). Thus, we expect the new α-ROM and λ-ROM to yield more accurate FTLE approximations than the standard G-ROM (17). In Section 5, we investigate the new α-ROM and λ-ROM, as well as the standard G-ROM, in the FTLE computation of a test problem that uses the QGE as mathematical model."
245986448,14/45,1.0,0.311,"A well-worn approach is to maximise the logarithm of the marginal likelihood, conditioned on the hyperparameters 29 . The values found from such a method are such that they maximise the likelihood that the user-specified model produces the **data** that is observed.
maximize θ log (p) := − 1 2 µ T ρ C −1 ρ µρ − 1 2 log Cρ − N 2 log (2π) ,(18)
where the mean and covariance terms are evaluated at the observed **data** only. A more developed approach is the Maximum a Posteriori (MAP) estimation method which in addition to the log-likelihood (log p (x | θ)) takes advantage of knowledge in the prior distribution with the log p(θ) term. The prior distribution creates a bias of the probability mass density towards regions that are preferred a priori 39 , which can be advantageous when trusted prior knowledge of the observed **data** is available.
θ MAP = arg max θ p (θ | x) = arg max θ log p (x | θ) + log p (θ) .(19)
Both the marginal likelihood and MAP make predictions based on a point estimate of θ which can be advantageous from a performance standpoint, but have the side-effect of focusing on a local maxima and not widely exploring the probability space. Ideally, a fully Bayesian approach would be used where instead of a point estimate, predictions would be made with a full distribution over θ, where, for each observed sample, either a positive probability would contribute for the next sample and any uncertainty would be accounted for in any predictions made 39 . However, it is intractable to attempt to make predictions using a full posterior distribution over θ."
207778550,6/48,1.0,0.125,"The most pressing concern faced by contemporary nuclear **data** evaluators is how to deal with inadequacy of the input **data** to be evaluated, both with respect to reliability and completeness. This unfortunate situation persists in spite of considerable effort expended by the nuclear science community over many decades to acquire and compile comprehensive databases of experimental values [55]. While this concern encompasses both theoretical and experimental information, the focus in this paper is on experimental data."
209991952,9/23,5.0,0.391,"The data-driven methods employed to estimate the reducible background are validated by comparing the event yields in **data** with the combined predictions for these backgrounds, added to Monte Carlo predictions for SM processes as described in section 5. Figure 2 shows such a comparison for a loose event preselection requiring same-sign leptons, E miss T > 50 GeV and at least three jets with p T > 40 GeV, binned in the different lepton flavour and b-tag multiplicity combinations. Simulation studies show that the sources of reducible background for such a preselection are dominated, as in the SRs, by tt processes. While the F/NP lepton background represents a major contribution to the total yields, the chargeflip background is always small. In all bins, the observed and predicted event yields agree within uncertainties. Figure 3 presents the distributions of E miss T and the number of jets in events with at least two jets and an otherwise identical preselection, for which good agreement is observed between **data** and predictions."
29602952,8/19,1.0,0.421,"We use the same assumption as in the original model that (a k ,r k ) are independent over families (k=1, . . . ,n), and we assume that , ,~Uniform(0,1) iid p π θ Then, using Bayes' theorem, the joint posterior density of (p, π, θ) is 
{ :1 1 0} { : 1} (1 ) (1 ) (1 ) ( , , | , ) {1 (1 ) } (1 ){1 (1 ) } { (1 ) } (1 ) , 0 , , 1. {1 (1 ) } (1 ){1 (1 ) } k k k k k k k k k k k k k k k k k k rθ π π π θ θ π θ π θ θ π π π θ θ π θ π − − ≤ ≤ − > − = > − − − ∝ Π − − + − − − + − − × Π < < − − + − − −(7)
For the ignorable selection model, the joint posterior density is 
π θ π π θ π π θ θ π π π θ − − = − − ≤ ≤ − > − = > ∝ Π − − × Π − − − × Π + − − < <(8)
Note that in (8) there is no term with a k =r k =0 because they are simply not in the **data** of ascertained families."
207778550,5/48,3.0,0.104,"The most commonly employed nuclear **data** evaluation approach in use today is the generalized least-square method, e.g., see [25][26][27][28][29]. One manifestation of the mathematical formulation of this method is summarized briefly in Sect. II of this paper, so this will not be repeated in this section. A variant of this approach to fitting smooth curves to **data** that is based on Padé polynomials [31][32][33] has also been employed, especially in Russia, e.g., see [34][35][36]. This method is described further in Sect. V C 2."
119479526,5/29,2.0,0.172,"In the previous example, we assume a Normal distribution for the Likelihood. While this is a reasonable assumption when each datum represents the mean of multiple **data** points, it does not necessarily hold in all cases. The use of a distribution with higher probability density in the tails, such as Student's-t distribution have a better chance of robustly accommodating outliers. The probability density function of Student's-t distribution is given by
f (t) = Γ ν+1 2 √ νπΓ ν 2 1 + t 2 ν − ν+1 2 ,(4)
where ν is the degrees of freedom and Γ is the gamma function. Gelman et al. suggest that it is appropriate to either select a reasonable value for the degrees of freedom, ν, or to include it as a parameter in the Bayesian analysis if finding the specific form of the error's distribution is important [19]. They also caution against employing ν <= 2, as this ν results in a distribution with infinite variance that is not realistic in the far tails. One significant advantage of the Bayesian framework is that modifying the choice for the Likelihood is, in this case, trivial to implement and does not add to the computational overhead when evaluating the Posterior using numerical samplers. We show an example of this in Appendix B."
17895596,4/11,1.0,0.364,"SKA will provide a deep and wide large-scale structure dataset that will enable separating the effects of the early and late universe on the observed CMB anisotropy. For example, the SKA **data** could be used to reconstruct the late-time contribution to the CMB anisotropy via the integrated Sachs-Wolfe effect, and thus provide information about the temporal evolution of the CMB anomalies."
29602952,8/19,2.0,0.421,"To make posterior inference about (p, π, θ), we use a grid method in three dimensions in a manner similar to the one discussed earlier for (p, π). With 100 intervals in each variable, we have to evaluate the joint posterior density at 106 values of (p, π, θ), not too time-consuming though. It is unnecessarily complex to run a Gibbs sampler here. Because each of p, π and θ lives in (0, 1), the grid procedure is still attractive. Note that for the ignorable selection model, a posteriori p and θ are jointly independent of π. In fact, Thus, we use a grid to draw (p, π), and we draw π independently. In either case, we have used 10, 000 iterations, perhaps too many! In Table 2 we have compared the ignorable and the nonignorable selection models for Crow's **data** when inference is made for p, π and θ. The correlation is almost zero under both the ignorable and the nonignorable selection models, but the difference between these models for inference about p and π is enormous with much larger estimates from the ignorable selection model. Under the nonignorable selection model, the posterior mean, posterior standard deviation and 95% credible interval for p are .257, .033, (.190, .320). This small correlation seems to have some effect: the posterior mean, posterior standard deviations, the 95% credible interval without the familial correlation are .271, .035 and (.206, .340)."
235829209,25/47,2.0,0.532,"Therefore, an intuitive method for detecting chaos directly, purely from **data** is possible. In other words, predictability itself can act as a chaos indicator. From our studies we observed that by using the predictability of less-complex surrogate models, and a small volume of training data, some nonlinear behaviors in a dynamical system can be well characterized."
233705419,27/53,1.0,0.509,"With the EDI hive IoT framework, an efficient test design and evaluation of the **data** guaranteed that all influencing parameters and the interactions between them were presented and evaluated. In this case, the AI-based hybrid model was used to examine the reduction potential of oscillating the coal mass flow to reduce NO x emissions. The campaign consisted of a screening trial, where the **data** were used to identify relevant influencing parameters. Based on these results, a second trial was initiated to further investigate the optimal test settings and the reproducibility of the test setup. The formalized expert knowledge, measurement data, and the decisions for all iterations were captured sustainably through the AI-based hybrid model procedure, which is supported by the EDI hive. With the statistical planning of the trial and the use of statistical analysis algorithms, it was possible to identify significant influences, such as the best oscillation frequency. In this case, it was found to be 1.8 Hz for coal oscillation to reduce the NO x concentration to 380 mg/Nm 3 ."
119247546,37/46,2.0,0.804,"Using the equation for and the initial **data** (0) = 1 one sees that (0) > 0 so that has a local minimum atτ = 0. Thus, at least for positive values ofτ close to 0 one has that must be increasing. Furthermore, asχ > 0 for r > r , one finds that β 2 ≤ . This last differential inequality can be integrated to yield ≥ cosh(βτ ). One concludes that is increasing for all τ ≥ 0. A similar argument with the function η ≡ − ς satisfying the equation
η − (β 2 +χ)η = 1, η(0) = 0, η (0) = 0,
shows that ≥ ς for allτ ≥ 0."
55381955,6/12,2.0,0.5,"Since it is hard to imagine a way to make ARGO and KASCADE **data** compatible with each other, we decided to take ARGO **data** at face value and tried to infer the consequences that would follow. This is illustrated in Fig. 6, where we plot the results of our calculations and compare them with the ARGO data, shown as a shadowed area. The relatively large area derives from the fact that a few different analysis techniques have been adopted so as to establish the independence on the Monte Carlo procedure adopted of the knee-like feature in the light component."
208006088,18/26,1.0,0.692,"We use a total of np = 200 × 100 bins in the model, where each bin is 5 m × 5 m. The likelihood for this experiment is given by
p(Y | f ) ≈ n i=1 p j=1
Poisson(Y ij | exp(f (r ij ))) where r ij is the coordinate of the ij th bin, Y ij is the number of **data** points in the ij th bin, and Y is the n × p matrix of counts. We perform 10 5 iterations of Gibbs sampling, alternating between Elliptical  Slice Sampling for the Gaussian process given its hyperparameters, and Metropolis Hastings with proposal distribution N (θ, 0.05 2 ) for the log of the hyperparameters given the latent GP-distributed function. The kernel is a product of two Matérn-5/2 kernels, one over the vertical dimension and the other over the horizontal one, each with their own length scale. A single process variance is utilised, and a nugget term is added. The log of each of the four hyperparameters was given a N (0, 1) prior. Figure 8 shows the log joint of the entire state after each iteration, while Fig. 9 shows the progress of each hyperparameter per iteration."
24865762,1/34,3.0,0.029,"The approach adopted in this article for the **data** representation of time-evolving datasets relies on the spectral analysis of integral operators L for which the kernels are built from periodic semigroups applied to the data's correlations. The resulting class of operators constitute one the two main contributions of this article, and a substantial portion of the latter is devoted to the analysis of their spectral properties; see Sectns. II, III, IV, and V. The other main contribution is concerned with the aforementioned class of SDEs, aimed at the modeling of the reduction coordinates obtained as projection of the original dataset onto the eigenfunctions of L. These SDEs fall into the class of networks of linearly coupled Stuart-Landau oscillators [32], that may be embedded within the class of multilayer stochastic models (MSMs) [30], allowing thus for the inclusion of memory effects in their formulation; see Sect. VII."
17837291,18/52,1.0,0.346,"Remark 5.5. In our applications later, the situation is better then the conditions (i)-(ii) demand. We will have two Euler **data** A, B such that A d , B d separately, rather than just A d − B d , will satisfy both conditions (i)-(ii) at the outset. In this situation, to prove that A = B, it suffices to prove that they are linked."
56062061,40/53,1.0,0.755,"Figure 15 .
15The 14 C level of the atmosphere and the babies. Within the last 50 years accurate dating is feasible by tracing the 14 C level of human tissues back to its atmospheric origin (solid curve). The dashed lines indicate how the measured 14 C levels are interpolated horizontally from the vertical axis onto the plot of the atmospheric 14 C concentration and subsequently down onto the horizontal axis. In this manner the measured 14 C raw **data** are transformed into calendar dates. The graphs display the 14 C concentration in the form of 'fraction modern carbon' , F( 14 C),(vertical axes), as a function of the time (in years, horizontal axes); the atmospheric **data** of Kueppers et al.[2004]  has been extended by an exponential fit (rightmost dotted curve). The average 14 C-levels and corresponding dates ('combined value') of the three babies discussed in the text are shown as the middle crosses in the three bottom graphs. In addition, the separate values of nails (lower crosses) and lenses (upper crosses) are displayed in the three bottom graphs. The vertical and horizontal lines indicate 68.2% probability intervals (1 s) of the 14 C raw **data** and 95.4% probability intervals of the calendar dates, respectively. The substantial variation of the atmospheric 14 C concentration is due to tests of nuclear weapons around the middle of the previous century and subsequent absorption of the excess 14 C into the oceans.
"
119313773,19/21,1.0,0.905,"Figure 12 :
12Breakdown parameterq (solid line) as a function of the **data** size N for the logistic map with fixed range of perturbations a = 3.8(1 + ε) with dε = 10 −6 and Gaussian observable with width w = w ⋆ and location xs = x ⋆ s tuned to capture the displacement of the n = 11th spike. The error bars show the two-sided 90% prediction interval for q as estimated from K = 200 realizations differing in the initial conditions. The dashed line shows qα for α = 0.05.
"
232380196,23/28,1.0,0.821,"Implementation details for long-tailed CIFAR For both long-tailed CIFAR-10 and CIFAR-100, we use ResNet-32 [10] as backbone network to extract image representation. Our hybrid network has two branches, which have independent input **data** as shown in Fig. 2. The basic set of **data** augmentation shared by both branches include random cropping with size 32 × 32, horizontal flip and random grayscale with probability of 0.2. Following SC loss, we also derive different views of an image by using different **data** augmentations in PSC loss. In our experiments, we simply use with and without color jitter as two different augmentation views. We use batch size of 512 for both SC and PSC based hybrid networks. The classifier learning branch uses class-wise balanced **data** sampling. We use SGD with a momentum of 0.9 and weight decay of 1×10 −4 as optimizer to train the hybrid networks. The networks are trained for 200 epochs with the learning rate being decayed by a factor of 10 at the 120 th epoch and 160 th epoch. The initial learning rate is 0.5. For the curriculum coefficient α, we use a parabolic decay w.r.t the epoch number [37], i.e., α = 1 − (T /T max ) 2 , where T denotes the current epoch number and T max indicates the maximum epoch number. For SC based hybrid network, the temperature τ in Eq. (3) is fixed to be 0.1. For PSC base hybrid network, τ is set to be 1 for CIFAR-10 and 0.1 for CIFAR-100."
251719245,6/21,1.0,0.286,"The standard approach for partitioning a **data** set is through clustering. The works of Ruan et al. [11] have proven that for some **data** sets, community detection in graphs provides more accurate partitions. In line with this, we use 4 different community detection algorithms to predict the grouping of genes based on the 5 functional groups. The first three algorithms use the concept of modularity while the last algorithm uses edge-betweenness. We'll discuss these two network metrics in the succeeding subsections. We subjected the largest component of each graph obtained by using value-based and rank-based construction with varying input parameters. Here, we will compare how the different community detection algorithms perform in predicting the 5 functional groups."
17671315,20/32,1.0,0.625,"Step 1. Select any target X listed in Table 1. We call X ""calibration target"". Let F X (t) be the **data** for the target X, which are pre-processed as in subsection 2.3.1, see Fig. 5."
246863515,1/44,5.0,0.023,"• We evaluate the performance of our proposed estimators based on large-scale experimental **data** for a job-training program with long-term employment observations. We combine part of the experimental **data** and some semi-synthetic observational **data** with realistic persistent confounding. We demonstrate that our proposed estimators can substantially outperform benchmarks that fail to handle persistent confounding, including the state-of-the-art estimator proposed by Athey et al. [2020]."
207778550,10/48,1.0,0.208,It is important that a clear definition of USU be provided as the basis for further discussions in this paper: 1 Eventually all **data** are combined in the GLS fit.
207847805,3/7,1.0,0.429,"Regarding dust properties, we rely on mid-infrared (MIR) scattering to constrain dust geometries (shape and sizes). In particular, to reproduce MIR observations of L183 we demonstrate that an important fraction of large aggregates with an equivalent size of 4 µm [14] is requested, in particular to explain the scattering contribution remaining at 8 µm [6, and Fig. 1 right]. Though the grain model we proposed has the suitable geometry to explain MIR observations, its emissivity is too high in the FIR/mm (see Fig. 2). While the dust size and porosity are well constrained by MIR scattering (called coreshine), the dust emissivity strongly varies depending on their composition. Recent laboratory measurements show the impact of the iron fraction inside silicates on dust emissivity [15,16]. Here, the silicates included in the dust mixture are pure enstatite (MgSiO3, 75%) mixed with iron sulfides (FeS, 10%) and carbonaceous (15%). We developed a code to test the impact of dust composition, in particular silicates and ice mantles, on dust emissivities. SIGMA is an open access code 2 to compute the dust properties of icy aggregates from NIR to mm wavelengths based on laboratory measurements (see Sect. 2.2). While we have already found a reasonable solution to model L183 in the NIR and MIR [6], the NIKA2 **data** will help to derive the best grain composition thanks to SIGMA and 3D radiative transfer modeling. Figure 2. Herschel observations on left column at 250 µm, 350 µm, and 500 µm and corresponding modeled maps on right side using the dust model and density profile built from MIR scattering [6]. Color scale is in MJy/sr, box size is 40 . The 500 µm modeled map shows a clear excess of emission towards the PSC suggesting that the dust population suitable to explain scattering observations is too emissive."
119383949,1/4,2.0,0.25,"Since the orbit bifurcations are of leading order inh, we now want to check if they have an increased influence on the amplitude of the conductance oscillations. In Fig. 3(a) we show the quantity Tr M of four typical periodic orbits (shown to the right) taking part in two successive bifurcations (where Tr M = 2) under variation of the magnetic field strength B. The left one is a tangent bifurcation, the right one a pitchfork bifurcation. In Fig. 3(b), the contribution of these orbits to the conductance is plotted. The dotted line gives the result of the trace formula Eq. (1). The amplitudes are diverging (arrows!) at the bifurcations. The uniform approximation (solid line) removes the divergences. Fig. 3(c) represents the corresponding **data** for a system scaled to have 10 times larger actions, thus being closer to the semiclassical limit. It is important now to note that the amplitudes in the uniform approximation are nearly constant over the bifurcations. We have thus shown that the bifurcations have no locally dominant influence on the conductance of the present system [23]. Having established this result, we can further simplify our semiclassical treatment. Whereas for individual orbits a uniform treatment of the bifurcations is vital, their influence becomes smaller if a larger number of orbits is included. This is demonstrated in Fig. 4, where the total δG xx has been calculated for s d = 1.86 including all relevant (∼ 60) periodic orbits. The solid line shows the result of the uniform approximation, whereas the dotted line corresponds to the standard Gutzwiller approach. To remove the spurious divergences in Eq. (1) (and those due to bifurcations of higher codimension in the uniform approach), we have additionally convoluted δG xx over the magnetic field B (cf. Ref. [24]). The results are very similar [25]. In particular, the maximum positions are practically identical. In the following, we therefore use simply Eq. (1) with an additional convolution over B."
55185767,7/16,1.0,0.438,"Let us now treat the case that Y is VG, using terminology and results from Section 5. Let us first suppose that the definable functions which take values in VG and which appear in the build-up of f (namely in the forms of generators (2) and (3) of Section 3.1) are linear over X, that W is the definable set
(4.1.2) {(x, y) ∈ X × VG | α(x) ≤ y β(x)}
where is < or no condition and where α, β : X → VG are definable functions, and that all other build-up **data** of f (namely, generators (4) of Section 3.1 and h and e as in (3.2.1)) factor through the projection W → X. Then the conclusion follows from Lemma 4.1.2. Indeed, for any K in Loc 0 , any x ∈ X K and any ψ in D K , f K,ψ (x, y) is a finite sum of terms T i of the form
(4.1.3) c i,K,ψ (x)y a i q b i y K
for integers a i ≥ 0, rational numbers b i and c i in C exp (X). The integrability of f K,ψ (x, y) over y in W K,x is automatic when is < and we get g from Lemma 4.1.2. When is no condition, we regroup the terms if necessary, so that the pairs (a i , b i ) are mutually different for different i. By observing different asymptotic behavior of these terms for growing y, we may consider the subsum i∈J T i with i ∈ J if b i < 0. This time we apply Lemma 4.1.2 to this sub-sum to find g."
233705419,18/53,2.0,0.34,"The overview of the model shown in Figure 6 illustrates the **data** set used, the target, and influencing parameters, as well as the quality of the model. "
238834501,1/19,2.0,0.053,"Evaluation of the regression methods with respect to 2m air temperatures. Our methods are generalisations of two types of estimates found in the literature: (1) averages over a number of 'similar' days, and (2) smoothed observations from the day of the eclipse with interpolated values for t • 1,2 . Approach (1) can formally be seen as a regression with selected days d ∈ d • serving as predictors and equal regression weights which sum to 1. Dropping the restrictions on the weights and allowing for an intercept generalises the approach, and any regression method with subset selection replaces the task of choosing 'similar' days with an objective procedure. Including an intercept can be interpreted as allowing for slowly-varying (synoptic) background conditions. Since we need to judge the similarity of days without considering t • , this regression is trained on t • even though we are interested in t • , for which the model is simply evaluated. Approach (2) can equally be cast as a regression, with t ∈ t • as targets and t ∈ t • as predictors, trained on examples d ∈ d • . This procedure is equivalent to a smoothing along the time axis with the kernel's weights fit by regression instead of having been determined a priori. We will call the two approaches 'average' and 'smoothing' type regressions, respectively. In both cases we can make use of the additional information we have from years of monitoring **data** at the stations at which we observe the eclipse; we use approximately 4 years of **data** preceding the eclipse, since for this time period, most CEAZAMet stations in the eclipse's umbra have been collecting **data** at a 5 min sampling rate."
235829209,29/47,1.0,0.617,"The NSLS-II [17] is a dedicated 3 rd generation medium energy (3 GeV) light source operated by Brookhaven National Laboratory. Its main storage ring lattice is a traditional DBA structure as illustrated in Fig. 8. After the linear chromaticity is corrected by chromatic sextupoles, the available ""tuning knobs"" used for DA optimization are six families of harmonic sextupoles. Twiss parameters [20]. The one-turn transportation can be accomplished with a symplectic particle tracking code, such as elegant [21]. The coordinates at the exit are the output − → X 1 . The volume ratio of the training and testing **data** is 90%:10%. The python packages scikit-learn [22] and keras [23] have The DAs of the total population in the 100 th generation were calculated."
253553223,1/28,2.0,0.036,"The remainder of this paper is structured as follows. Section 2 gives the problem setting and fixes some basic notation for the rest of the paper. Section 3 formally introduces the precomposition operator, investigates its mathematical properties and its establishes its connection to the infinite-dimensional regression problem. In Section 4, we lay the groundwork for a detailed statistical analysis of regression with Hilbert-Schmidt operators by combining spectral regularisation techniques with concentration bounds for sub-exponential Hilbertian random variables. In particular, we derive basic rates under the assumption of Hölder-type source conditions. Section 5 discusses the consequences of our results for specific statistical fields and connects them to the relevant literature. Section 6 recapitulates the key points of our work and their implications in the abstract context of learning infinite-dimensional information from **data** by comparing the infinite-dimensional response setting to standard regression with real-valued responses."
238834501,12/19,2.0,0.632,"We therefore chose to focus on the problem of how to separate the effect of the eclipse from other circumstantial influences on the observed meteorological variables. The reference estimate of how a meteorological variable would have behaved on the same day but in absence of the eclipse is speculative, and we cannot confidently use its point value without some idea of how uncertain it is. While some previous studies give confidence intervals on certain specific estimates 4,5 , these are not related to the uncertainties surrounding the actual effect sizes at specific times and locations. Here, we calculate uncertainties and their bias (for the lasso estimates) empirically on the basis of longer-term records from each of the stations at which we observed the eclipse, by performing our estimation procedure for any non-eclipse day and calculating the errors with respect to the true evolution of that day. The error distribution thus pertains to any arbitrary day of the year and includes uncertainties arising from annual-scale variability; in areas with a more pronounced annual cycle than the Coquimbo region, we would expect the estimates to exhibit larger uncertainties, which in turn could be counteracted by restricting the training **data** set to days closer to the day to be predicted (in terms of the day of year, DOY) if longer-term records are available."
2423505,8/23,1.0,0.348,"Covariance shift [Shimodaira, 2000], is another relaxation of DA. Here, given an observation, the class distributions are same in the source and target domains, but the marginal **data** distributions are different. P t (Y |X) = P s (Y |X), but P t (X) = P s (X). This situation arise, for example, in active learning, where the P s (X) tend to be biased to lie near the margin of the classifier. At a first glance, this situation appears not to present a problem, since P t (Y |X) = P s (Y |X), which we can estimate from the data. Here is why it becomes a problem in practice. Assuming, first of all, that the model family we use is mismatched to the data, i.e. regardless of what parameter we choose the model won't fit the underlying distribution. Under this assumption, covariate shift becomes a problem for the following reason. The optimal fit of the source **data** will be such that it minimize model error in the dense area of P s (X) (because these areas will dominate the error). Now, since P t (X) is different from P s (X), the learned model will not be optimal for the target **data** (again, since the model family is mismatched)."
207778550,20/48,1.0,0.417,"In the following discussion, it is assumed that the evaluation methods employed are based on the least-squares approach, e.g., as embodied in code GMAP [49]. The estimated USU contributions, when significant, need to be introduced as components of augmented covariance matrices associated with all the input experimental data. It has been shown by Capote and Neudecker [78] that when more than a one-dimensional quantity is to be evaluated (e.g., energy-dependent cross sections), the evaluated mean values, as well as the derived covariances, will be altered by the use of augmented covariance matrices that include the USU contributions. This is intuitively evident because a change in the inclusive **data** covariance matrix changes the effective weighings of the input **data** points and this will alter the evaluated mean values. Therefore, a-priori determined USU contribution (e.g., as done for a one-dimensional quantity 252 Cf(sf) ν tot , see Sect. V A below) cannot be added a-posteriori to evaluated uncertainties while keeping the originally evaluated mean values. Instead, the evaluations need to be repeated including these augmented covariance matrices that contain an USU component in the experimental covariance matrix."
209947757,5/18,1.0,0.278,"The identification of modal parameters is accomplished by using the PolyMAX frequency domain method (cf. [15]) which is implemented in the LMS Test.Lab environment. In case of shaker excitation, time **data** signal processing is done with DLR in-house tools that interact with LMS software. From the amount of all measurement runs, each mode shape can be identified several times. They are stored together with the other modal parameters and related metadata into an SQL database. Accessing the database, modes with identical properties coming from different runs are grouped in socalled mode families. For each family, a single mode is selected as the master mode according to various quality measures. Besides the Mode Indicator Function (MIF), considered quality criteria are scalars evaluating phase purity like Modal Phase Collinearity (MPC) and Mean Phase Deviation (MPD), the level of excitation in terms of participation factors, and the generalised force of excitation (cf. [16]). The final modal model represents the collection of all selected master modes."
119313773,1/21,2.0,0.048,"The paper is organized as follows. In Section 2 we briefly review linear response theory and the fluctuation-dissipation theorem. In Section 3 we propose a goodness-of-fit test to probe for the validity of linear response in time series. In Section 4 we discuss the logistic map, demonstrate the mechanism leading to the breakdown of linear response for this one-dimensional map and show how this breakdown might not be apparent with time series of insufficient length. We show the effect of finite **data** size as well as how the choice of the observable can either mask or emphasize the non-smoothness of the invariant measure. In Section 6 we show further that an application of the FDT in situations where linear response does not exist cannot provide any reliable statistical information, not even in an averaged sense. We conclude with a summary in Section 7."
56062061,34/53,1.0,0.642,"Figure 6 .
6Plot of counts of secondary osteons (medians) per area (3.5 mm 2 ) versus age at death. One **data** point is designated as an outlier (filled circle). The smoother (linear) was calculated without the outlier.
"
246863515,13/44,2.0,0.295,"We construct the observational dataset using a biased subsampling procedure. We randomly subsample units from the Riverside **data** according to a sampling probability function π(A, U ) ∈ (0, 1), where A ∈ {0, 1} is the treatment assignment and U ∈ {0, 1, 2, 3} is the highest education level (""0"" means below 9-th grade, ""1"" means 9-th to 11-th grade, ""2"" means 12-th grade, and ""3"" means above 12-th grade). This creates dependence between the treatment assignment and the education level for the units subsampled into D O . We choose education because it is quite likely to have persistent effects on participants' potential employment in all quarters following the treatment. Then we remove the education level U from D O (and also D E ). As a result, the education level becomes a plausible persistent unmeasured confounder in D O ."
53380409,28/119,1.0,0.235,"The **data** sample was collected by the ATLAS detector during the pp collision running of the LHC at ffiffi ffi s p ¼ 13 TeV in 2015 and 2016. Events were selected for the different channels with various triggers, as described in their respective papers [9-18]. Channels featuring charged or neutral leptons were selected with single or multiple electron and muon triggers with various p T thresholds and isolation requirements or with missing transverse momentum triggers with varying thresholds. A high-p T jet trigger was used in the fully hadronic channels. After requiring that the **data** were collected during stable beam conditions and with a functional detector, the integrated luminosity amounts to 36.1 fb −1 ."
15294885,4/25,1.0,0.16,"In (1), it can be easily shown that the desired signal component
X(t) ∞ k=−∞ b[k]p(t − kT ) becomes
a zero-mean SOCS random process due to the second-order property of the zero-mean SOS **data** sequence {b[l]} l∈Z . In other words, the mean, the auto-covariance, and the complementary auto-covariance functions of X(t) satisfy, respectively, µ X (t) E{X(t)} = 0, r X (t, s) E{X(t)X(s) * } = r X (t + T, s + T ), and
r X (t, s) E{X(t)X(s)} =r X (t+T, s+T ), ∀t, ∀s.
In what follows, we also call r X (t, s) andr X (t, s) the auto-correlation and the complementary auto-correlation functions, respectively, because X(t) has mean zero."
14392421,13/15,1.0,0.867,"S. 1
1Spectral mapping: For 2 ≤ c ≤ C, Find the top c generalized eigenvectors E c = [e 1 , e 2 , · · · , e c ] of the eigensystem Ax = λDx, where A and D are the adjacent and the degree matrix respectively. S.2 Evidential c-means: For each value of c (2 ≤ c ≤ C), let E c = [e 2 , · · · , e c ]. Use ECM to partition the n samples (each row of E c is a sample **data** on the c − 1 dimensional Euclidean space) into c classes. And we can get a credal partition M for the graph. S.3 Choosing the number of communities:
"
253553223,15/28,1.0,0.536,"Corollary 4.14 (Convergence rates in probability). Suppose the regularisation strategy g α has qualification q ν + s. Suppose that Y ∈ L ψ 2 (P; Y), X ∈ L ψ 2 (P; X ), θ ⋆ ∈ Ω(ν, R), and 0 < α < 1. Let δ ∈ (0, 1 e ] and s ∈ [0, 1 2 ]. If the regularisation parameter α = α n is chosen to depend on the number n of **data** points via
α n := 1 √ n 1 ν+1
, then, for n n 0 := max X 4 L ψ 2 (P;X ) , 1152e 2 X 4 L ψ 2 (P;X ) log(1/δ)
1 ν 1+ν ,
with P ⊗n -probability at least 1 − 2δ,
θ ⋆ − θ αn C s XX S 2 (X ,Y) 3κ log(1/δ) 1 √ n s+ν 1+ν
, whereκ := max{κ ν,C XXγ R, 64κ D,B eB ψ 2 } < ∞ is obtained from the constants appearing in Propositions 4.12 and 4.13."
235727288,17/46,1.0,0.37,"AIS with geometric paths is often considered the goldstandard for evaluating decoder-based generative models (Wu et al., 2017). In this section, we evaluate whether qpaths can improve marginal likelihood estimation for a vari-  First, we use AIS to evaluate the trained generative model on the true test set, with a Gaussian prior π 0 (z) = p(z) as the base distribution and true posterior π 1 (z) = p(z|x) ∝ p(x, z) as the target. Intermediate distributions then becomẽ π β (z) = p(z)p(x|z) β . We report stochastic lower bound estimates (Grosse et al., 2015) of E pdata(x) log p(x) in Fig. 6( When exact posterior samples are available, we can use a reverse AIS chain from the target density to the base to obtain a stochastic upper bound on the log marginal likelihood (Grosse et al., 2015). While such samples are not available on the real data, we can use simulated **data** drawn from the model using ancestral sampling x, z ∼ p(z)p(x|z) as the dataset, and interpret z as a posterior sample. We use the Bidirectional Monte Carlo (BDMC) gap, or difference between the stochastic lower and upper bounds obtained from forward and reverse chains on simulated data, to evaluate the quality of the AIS procedure."
207778550,10/48,4.0,0.208,"USU are experimental uncertainty sources whose estimated magnitudes must be determined based on the experience of experimenters and evaluators. It should be noted that Report JCGM 100:2008 [79] clearly indicates that what it considers to be USU are completely indeterminable (unknowable). As mentioned earlier, nuclear **data** evaluators, motivated by practical necessities, must proceed, albeit cautiously, beyond this STOP sign! This observation is fully consistent with the discussion in Sect. II that points out differences in how uncertainties are approached in traditional metrology and in nuclear **data** evaluation work."
249258070,51/119,4.0,0.429,"The breakdown of self-initiated activities across these three units were 64.2% for patrol, 17.5% for special crime units, and 18.3% for the differential response team. Unfortunately, the HPD CFS **data** do not provide detailed information about the nature of self-initiated activities, such as offense or incident type for patrol officers. Activities performed by specialized crime investigation and DRT officers might be deduced approximately using the division information. But to further breakdown the activity to specific units produces small frequencies of specific offenses, which would have been insufficient to conduct meaningful analysis. For example, there were only 338 self-initiated activities recorded under gang division, and 165 self-initiated activities recorded under robbery division during the study period. The **data** do include some information on dispositional outcomes of police self-initiated activities. The most frequent type of action for self-initiated activities is ""collecting information"" for each of the three types of police units. The disposition of ""collecting information"" is used in instances where the incident is resolved without needing to generate an offense report or referral for further investigation. Some examples are if the officer resolves the issue by mediation, the complaint is a civil issue rather than criminal, or if it is to provide further information about an offense report that already exists. Other self-initiated actions include making an offense or supplementary report, making an arrest, issuing tickets, etc."
17671315,13/32,1.0,0.406,"Theorem 1. Let the function ε * r (x) satisfying conditions (4), (5) be the exact solution of our CIP for the noiseless **data** g * (t) in (9). Fix the truncation pseudo frequency s > 1. Let the first tail function V 1,1 (x) be defined via (37)-(39). Let γ ∈ (0, 1) be the level of the error in the boundary data, i.e.
|ψ 0 (s) − ψ * 0 (s)| ≤ γ, |ψ 1 (s) − ψ * 1 (s)| ≤ γ, for s ∈ [s, s] ,
where functions ψ 0 (s) , ψ 1 (s) depend on the function g (t) in (9) via (11), (21), (23) and functions ψ * 0 (s) , ψ * 1 (s) depend on the noiseless **data** g * (t) in the same way. Let 
h < 1 D Q ,(40)
then the following convergence estimate is valid
ε (n,k) r − ε * r L 2 (0,1) ≤ h ω ,(41)
where the number ω ∈ (0, 1) is independent on n, k, h, ε (n,k) r , ε * r . Theorem 1 guarantees that if the total number Q of computed functions ε (n,k) r is fixed and error parameters σ, h are sufficiently small, then iterative solutions ε (n,k) r (x) are sufficiently close to the exact solution ε * r , and this closeness is defined by the error parameters. Estimate (41) guarantees the stability of AGCM with respect to a small error in the data. Therefore the total number of iterations Q can be considered as one of regularization parameters of our process. Two other regularization parameters are the numbers s and α. The combination of inequalities (40) and (41) has a direct analog in the inequality of Lemma 6.2 on page 156 of the book [15] for classical Landweber iterations, which are defined for a substantially different ill-posed problem. Indeed, it is stated on page 157 of the book [15] that the number of iterations can serve as a regularization parameter for an ill-posed problem."
119634392,1/9,3.0,0.111,"In [18], Kazuo obtains a unique global solution to equation (1.2) in dimension three provided the initial **data** is in the Sobolev space H m,2 (R 3 ), with m > max{5/2, 1 + 2γ 1 }, provided γ 1 and γ 2 satisfy the inequality 2γ 1 + γ 2 ≥ 5 and that
(1.3) ∞ 1 ds sg 2 1 (s)g 2 (s) = ∞.
The goal of this paper is to obtain a much wider array of existence results, specifically existence results for initial **data** with low regularity and for initial **data** outside the L 2 setting. We will also, when applicable, use the energy bound from [18] to extend these local solutions to global solutions. Our plan is to follow the general contraction-mapping based procedure outlined by Kato and Ponce in [7] for the Navier-Stokes equation, with two key modification."
238834501,11/19,2.0,0.579,"Evaluation of the regression methods with respect to 2m air temperatures. Our methods are generalisations of two types of estimates found in the literature: (1) averages over a number of 'similar' days, and (2) smoothed observations from the day of the eclipse with interpolated values for t • 1,2 . Approach (1) can formally be seen as a regression with selected days d ∈ d • serving as predictors and equal regression weights which sum to 1. Dropping the restrictions on the weights and allowing for an intercept generalises the approach, and any regression method with subset selection replaces the task of choosing 'similar' days with an objective procedure. Including an intercept can be interpreted as allowing for slowly-varying (synoptic) background conditions. Since we need to judge the similarity of days without considering t • , this regression is trained on t • even though we are interested in t • , for which the model is simply evaluated. Approach (2) can equally be cast as a regression, with t ∈ t • as targets and t ∈ t • as predictors, trained on examples d ∈ d • . This procedure is equivalent to a smoothing along the time axis with the kernel's weights fit by regression instead of having been determined a priori. We will call the two approaches 'average' and 'smoothing' type regressions, respectively. In both cases we can make use of the additional information we have from years of monitoring **data** at the stations at which we observe the eclipse; we use approximately 4 years of **data** preceding the eclipse, since for this time period, most CEAZAMet stations in the eclipse's umbra have been collecting **data** at a 5 min sampling rate."
119080499,14/30,1.0,0.467,"The localized nature of the vacancy state in LaH x is consistent with the temperature dependence of the d.c. resistivity **data** at room temperatures, which has a temperature depedence consistent with variable range hopping [11]. Localized states are a prerequisite for variable range hopping. In the next section we will discuss the electronic band width of the vacancy states."
236766189,1/65,1.0,0.015,"The lack of adequate seismic damage **data** from past earthquakes inhibits the sufficiency of the implementation of a pure empirical approach for the assessment of the seismic vulnerability of RC buildings. Hence, a wide range of analytical methodologies have been proposed during the last decades for the determination of fragility curves towards the assessment of the seismic vulnerability of existing RC buildings. Singhal and Kiremidjian (1996) performed nonlinear dynamic analyses of RC frame buildings subjected to artificial ground motion records to propose analytical fragility curves based on the Park-Ang global damage indices (1985). The application of a similar methodology was presented by Masi (2003) for the determination of fragility curves of post-1970 RC buildings designed only for vertical loads, including two additional RC building configurations: RC buildings with masonry infills and Pilotis frames. Zeris et al. (2002Zeris et al. ( , 2006 highlighted the importance of conducting nonlinear dynamic analysis for an accurate estimation of the local inelastic deformation demand of RC frame buildings, designed before the adoption of the modern seismic code provisions. The use of a predetermined, finite number of natural or artificial ground motion records for the derivation of the aforementioned fragility curves leads to a discrete representation of the seismic vulnerability of RC buildings. Nevertheless, the increase of the accuracy of this representation necessitates the use of a large set of ground motion records for the conduction of nonlinear dynamic analysis, which can lead to high computational cost."
207778550,3/48,3.0,0.062,"Before undertaking a nuclear **data** evaluation, it is of utmost importance that values, uncertainties and correlations are adjusted and corrected where possible and warranted. The identification of outliers is an essential part of such a procedure. For a single quantity, outliers are usually defined by considering a standard score which is a relative difference between y i andμ. The ratio r i = (y i −μ)/ (u 2 i + var(μ)), see for example Ref. [18] and similar definitions [19][20][21], is easy to use. A score beyond this definition for an outlier must be documented if values are discarded. Only when **data** are still inconsistent after rejecting such outlier **data** points does it make sense to search for hidden or unrecognized uncertainties."
201251303,1/10,2.0,0.1,"Probit models with spatial dependencies were first studied by McMillen (1992), and he proposed an EM algorithm to produce consistent maximum likelihood estimates for the model. In SAR probit model, the spatial dependence structure adds complexity to the estimation of parameters. The main assumption of the model is that the distribution of errors is known and is often assumed to be normal. Parameter estimation using a full maximum likelihood method is problematic because the likelihood function involves n integrals, where n is the sample size. To avoid the direct calculation of ndimensional integration, several estimators have been proposed that can produce consistent estimates when **data** are spatially autocorrelated and heteroscedastic (e.g., Beron Pinkse and Slade (1998) proposed estimators for the parameter of SAR probit model who becomes infeasible for large samples because they require the inversion of × matrices. LeSage (2000) used Bayesian estimates through the Markov Chain Monte Carlo and Gibbs sampling, which sampled sequentially from the complete conditional distribution for all parameters. Klier and McMillen (2008) have proposed a linearized version of the GMM estimator that avoids the infeasible problem of inverting × matrices when employing large samples and show that standard GMM reduces to a nonlinear two-stage least squares problem."
6811986,15/20,1.0,0.75,"Corollary 29. If C is restricted to range over sets of linear TGDs, then NQI(Q, C, S, V) has **data** complexity in PTime and combined complexity in ExpTime."
246165803,23/45,1.0,0.511,"Both algorithms were trained, with highly acceptable results. Both algorithms had over 90% accuracy during training, with the KNN having 95.55% accuracy and the ANN scoring 96.79% over the training data. Figures 10 and 11 show the confusion matrices generated with the test data. While the KNN algorithm did have minor errors in classifying non-anomalous **data** and curves, the ANN had trouble classifying only curves. Thus, the number 0 represents healthy data; number 1 represents possible potholes, number 2 speed bumps, and number 3 harsh curves.  Lastly, both algorithms were given 85% of the whole dataset to train and 15% to test. These tests consisted of accuracy for training, and F1-Score for test data, as well as a confusion Matrix, to determine the specific classifications in which they did not perform well enough."
207778550,27/48,6.0,0.562,"The most significant quantity in all these criticality experiments is the mass of the assembled fissile material in the benchmark assembly when it is close to the criticality point (critical mass). Measured physical parameters (other than nuclear data) that influence determination of the critical mass are the geometry of the fissile material, its isotopic composition and density, the geometry and material composition of the support structure, and characteristics of the surrounding environment. The neutron leakage from cylinders with various geometries is rather sensitive to the energy-angular distribution of neutrons scattered in the benchmark fissile material and less sensitive for geometrically similar spheres. Uncertainties of the physical parameters (e.g., dimensions) for manufactured cylinders of fissile material are much smaller than those for comparable spheres. Criticality measurements are very sensitive to fissile material geometries. Errors in determining benchmark physical parameters, underestimation of their uncertainties, failure to apply required corrections, and deficiencies or simplifications in the models used in simulations can affect the ∆k eff comparisons significantly. The excessive impact of these benchmarks on **data** validation and/or **data** adjustment could be mitigated by considering USU."
207778550,7/48,1.0,0.146,"A specific concern that emerged a while ago (for Neutron Standards this issue was raised at CSWEG in 1991) relates to the topic of unrealistically low evaluated uncertainties. Strong reduction of uncertainties was observed for R-matrix model fits, especially for cross-section **data** fitted with small numbers of parameters (uncertainties as small as 0.02% were observed for 6 Li(n,t) in the 1/v energy range). Expert assessment based on assumed normal probability population statistics (the so called ""2/3 rule"") was used to increase uncertainties. This issue was addressed explicitly in papers by Gai [66,67] and by Badikov and Gai [68]. Of particular concern is the impact of this phenomenon on the standards evaluations [50][51][52][53]. Pronyaev [69] has explored the matter of too-low uncertainties that appears to arise when model fitting is used in evaluating **data** rather than basing evaluations solely on experimental data. The concern for too-low uncertainties has also been raised in several papers presented at four workshops during the past decade that were devoted to nuclear **data** covariances [70][71][72][73], as well as being discussed in numerous additional reports, theses, conference proceedings, and published papers. While it remains an open area of research, an emerging consensus view in the nuclear **data** community is that this effect is primarily related to deficiencies in the input **data** rather than to evaluation methods, provided that evaluators incorporate the available capabilities provided by these methods properly. This phenomenon can be attributed to under-estimation of experimental **data** uncertainties (for various reasons) and, in many instances, also to inadequate consideration of uncertainty **data** correlations [74,75]."
246863515,3/44,3.0,0.068,"Assumption 3 (Data Combination). For any a ∈ {0, 1},
(S (a) , U, X) ⊥ G,(5)
and almost surely,
p (U, X | A = a, G = E) p (U, X | A = a, G = O) < ∞.(6)
Equation (5) in Assumption 3 means that the experimental **data** has external validity, in that the distribution of (S (a) , U, X) in the experimental **data** is the same as that in the observational **data** (i.e., the population of interest). Similar assumptions also appear in previous literature that attempt to combine different samples [e.g., Athey et al., 2020, 2019, Kallus and Mao, 2020. Equation (6) means that the conditional distributions of (U, X) | A on the experimental and observational **data** have enough overlap, which is also a common assumption in missing **data** literature [Tsiatis, 2007]. Assumption 3 ensures that the two samples have enough commonality, so it is meaningful to combine them."
117119864,51/83,2.0,0.614,"
Fermi surface map for T = 120K >> TK = 20K of CeRu2Si2 compared to ARPES map of LaRu2Si2, showing same size of large hole surface contour for both compounds. Absence of reduced hole surface size due to Ce 4f electron inclusion, as predicted by LDA and observed at very low T in dHvA experiments, is evidence of Ce 4f electron exclusion from Fermi surface at T >> TK, as proposed in Refs.[77,78].[from Ref. 18]. 5. Summary of below (102 eV) and above (108 eV and 112 eV) resonance ARPES **data** in panels (a)-(c) for heavy Fermion material URu2Si2, showing confinement of 5f weight (panel (b)) to interior of Ru d-band hole pocket (panel (c)), as in simple 2-band ansatz for the Anderson lattice model where a Kondo-renormalized f-state at εf′ hybridizes to the d-band, as shown in panels (d) and (e). [from Ref. 18] 6. Bulk sensitive PES spectra taken at photon energy 500 eV, showing V 3d valence band in all three phases of MI transition material (V0.982Cr0.018)2O3. Paramagnetic metal (PM) phase weight near EF could be quasi-particle weight transferred from Hubbard peaks of the antiferromagnetic insulating (AFI) or paramagnetic insulating (PI) phases, as in the dynamic mean field theory [91, 106] of the MI transition. [unpublished **data** from collaboration of Ref. 102]. 7. (a) schematic band structure of quasi-1-d metal Li0.9Mo6O17; (b) ARPES spectra showing all four bands A -D; (c) overlaid plot of ARPES spectra for comparison of band C lineshape to spectral theory [Ref. 126] of Tomonaga-Luttinger model in panel (d), showing lack of Fermi liquid quasi-particle due to Luttinger-liquid-like electron fractionalization, presently the only such ARPES example. [from Refs. 135 and 150].
"
207778550,21/48,8.0,0.438,"From the **data** in Table III we estimateσ 2 δ ≈ 0.0035 2 (0.6%) andσ 2 δ ≈ 0.0069 2 (1.2%) for the USU contribution at 9 MeV and 10 MeV, respectively. These values are only in the first approximation consistent with those derived from a Maximum Likelihood Estimate (MLE) approach, being the agreement better for the 9 MeV group. Note that both the MLE approach, e.g., see Eq. (31) in Appendix VI B and Eqs. (19) and (20) are founded on the basic assumption that the USU contribution for each **data** set is sampled independently from a distribution with the same variance."
207778550,14/48,3.0,0.292,"It may be surmised from the preceding discussion that uncovering the existence of USU, and developing procedures by which these uncertainties can be taken into consideration, requires evaluators to examine features of the **data** points themselves, including both their mean values and assigned uncertainties, and not simply to accept the author-provided mean values and uncertainties as a matter of faith. Failure to consider the possible existence of USU, if the contributions are significant, will result in the very deficiencies that manifest themselves in the aforementioned clues."
246493125,3/5,10.0,0.6,"Scale-dependency of potential cloud effect of forest. To investigate how the potential cloud effect varies with spatial scale, we reprocessed the MODIS cloud cover and GFC **data** into different spatial resolutions to emulate the scale change (using ""mean"" for cloud cover and ""major"" method for forest cover). Specifically, the 0.05°cloud and GFC **data** used in the main analysis were aggregated to coarser resolutions (0.1°, 0.25°, 0.5°, and 1°) and ΔCloud was re-estimated with the window searching strategy of slightly different configurations to accommodate the resolution change ( Supplementary Fig. 12). The specific parameters of the window searching strategy under different resolutions are provided in Supplementary Table 2, including raw **data** resolution, window size, window distance, and display resolution. For a given resolution, ΔCloud was estimated with two-parameter combinations to ensure the robustness of the results.
"
249258070,51/119,2.0,0.429,"Collectively, there were ten categories of police reactive and proactive activities coded from the data, and these categories make up the dependent variables for the current study. Police reactivity to CFS were coded into seven different categories using an approach consistent with previous research utilizing CFS **data** (see Wu & Lum, 2017). Violent crimes included CFS involving reported shootings, robbery, and several types of assaults. Property offenses was a category comprised of burglary, theft, and forgery type offenses. Disorder incidents consisted of an array of disturbances, including general, family, and noise CFS. Suspicious incidents included calls where an alarm was sounded (e.g., vehicle alarm set off) and reports involving a suspicious person or vehicle. The trafficrelated activities category comprised CFS for traffic accidents, driving while intoxicated (DWI), and other traffic-related issues (i.e., road rage). Service-related activities included calls to assist first-responders, including other officers, fire fighters, or emergency medical services. Lastly, the non-crime events category consisted primarily of assisting specialized units with transporting individuals and responding to silent 911 calls."
233705419,26/53,1.0,0.491,"With the EDI hive IoT framework, an efficient test design and evaluation of the **data** guaranteed that all influencing parameters and the interactions between them were presented and evaluated. In this case, the AI-based hybrid model was used to examine the reduction potential of oscillating the coal mass flow to reduce NOx emissions. The campaign consisted of a screening trial, where the **data** were used to identify relevant influencing parameters. Based on these results, a second trial was initiated to further investigate the optimal test settings and the reproducibility of the test setup. The formalized expert knowledge, measurement data, and the decisions for all iterations were captured sustainably through the AI-based hybrid model procedure, which is supported by the EDI hive. With the statistical planning of the trial and the use of statistical analysis algorithms, it was possible to identify significant influences, such as the best oscillation frequency. In this case, it was found to be 1.8 Hz for coal oscillation to reduce the NOx concentration to 380 mg/Nm³."
119479526,24/29,1.0,0.828,"Figure 11 :
11The model prediction, uncertainty intervals, true model and synthetic **data** points are plotted versus x for (a) kombine and (b) MultiNest, respectively.
"
251719245,5/21,1.0,0.238,"Rank-based construction guarantees that each vertex is reachable from at least vertices in the graph. As reflected in Table 2, no singletons were created even for the graph with the least number of edges. With this construction, even with 621 edges, we can already get relationships involving all genes in the **data** set. In contrast with the graph obtained by value-based with = 0.90 with 1, 177 edges,  we can only relate about 63% of the genes in the **data** set. However, this construction also allows edges with weak correlation to be part of the network. We show the comparison of the total number of edges between all the graphs obtained by value-based and rank-based construction. In Figure 6, we see that the total number of edges as we increase is far below the total number of edges for the value-based construction. The highest number of edges is with parameter = 10 which is approximately 17% of the total number of edges of the graph with = 0.70. In Figures 7 and 8, we show the two graphs obtained by rankbased construction with parameters = 2 and = 10, respectively. We visualized both graphs using the Fruchterman-Reingold algorithm to compute for the position of the nodes and used the biological classification to color the set of nodes. With = 2, the resulting graph consists of all the 384 genes in the original **data** set. The genes belonging to groups are not visually separated as compared to the graph obtained by = 10. Even though the graph uses way less number of edges as compared to the graph obtained through valuebased with = 0.70. The visual closeness of the nodes belonging to the same functional group is present. The temporal relationship genes that are present in the original NMDS visualization are also reflected. Visually, we can see the proximity of genes belonging to the same functional groups for both value-based and rank-based construction with = 0.70 and = 10, respectively. According to Ruan et al., [11], the comparison of the rank-based and the value-based construction has not been rigorously examined in the literature. In the following section, we will test whether which of the following graphs can be used to predict functional groups using community detection algorithms."
52141668,8/16,1.0,0.5,"As the result of the low accuracy in detecting deception, we perform an additional experiment. In this case, we also try to use features from the  Figure 3: Comparison accuracy and F-measure between using development **data** and test **data** acoustic/prosody that can be extracted from the recorded sound **data** of IDC. In accordance with previous research related to detecting deception using speech analysis (Enos, 2009;Graciarena et al., 2006;Hirschberg et al., 2005), we use features from silence, energy, and pitch category then apply some normalization techniques to the extracted features."
119598666,2/9,1.0,0.222,"The recursion (6) together with the original self-replicating equation (4) suggest that the choice λ 2 = µ in (5) is of special arithmetic significance. This is substantiated in the next example. Example 1. We consider the functional relation (5) with integer parameters λ and µ. The terms c n (λ, µ) of the corresponding sequence are polynomials in λ and µ. The first few are
1, 2(µ − λ), 7µ 3 − 10λµ + 3λ 2 + 2µ − 2λ, . . . .
Using these initial terms we then determine congruence conditions on the parameters λ, µ, which need to be satisfied in order that the p r -congruences (8), with ℓ = 1, hold for all primes p. In the case λ = µ we have c n (λ, λ) = 0 for n ≥ 1, and so we assume λ = µ in the sequel. Remarkably, the empirical **data** suggests that these congruences only hold in a finite number of cases, all of modular origin, as well as an infinite family of unclear origin. Specifically, the p r -congruences appear to hold modulo p r for all primes p only in the cases λ 2 = µ with λ ∈ {−2, −1, 2, 4, 16} and the infinite family of cases λ = −2µ where µ is any even integer. If there exist further cases, then min{|µ|, |λ|} > 10, 000 (and |λ| > 100, 000 in the particularly relevant case λ 2 = µ)."
238249036,5/5,1.0,1.0,"FIG. 2 .
2Neutron multiplicity dependence of (upper) hα core i and (lower) hm μμ i of μ þ μ − pairs in ultraperipheral Pb-Pb collisions at ffiffiffiffiffiffiffiffi s NN p ¼ 5.02 TeV. The vertical lines on **data** points depict the statistical uncertainties, while the systematic uncertainties of the **data** are shown as shaded areas. The dot-dashed line shows the STARLIGHT prediction, and the dashed line corresponds to the leading-order QED calculation of Ref.[48].
"
129244480,20/39,1.0,0.513,"Carbon isotope values with depth from different coal seam gas types in the Sydney Basin. Figure 16 Cross-plot of carbon isotope values from methane and CO 2 in the Sydney Basin. Carbon isotope fractionations from 1.06 to 1.09 are characteristic of CO 2 reduction, whereas acetate fermentation results in a values from 1.03 to 1.06 where aCO 2 -CH 4 ¼ (1000 þ d 13 C-CO 2 ) / (1000 þ d 13 C-CH 4 ) Smith et al. 1992;Golding et al. 2013). settings (e.g. Van Voast 2003), which also have relatively elevated salinity and TDS. Golding et al. (2013) and Taulis & Milke (2013) have highlighted increased alkalinity levels in such waters as well. Limited water **data** from the Sydney Basin (McLean et al. 2010a, b;Holmes & Ross 2011) are consistent with these findings, with principally Na and HCO 3 -dominated waters occurring in regional methane production observation wells that show increasing alkalinity, salinity and Cl content with depth."
2482529,22/37,1.0,0.595,"(Note that the error is the error relative to the target , not the error relative to the testing **data** . The difference between these two errors is random noise, known as irreducible prediction error.) Typically there is a trade-off between bias and variance -decrease in one usually causes increase in the other -but we have seen here that it is sometimes possible to minimize both simultaneously.
1 4 ⁄ d i t i ≠ d i t i = squared error E g i t i - ( ) 2 ( ) = E g i t i - ( ) 2 ( ) E g i ( ) t i - ( ) 2 E E g i ( ) g i - ( ) 2 ( ) + = E g i t i - ( ) 2 ( ) t i E g i β i - ( ) 2 ( ) β i"
16542351,6/8,2.0,0.75,"
sti cs on b events atLEP w i l lsoon al l ow to study pol ari zati on e ects i n a purel y b sam pl e,thati sforeseen to retai n m ostofthei ni ti alb-quark pol ari zati on,i . e. an orderofm agni tudel argerthan i n theprevi ousi ncl usi veanal ysi s.In refs.[ 16,25,26] and[ 27] ,som estudi esofexcl usi ve decaysofpol ari zed b arepresented.O fcourse,the am bi gui ti es i n the know l edge ofthe b-quark fragm entati on al so appl y to pol ari zati on m easurem ents based on sem i l eptoni c b-baryon decays.A rel evant questi on i s w hether, w i th a reasonabl e stati sti cs on sem i l eptoni c b decays (som e **data** are al ready avai l abl e[ 28] ),LEP **data** w i l lno l onger need to be norm al i zed w i th l ower-energy data. O ne coul d study the rati os hx N ' i b (P )=hx N ' i B ofthe l epton m om ents for the b sam pl e over those for the B -m eson sam pl e,both m easured at LEP.In thi s case,one gets
"
118072666,8/13,4.0,0.615,"The use of one single kaon selection efficiency for all φ momenta was compared to the use of separate values above and below p φ = 1.2 GeV/c. The observed difference in the average multiplicity was 0.9%. This is below the statistical error on the kaon identification efficiencies, hence no additional error was assigned to this source. The statistical error on the kaon selection efficiencies is treated as part of the statistical error in this analysis as it is obtained from the same **data** set as our signal and scales appropriately."
17309701,9/16,1.0,0.562,"Fig. 2 .
2The radial velocity observations of CV Vel versus the orbital phase. The filled and open circles correspond to the velocities of the primary and the secondary, respectively. The **data** are listed in
"
235829209,6/47,2.0,0.128,"There are also some other advanced machine-learning techniques available that can better evaluate accuracy of an ANN model. For example, using the Kfold cross validation method [31] ensures that every observation from the original dataset has the chance of appearing in training and test sets, especially when limited input **data** is available. In our examples for lattice optimization, rather than using the time-consuming cross-validation that K-fold requires, sufficient training **data** were generated with a particle tracking simulation code, because using only one-turn tracking for such rings requires much less of a demand on computational resources."
119313773,14/21,1.0,0.667,"Figure 6 :Figure 7 :
67Breakdown parameterq (solid line) as a function of the **data** size N , estimated using dε = 10 −6 . Top: logistic map (17) with fixed range of perturbations a = 3.8(1 + ε) and observable A(x) = x. Bottom: doubling map with perturbation X(x) = sin 2πx and observable A(x) = cos 2πx. The error bars show the two-sided 90% prediction interval for q as estimated from K = 200 realizations differing in the initial conditions. The dashed line shows qα for α = 0.05. Note that for the doubling map (bottom) the breakdown parameterq assumes values below the plotted range for some values of N . The observed sample averageĀ as a function of the perturbation size ε for an observable A(x) = x and a linear fit (solid line). Top: for **data** length N = 10 5 wherê q = 5.03 × 10 −5 and the breakdown is not detectable (p = 0.148). Bottom: for **data** length N = 10 6 whereq = 5.32 × 10 −5 and the breakdown is detectable (p = 1.31 × 10 −8 ). Herē A was obtained from a single simulation.
"
246863515,7/44,1.0,0.159,"Theorem 1. Under conditions in Lemma 1, the average long-term treatment effect is identifiable: for any function h 0 that satisfies Equation (10),
τ = E [h 0 (S 3 , S 2 , A, X) | A = 1, G = E] − E [h 0 (S 3 , S 2 , A, X) | A = 0, G = E] .(11)
Theorem 1 states that the average long-term treatment effect can be recovered by marginalizing any outcome bridge function (which is defined on the observational **data** distribution) over the experimental **data** distribution. This shows how observational and experimental **data** can be combined together to identify the long-term treatment effect."
207778550,11/48,2.0,0.229,"For example, suppose hypothetically that the evaluated physics **data** applicable to calculation of the criticality parameter k eff for a particular nuclear reactor are very well known, with a single exception being the neutron multiplicity ν, and that the computational model and simulation procedures also are assumed to be precise and accurate, e.g., see [78,81,82]. Furthermore, let's postulate that a significant discrepancy does exist between the currently accepted value of ν and its true value. Then, the calculated criticality parameter k eff will differ from the measured one, and existence of the ν **data** discrepancy may be revealed. Unfortunately, such sources of uncertainty that produce biases in application simulations rarely manifest themselves with such transparency in realistic situations."
135401293,8/16,1.0,0.5,"Based on the experimental work an Artificial Neural Network (ANN) model was developed to predict BSFC and BTE. The input parameters were load, percentage of biodiesel, injection angle, injection pressure. The predictive ability of the developed network model for BSFC and BTE is excellent. The selected input parameter for this work is based on e review of the literature [23][24][25][26]. The comparison is made between network predicted Thermal efficiency with experimental **data** and which is shown in Fig. 11. The percentage of deviation between ANN predicted BTE with experimental BTE is exhibited in Fig. 12. The developed ANN model for Brake Thermal Efficiency (BTE) has a very low MSE content of 0.0069 along with RMSE of 0.66522% and MEP of 1.12% across all the test points. The correlation coefficient (R) for network estimated values is 0.995355. "
207778550,30/48,2.0,0.625,"Thus measurements with ionization chambers that do not take particle leakage into account, and for which the particle leaking effect cannot be corrected, should not be considered as containing a correctable USU contribution. A component of uncertainty should not be added to these **data** to compensate for a deficient correction of raw experimental data. In most cases it would be best to discard such **data** when it is known or assumed that such a correction was not made."
2482529,11/37,1.0,0.297,"We generate and from by randomly flipping bits in with probability p. The probability that the class of a training example or a testing example matches the target is , but the probability that the class of the training example matches the class of the testing example is . target
t 1 … t 32 , , t = = train α 1 … α 32 , , α = = test β 1 … β 32 , , β = = t i α i β i 0 1 , { } ∈ , , α β t t 1 p - 1 2p - 2p 2 + P α i t i = ( ) 1 p - = P β i t i = ( ) 1 p - = P α i β i = ( ) 1 2p - 2p 2 + = α i β i t i = = 1 p - ( ) 2 α i β i t i ≠ = p 2 1 p - ( ) 2 p 2 + 1 2p - 2p 2 + = α β t 2 n (9)(10)
If the i-th bias strength gene has a value , then there is a probability that the individual will guess ; otherwise, there is a probability that the individual will guess . This simplified model does not describe the learning mechanism. We are dealing with a level of abstraction where the exact learning mechanism is not important. Since we assume that all 32 possible cases are in the training **data** , the individual can learn by simply storing the training data. In a more complex model, the genotype could encode the architecture of a neural network, and back propagation could be used to learn from the training **data** (Balakrishnan and Honavar, 1995)."
2423505,16/23,1.0,0.696,"As mentioned in the introduction, transfer learning, sometimes called multi-task learning is different from DA. In transfer learning (TL) the joint probability of each task {P (Y k , X)} m k=1 are different but there is only one marginal **data** distribution P (X). Normally, the state space of the Y k are assumed to be different, e.g. Ω(Y 1 ) = Ω(Y 2 ). When learning class conditional models, {P (Y k |X, θ k )} m k=1 , it is typically assumed a common prior distribution of the variables θ 1 . . . θ k ∼ P Θ (θ)."
119313773,1/21,1.0,0.048,"On a more fundamental level, however, it is by no means clear that highdimensional complex systems do obey linear response theory. In this paper we do not attempt to answer this question. Rather, we consider the following practical issue: systems which do not obey linear response theory are observed with finite time series. In such cases we seek to show that the breakdown might not be detectable, and the system's observed behavior may appear consistent with linear response theory. Moreover, the choice of the observable is crucial for the detectability of the breakdown of linear response in finite time series. In particular, we will show that global observables are less able to detect the non-smoothness of the invariant measure whereas local observables which hone in on the roughness of the invariant measure will make the non-smoothness apparent for smaller amounts of data. Finally, the perturbation size also impacts on the detectability of breakdown, with smaller perturbations requiring more **data** for successful breakdown detection. This work is motivated by the contradiction between the reported success of linear response theory in the climate sciences and rigorous mathematical results proving the non-existence of linear response theory for a large class of dynamical systems."
57758460,1/11,1.0,0.091,"Water quality deterioration has a great influence on the aquatic biota and the ecosystem of a river. The increase in suspended solids limits the light penetration which has major impacts on algae and macrophytes while nutrient enrichment can lead to the depletion of oxygen and subsequently fish kill [9][10][11]. Exposure to high turbidity and suspended solids impacts fish growth and increases the mortality of fish [12][13][14][15][16]. Hence, water quality monitoring is important in order to evaluate the quality of the river for the health of sensitive aquatic organisms. The baseline **data** is also useful in management decision for improving and protecting the environment."
118661060,5/17,1.0,0.294,"with cos 2 θ 12 = 0.692, cos 2 θ 13 = 0.9766 for normal hierarchy and sin 2 θ 13 = 0.024 for inverted hierarchy (PDG **data** 2014). In the above expression, the U e1 and U e3 are the elements of the unitary matrix U transforming the mass eigen states to the flavor eigenstates as
ν e ν µ ν τ = U ν 1 ν 2 ν 3 .(46)
It is normally parametrized by three mixing angles, θ 12 , θ 13 , θ 23 and a CP-violating phase, δ, as 
U = U e1 U e2 U e3 U µ1 U µ2 U µ3 U τ 1 U τ 2 U τ 3 =  c
with the common shorthand notations of c ij = cos θ ij and s ij = sin θ ij for i, j = 1, 2, 3. Fig. 14 show the cumulatice event numbers at Super-Kamiokande and KamLAND as functions of time. The upper panel is for the Fe core models whereas the lower one corresponds to the ONe core model. The distance to the star is assumed to be 200 pc, the estimated distance to Betelgeuse. In the case of the Fe core models, more than 1 neutrinos will be detected a day before collapse both at Super-Kamiokande and KamLAND, which seems to be consistent with the preceding paper (Odrzywolek et al. 2004). The total event numbers are also comparable between the two detectors although the detector volumes are vastly different. This is mainly due to the sensitivity of KamLAND to low energy neutrinos (see Table 1 for the energy threshold of each detector)."
17309701,3/16,1.0,0.188,"To measure the ω(O − C) 2 values, given in Table 4, we have re-analyzed the **data** of Feast (1954) and Andersen (1975). This showed that the **data** of Andersen (1975) and those of our paper have similar quality while those of Feast (1954) have clearly lower quality, as already emphasised by Andersen (1975). For this reason, we omitted Feast's **data** in the remaining of this paper. We combined the newly obtained radial-velocity values ( Table 2) with those previously obtained by Anderson (1975). This gave a total of 62 radial-velocity values for each of the components to which we assigned equal weights. As can be seen in Table 4, we find an offset in average velocity V 0 between the two radial-velocity sets. This could be due to the difference in used spectral lines in the two studies, but it may also be that we find a downward trend in V 0 over time due to the presence of a third body. We have too few epochs to model any V 0 trend for the time being. For this reason, we shifted the **data** towards the CORALIE    Table 2 and the sine curve corresponds to the elements given in Table 4. The center-of-mass velocity is indicated by the dashed-dotted line."
117119864,68/83,2.0,0.819,"The literature paradigm for low temperature f-electron inclusion in the FS volume comes from beautiful FS studies in the early 90's of the isostructural compounds LaRu2Si2, CeRu2Si2, and CeRu2Ge2 [79][80][81], using magneto-oscillatory (MO) techniques such as the de Haas-van Alphen (dHvA) effect. The cross-sections of panel (b) of Fig. 4 illustrate the LDA FS's of f 0 LaRu2Si2 and f 1 CeRu2Si2 [82]. Hole (electron) pockets are shown with bold (thin) lines. There are several pieces, including a large hole pocket centered around the Z-point of the Brillouin zone, shown in panel (a), smaller hole pockets centered around Z, a multiply connected electron sheet centered around Γ, and also a small electron pocket around Γ. The LDA bandstructure for CeRu2Si2 includes the 4f electron of the Ce 3+ ion in the FS. The electron is accommodated about 50% each in the multiply connected electron sheet, which expands and changes topology, and the large Z-point hole pocket, which shrinks to become the so called ""pillow"" of CeRu2Si2, shown in panel (a). These two pieces of Fermi surface have the large masses that account for the γ-value of this heavy-Fermion material. It is found [78] that dHvA **data** show all orbits expected within the renormalized LDA for CeRu2Si2, and show the expected size differences for the La and Ce systems. Further, CeRu2Si2 displays a metamagnetic transition under a sufficiently high magnetic field and CeRu2Ge2 is a ferromagnet. The dHvA experiments find that in these two situations where the f-moment is restored, the measured FS is like that of LaRu2Si2, again showing that the Luttinger theorem is obeyed [79,80]."
119313773,10/21,4.0,0.476,"• The global character of an observable may inhibit the detection of breakdown of linear response. For a given finite **data** length and given perturbation size, suitably localized observables may be needed to probe linear response. This, however, requires either detailed knowledge of the underlying dynamical system or computationally involved scans of the parameters of the observable such as its scale and its location."
246863515,3/44,1.0,0.068,"Here Equations (2) and (3) mean that U and X together account for all confounding in the observational data, but the observed covariates X alone are not enough. Moreover, we impose the overlap condition 0 < P (A = 1 | U, X, G = O) < 1, which is a standard assumption in causal inference literature. Because of the unmeasured confounders U , the observational **data** alone is not enough to identify the treatment effect parameter τ in eq. (1)."
135401293,10/16,2.0,0.625,"The present work also exhibits fruitful application of ANN model to estimate the output parameters of compression ignition engine fueled by NOME and Biogas with varying injection pressures. The results of developed ANN model is compared with actual results found by experimentation. From the error analysis, it was evident that the ANN predicted **data** matched the experimental **data** with high accuracy and correlation coefficient (R) values ranging from 0.977 to 0.999. The MEP was observed to be in the range of 1.1-4.57% with very low MSE. Study of proposed Back propagation network results concludes that ANN is one of the powerful predictor tools.
"
208006088,9/26,1.0,0.346,We test the OLMM in experiments on synthetic and real-world **data** sets. An implementation is available at https://github.com/wesselb/olmm.
235829209,12/47,1.0,0.255,"Figure 5 :
5Simulated one-turn transportation of a storage ring lattice with some given inputs (left) and their outputs (right) in the 4-dimensional phase space. The ranges of inputs are comparable to the desired dynamic aperture there. This is the input and output **data** that the ANN is trained on.
"
118072666,2/13,2.0,0.154,"We use Monte Carlo samples of Υ (4S) → B 0 B 0 and B + B − decays, corresponding to twice the expected number of B mesons in the **data** sample, to study our selection efficiency. The B-meson decays are simulated according to previously measured branching fractions which account for approximately 60% of all B decays. The remaining 40% are modeled by JETSET [7], while preventing any enhancement of the first 60%. The detector response in these samples is simulated with the GEANT4 program [8] and cross-checked with control samples in the data."
67205543,9/18,1.0,0.5,"The testing **data** including 5 dimension features of 30 grains (15 bread wheat grains and 15 durum wheat grains) and their testing results are tabulated in Table 2 to further inspect the **data** and results. While the number of ""2"" is assigned to specify bread grains, ""1"" is appointed to define the durum grains as targets of the ANN-ABC model. The ANN-ABC model proposed in this study accurately classifies 19 grains with 0 (zero) and 11 grains with very small absolute errors. Therefore, it classifies the total grains of 30 with a negligible MAE of 0.0034 and with 100% accuracy. It demonstrates that the proposed IPT based ANN-ABC model can be successfully utilized to classify the wheat grain varieties in an automatic manner.  "
117119864,64/83,1.0,0.771,"TiTe 2 is a quasi-2d material whose transport properties show it to be a Fermi liquid [36]. Its electronic structure has a non-degenerate band crossing EF at an isolated place in k-space. This band is easily observed in ARPES and is widely studied as an example of Luttinger's FL spectral function. Fig. 1 shows the comparison of ARPES **data** and FL theory lineshapes from Ref. [37] in which the material was first proposed as a FL reference. The fitting parameter given in the figure is β′= ZFβ, where ZF is the quasi-particle weight on the FS, taken as k-independent in the **data** analysis. Further details of the experimental procedures and **data** analysis are set forth in Ref. [37]. Here we note only a few points. First, the lineshapes near EF are well described by the theory, but further from EF near the bottom of the dispersing band it is clear that the lineshape is more complex. This lineshape can actually be fit [38] by the Matho self energy if the self energy is allowed to be k-dependent. Second, the amplitudes of the various spectra have been normalized to be the same at the peak position. In fact the intensity falls sharply for k outside the FS, as would be expected, and the spectra for these k are fit by the part of the k > kF spectral function lying below EF, i.e. the part mentioned above that gives rise to nk > 0 for k > kF. Third, the least binding energy edge is dominated by the broadenings due to the experimental angle and energy resolutions and the temperature. Data utilizing the better resolutions recently available, at lower temperature, and also over a wider photon energy range to better determine the departure from quasi-2-dimensionality, are being taken by various groups [39], but have not yet appeared in the literature."
118072666,1/13,2.0,0.077,"Given the large size of the BABAR **data** sample, this measurement is limited by systematic errors. As a result, this analysis is designed to minimize these systematic errors. Minimal selection criteria are applied, and efficiencies and backgrounds are evaluated directly from **data** where possible. The measurement is performed in φ-momentum intervals to minimize the systematic effects that may be introduced by differences between the φ momentum spectrum in **data** and simulation."
40828380,6/13,1.0,0.462,"Our interest in the first example is to study the effect which the parameter p has on various performance measures. The **data** for this example is as follows: K 15, L-5, the service rates for servers 1 and 2 are geometrically decreasing with (a) 3 (1) 2 (2) 2, It can be verified that A-4."
235829209,1/47,2.0,0.021,"Therefore, an intuitive method for detecting chaos directly, purely from **data** is possible. In other words, predictability itself can act as a chaos indicator. From our studies we observed that by using the predictability of less-complex surrogate models, and a small volume of training data, some nonlinear behaviors in a dynamical system can be well characterized."
236522469,6/33,1.0,0.182,"Validation of the classifications was performed on the dataset for the test. For this, statistical metrics were used to evaluate and test the performance of the adjusted CNN. The metrics are specified in Table 3, of which are calculated according to the results of the classifications. The Kappa index is a measure of agreement used in nominal scales that gives us an idea of how far the observations deviate from those expected, at random, thus indicating to us how legitimate the interpretations are. It measures the percentage of the **data** values on the main diagonal of the table and then adjusts these values for the amount of agreement that might be expected [50]. Accuracy is an index that reflects the rate at which individuals are correctly classified into the category containing their true score. Ranking accuracy is usually attributed to the appropriateness and validity of your decisions based on the obtained score. A large value for the index indicates a high hit rate of individuals in the correct categories, and a low value indicates a lower rate of correct classification of individuals [51]. Adjusted F-score AGF =
5 × PPV × TPR (4 × PPV)+ TPR × (1 + 0.5 2 ) × NPV × TNR (0.5 2 × NPV)+ TNR
Use all confusion matrix elements and provide more weights to samples that are correctly classified in the lowest class."
53478339,24/24,1.0,1.0,"Figure 8 .
8Temperature records from (a) mid-Holocene Sr/Ca derived record from KR-AMA-2, (b) modern microatoll KR-MMA-1 Sr/Ca record, and (c) instrumental **data** obtained from Mourilyan Harbour in situ **data** logger (http://www.aims.gov.au). Temperature values given are the mean of each dataset, with gray shading indicating the 95% confidence interval of the mean."
17228594,6/7,1.0,0.857,"Figure 2 :
2c(r). c pert (r) : 1-loop perturbation theory. β = 12.5 closest to **data** and β = 5 farthest.
"
17837291,15/52,4.0,0.288,"In each of Examples 1-4 above, the Euler **data** identity follows immediately from the algebraic identity Ω j * r P d = j * 0 P r j * 0 P d−r , and Lemma 3.3. Strictly speaking, in the examples above, we must require that c 1 (L) be an invertible class. This requirement can be easily met by twisting L by a trivial line bundle on which T acts by a suitable weight. In the end, we will only be interested in the nonequivariant limit of an Euler data. Thus the choice of twisting is of no consequence at the end."
207778550,12/48,1.0,0.25,"A quantitative clue that USU need to be considered is manifested when a calculated global chi-square-perdegree-of-freedom parameter (χ 2 /df ) that exceeds 1 significantly is generated in an evaluation without consideration of an USU contribution. This is the case especially if care has been taken by an evaluator to eliminate clearly discrepant experimental data, and to consider all known sources of uncertainty (as well as their correlations) in a realistic manner. Unfortunately, it is common practice for evaluators to simply multiply all the input **data** uncertainties by the factor χ 2 /df when this happens, and thus artificially force χ 2 /df to be exactly 1, e.g., see [3,23,25]. This is not considered to be good evaluation practice because it treats all input **data** points equally, e.g., see [79]. Doing so may lead to biases owing to the excessive influence of those **data** points that contribute to the large chi square per degree of freedom. So, it is a poor substitute for actually uncovering the specific origin of the problem (e.g., discrepant **data** points, overlooked known sources of uncertainty, failure to consider correlations, etc.), or for performing a detailed examination of the individual terms that contribute to the calculated global value of χ 2 /df , and only then introducing a targeted USU contribution when no other options for eliminating the problem are envisioned."
218971725,1/10,1.0,0.1,"Large epidemiological studies often collect information on disease status and a large number of covariates for the entire cohort. However, variables of interest, such as risk factors or some expensive exposures, are cost-prohibitive to collect. It is only possible to measure these variables on a subsample of individuals under a fixed budget. Two-phase stratified sampling (Neyman, 1938) can be useful in this situation. At phase 1, we collect relatively cheap information for the entire cohort, and at phase 2, we sample a small number of individuals from the strata defined by phase-1 **data** and measure the variables of interest. With considerate choices of stratification and phase-2 sampling probabilities, the two-phase design will result in efficient parameter estimations under a fixed budget constraint (Breslow and Chatterjee, 1999)."
52141668,15/16,1.0,0.938,"Table 3 :
3Experiment result of Random Forest (RF), Support Vector Machine (SVM), and Neural Network (NN) models using several resampling techniquestruth class. We obtained 98.19% accuracy for 
classifying the truth **data** and only 9.01% for 
classifying the lie data. "
246165803,26/45,1.0,0.578,"Three factors heavily influenced the experiment, and can substantially improve the prediction in real scenarios. The first factor is the amount of **data** collected, the second is the quality of the road, and the third is the different road characteristics that exist in real scenarios."
118072666,3/13,1.0,0.231,"The selection of φ → K + K − candidates requires two oppositely-charged tracks that satisfy 0.1 < p T < 10 GeV/c, have at least 12 hits in the DCH, are consistent with originating from the primary interaction point, and satisfy kaon identification criteria based on dE/dx measurements and Cherenkov radiation. Tracks are assigned a kaon mass hypothesis and neutral two-track combinations are formed. Candidates are selected if their invariant mass is in the range 1.004 < m KK < 1.036 GeV/c 2 . This mass window is equivalent to about 4.5 standard deviations on either side of the nominal φ mass, where the RMS spread in the m KK distribution is due to both the natural φ width and the detector resolution. This relatively large acceptance is chosen to reduce the effect of a possible mass resolution difference between **data** and Monte Carlo, at the expense of signal-to-background sig-nificance."
216162633,2/6,4.0,0.333,"• There exists some t 0 ≥ 0 such that E(t 0 ) < d, then either  We notice that the only way to get blow up of in finite time is that, along the solution and since the energy is a non-increasing function, there exists some t 0 ≥ 0 such that E(t 0 ) < d and u(t 0 ) ∈ V . If the damping term is nonlinear of the form g(u t ) ≡ δ|u t | λ u t , λ > 2, although the dynamics is more complicated, a similar result holds, see [8]. Indeed, the solution blows up in a a finite time if and only if r > λ and there exists some t 0 ≥ 0 such that E(t 0 ) < d and u(t 0 ) ∈ V . An open problem is to characterize the set of initial **data** such that E(t 0 ) < d and u(t 0 ) ∈ V , for some t 0 > 0, especially when E 0 ≡ E(0) ≥ d and u 0 / ∈ V . Another open problem is to find conditions on the initial **data** in order to get E(t) ≥ d, for all t ≥ 0. Then, according with Theorem 2.2 those solutions are global and they approach to the set of nonzero equilibria. See also [9], where Theorem 2.2 is proved for the Timoshenko equation. In the conservative case, δ = 0, we know the following result for E 0 ≤ d. 
Theorem 2.3 ([24, 10]). Consider any solution (u,u) of (NLW), in the sense of Definition 2.1, with initial **data** (u 0 , u 1 ) ∈ H = V A × W P ≡ H 1 0 (Ω) × L 2 (Ω) and δ = 0. If E 0 < d,If E 0 = d, and
• there exists some t 0 ≥ 0 such that I(u(t 0 )) > 0, then the solution is global and uniformly bounded in time in the norm of H. • there exists some t 0 ≥ 0 such that I(u(t 0 )) < 0 and (u(t 0 ), v(t 0 )) 2 ≥ 0, then the solution blows up in finite time."
249258070,91/119,4.0,0.765,"The breakdown of self-initiated activities across these three units were 64.2% for patrol, 17.5% for special crime units, and 18.3% for the differential response team. Unfortunately, the HPD CFS **data** do not provide detailed information about the nature of self-initiated activities, such as offense or incident type for patrol officers. Activities performed by specialized crime investigation and DRT officers might be deduced approximately using the division information. But to further breakdown the activity to specific units produces small frequencies of specific offenses, which would have been insufficient to conduct meaningful analysis. For example, there were only 338 self-initiated activities recorded under gang division, and 165 self-initiated activities recorded under robbery division during the study period. The **data** do include some information on dispositional outcomes of police self-initiated activities. The most frequent type of action for self-initiated activities is ""collecting information"" for each of the three types of police units. The disposition of ""collecting information"" is used in instances where the incident is resolved without needing to generate an offense report or referral for further investigation. Some examples are if the officer resolves the issue by mediation, the complaint is a civil issue rather than criminal, or if it is to provide further information about an offense report that already exists. Other self-initiated actions include making an offense or supplementary report, making an arrest, issuing tickets, etc."
119353639,1/5,1.0,0.2,"The enormous volume of observational astronomical **data** [1][2][3][4][5][6][7] shows us a universe well adjusted by the model ΛCDM with respect to flatness, expansion and acceleration. However, there are controversies that current physics tries to settle in different ways. One widely used way is to assume that there are interactions in the dark sector [8][9][10][11][12][13][14][15][16][17][18] that explain, for example, the difference in energy density values between the tentative calculations assigned to the primordial vacuum and what is currently measured as dark energy. Other discrepancies refer to the coincidence between ranges of magnitude of the components of dark matter and dark energy and also to the unexplained evidence that assigns greater longevity to certain some old high redshift objects (OHROs), old galaxy and quasar discovered, compared to what is considered the age of our universe [19]. These disagreements have impelled to consider interactive models that generally only contain two components. In particular, two-component models have been developed where a holographic fluid plays the role of dark energy [20,21,[23][24][25] and has been shown to be intrinsically interactive [26].The next step has been to consider cosmological models of three interacting components with constant barotropic indices [27,28]. In this work, we present an analytical treatment of cosmological models commanded by three interactive fluids with arbitrary barotropic indexes, constant [29] or not. This last case is applicable to universes with quintessences, k-essences and holographic fluids. We also propose the need to extend the functional forms of the holographic energy densities that are usual in the literature in the face of the incompatibility presented by them when special interactions are used."
119479526,5/29,1.0,0.172,"It is common practice in the development of thermodynamic property models to identify outliers and remove them from the analysis. This may be a reasonable approach in many cases, especially when the questionable **data** points or sets have known sources of error or capture different physical phenomena. In other cases, however, we may not know the reasons for the discrepancy, and we risk ignoring relevant physics or uncertainty in the property of interest by removing the data. In the following section, we present an approach to incorporate datasets with potential systematic errors."
207847805,1/7,1.0,0.143,"Dust evolution starts at the earliest stage of star formation, during the formation of cores that slowly contract to form pre-stellar cores (PSC), to the collapse of these PSCs into protostars and protoplanetary disks (PPD). The contraction time of PSCs is still a debated question and ranges from less than 1 My [1,2] to 10 My lifetime [3]. PSCs are dense (>10 5 H . cm −3 ), compact (< 10 4 AU) and cold (5-12 K) objects, making their study difficult. They are very opaque and most gaseous species are depleted onto grains preventing the study of the inner parts. Dust is the only tracer that is present from cloud edge to the densest part, allowing to characterize cloud density structure. Nevertheless, dust itself is a poor tracer in visible and near-infrared (NIR) since its absorption is too high to detect the reddening of the stars above A V ∼ 50 mag. Recent observations of PSCs with Spitzer [4][5][6] and PPDs with SPHERE [7] in scattered light has brought hope to put more constraints on dust properties. Indeed, all parameters involved in the dust emission (dust temperature, density, grain size, emissivity and spectral index) are varying across the cloud leading to degenerate solutions when dust emission is used alone [8]. Studies combining dust scattering and emission reproduced successfully PPD observations, allowing to constrain the geometry and density structure in PPDs [e.g. 9]. On the other hand, a consistent multi-wavelength modeling of PSCs remains an unachieved goal. Grain emissivity in the far-infrared (FIR) has been linked consistently to its absorption efficiency in the near-infrared (NIR) at short wavelengths only [10] or at low resolution with 5 Planck **data** [11], and thus never reproducing observations of the densest part. With our on-going NIKA2 open time Program, we aim at building a consistent multiwavelength picture of two neighbor molecular clouds, L183 and L134 (116±6 pc [C. Zucker priv. comm.] and 107±5 pc [12] respectively) hosting 4 PSCs."
246863515,1/44,7.0,0.023,"In contrast, our paper does not need long-term outcome observations in the experimental **data** but only need them in observational data. Moreover, our paper does not view short-term outcomes as proxies for the long-term outcome, so we avoid these previous surrogate criteria. Instead, we view them as proxies for unmeasured confounders to correct for confounding bias. See also discussions in Section 2.3."
17837291,27/52,1.0,0.519,"To proceed, we make two further choices: let b T be the T -equivariant Chern polynomial c T , and let V = V + ⊕ V − be a mixed bundle with splitting type
(L + 1 , .., L + N + ; L − 1 , .., L − N − ).
Here the L's are T -equivariant line bundles on X with
c 1 (L + i ) ≥ 0, c 1 (L − j ) < 0, Ω := c T (V + )/c T (V − ) = i (x + c 1 (L + i ))/ j (x + c 1 (L − j )) i c 1 (L + i ) − j c 1 (L − j ) = c 1 (X).
From this, we get an Ω-Euler **data** Q : Q d = ϕ ! (π * c T (V d )) as before. By the Linking Theorem, Q is linked to the Euler data
P : P d = i c 1 (L + i ),d k=0 (x +L + i − kα) × j − c 1 (L − j ),d −1 k=1 (x +L − j + kα).
As before, we set
B : B d = i * 0 P v d , A : A d = i * 0 Q v d .
We consider three separate cases. We will be using the elementary formula
M k=1 ( ω α − k) ≡ (−1) M M !(1 − ω α M k=1 1 k ) (8.1)
where "" ≡ ′′ here means equal mod O(α −2 ), to compute the leading terms of
B d = i c 1 (L + i ),d k=0 (x + c 1 (L + i ) − kα) × j − c 1 (L − i ),d −1 k=1 (x + c 1 (L − i ) + kα) × 1 a D a ,d k=1 (D a − kα) = Ω c T (V − ) α −N − i c 1 (L + i ),d k=1 ( x + c 1 (L + i ) α − k) × j − c 1 (L − i ),d −1 k=1 ( x + c 1 (L − i ) α + k) × 1 a D a ,d k=1 ( D a α − k)
."
207778550,5/48,6.0,0.104,"An empirical ""fix"" (algorithm) that compensates for PPP (discussed in Sect. II) in a practical way was suggested by Chiba and Smith [54]. This algorithm was incorporated in code GMAP. Since this revised version has been used exclusively in the Neutron Standards evaluation work since 2003, henceforth in this paper the code and accompanying database will be referred to by the new name GMAP. However, the reader should be aware that the name GMA is still widely used by some individuals in reference to even the revised version. As is evident from the discussion in Sect. II, the least-squares method is an inherently linear formalism that can generate significantly biased results when large input **data** uncertainties are involved and there exist non-linear relationships between the input **data** and derived values for observable quantities, e.g., as may happen when ratio **data** are included [37]. Suggestions for dealing with nonlinear effects within the framework of Bayesian evaluation theory are being explored, but they have been implemented only in a few cases, e.g., see [40]."
135401293,5/16,1.0,0.312,"In this proposed work, feed-forward back propagation network was designed and applied. This network is equipped with four input layers, ten hidden layers and two output layers. The input **data** sets and corresponding output **data** sets are required to train and test the network, available experimental **data** set is divided into two groups . One group of **data** for training the network and the other was used to validate the network. Haykin has presented a mathematical model for testing and training ANN [22]. The weights which are in the hidden layers are adjusted by training the network. The weights are stabilized by training the ANN using input and output **data** sets which are obtained by experiment. The weights are adjusted to minimize the error between predicted outputs to actual value. The input parameters are load, percentage of biodiesel, injection angle, injection pressure. The output parameters are BSFC, thermal efficiency. The toolbox of MATLAB07 is used to develop the network and tangent sigmoid transfer function has been used in the hidden layers. The network is trained by using Levenberg-Marquardt method. The performance index of TrainLM algorithm is the mean squared error (MSE) [23] and it is formulated as given below in Table 4. Where, yi is the predicted value of the i th pattern, yk is the target value of the i th pattern and N is Number of pattern."
248366486,8/24,2.0,0.333,"Finally, and in parallel with the behavior of the one-point statistics (front PDF), the two-point statistics estimated by the front covariance also shows a nontrivial time evolution for Eq. (16). This is seen in Fig. 14(b), which shows the behavior of the **data** collapse described in Eq. (15) for different values of time. In parallel with the behavior just discussed for panel (a) of the same figure, 1D KPZ behavior, i.e. convergence to the Airy 1 covariance, is achieved in the growth regime. We can note here that Airy 1 covariances have been also recently found [23] for the linear Edwards-Wilkinson (EW) equation [15,16] in which the 1D KPZ is remarkably absent, perhaps related with the fluctuation-dissipation relation which the KPZ equation satisfies exceptionally in 1D. However, in that case the PDF is Gaussian, as the EW equation is linear, while in our present case the PDF is also of the (TW) KPZ form."
15394341,3/18,1.0,0.167,"The selected week of continuous **data** from the NCEDC contained 7 time gaps, with the longest time gap around 14 minutes in duration. We stitched together the time series data, and placed uncorrelated white Gaussian noise in the time gaps, scaled by the mean and standard deviation of 1000 **data** samples on either end of the time gap. We confirmed that FAST did not detect any spurious events in or near the time gaps filled with synthetic noise."
119415579,8/41,2.0,0.195,"Both the Bz measurements and the Hipparcos Epoch Photometry were analyzed using two methods to identify the most probable rotational periods. First, normalized Lomb-Scargle periodograms were generated using an idl routine based on the algorithm presented by Press (2007). This method yields the spectral power distribution, which is used to identify statistically significant frequencies (i.e. those having false alarm probabilities < 3 per cent) inherent to an unevenly sampled time series **data** set. A substantial benefit of this method is that it can be performed relatively quickly compared to the second period search analysis described below thereby allowing potentially relevant periods to be recognized efficiently. However, for the majority of the mCP stars, an insufficient number of Bz measurements were available to yield statistically significant frequencies. This technique was found to be more useful when applied to the Hipparcos Epoch Photometry because of the larger number of **data** points available for each star. The Bz measurements were then used to verify that the derived Hippar-cos period provided an acceptable phasing of the magnetic data."
119313773,5/21,1.0,0.238,"The rigorous theory by Baladi and co-workers [7,8,6,10] shows that certain dynamical systems such as the logistic map do not obey a linear response. In this section we will investigate how the finitude of **data** may prevent the breakdown to be detectable and how one may falsely be led to believe that linear response was valid."
5648146,9/18,1.0,0.5,"For computing the LLE, the first step is to carry out a reasonable reconstruction of the phase space of the system [12]. The reconstructed phase space from the dynamical **data** must preserve the invariant characteristics of the original unknown multivariate system. Because the **data** in experiment is finite and noisy, choice of delay time is important in the reconstruction of the attractor. As in the case of one-dimensional discrete time series, the coordinate delay method is used by embedding time series variable delay to reconstruct the phase space of a nonlinear system. Supposing that the human body is a multivariate system, we then have an -dimensional time series from the electrical goniometer system: { } =1 = Computational and Mathematical Methods in Medicine 3 { 1, , 2, , . . . , , } =1 . The phase space reconstruction can be described by
= { 1, , 1, − 1 , . . . , 1, −( 1 −1) 1 ; 2, , 2, − 2 , . . . , 2, −( 2 −1) 2 ; . . . ; , , , − , . . . , , −( −1) } = 0 , 0 + 1, . . . , ; 0 = max 1≤ ≤ {( − 1) + 1} ,(1)
where and ( = 1, 2, . . . , ) are the time delays and the embedding dimensions, respectively. Following Takens' delay-embedding theorem, there exists in the generic case a function :
R → R ( = ∑ =1 ), +1 = ( ) .(2)
Time delay affects the quality of the reconstructed phase space. If the selection of each time delay is reasonable and each embedding dimension is sufficiently large, the geometrical characteristics of the strange attractor in the reconstructed space are equivalent to the original state space [13]."
253708011,1/6,1.0,0.167,"The projection of the pear shape [26] from the intrinsic to the laboratory reference frame results in good-parity wave functions, that are linear combinations of the pear shape pointing to the left Ψ l and to the right Ψ r . Assuming that Ψ l and Ψ r are orthogonal, the linear combinations are
Ψ + = 1 √ 2 (Ψ l + Ψ r ) , Ψ − = 1 √ 2 (Ψ l − Ψ r ) .(1)
The linear combination Ψ + is invariant under the space-inversion P operation. However, the Ψ − linear combination inverts its sign and, therefore, has negative parity π = −. In an odd-mass nucleus, the coupling of the unpaired particle to these two states leads to the presence of parity doublets, which are two levels with identical angular momentum but opposite parity. Indeed, as shown in Table 1, several odd-mass nuclei in the region near Z = 88/90 and N = 134/136 exhibit parity-doublet candidates. Clearly, the **data** in Table 1 suffer from uncertainties concerning spin and, especially, parity assignment, and unobserved upper partner levels. Nevertheless, at present 227 Ac is the nucleus with the lowest established parity-doublet energy difference ∆E P D of 27.4 keV."
118072666,10/13,1.0,0.769,"FIG. 1 :
1Top: Invariant mass distributions of candidates passing all selection requirements except that for the mass. The solid histogram shows candidates in the on-resonance **data** sample, while the dashed histogram shows the off-resonance sample, scaled to the luminosity of the on-resonance data.
"
5181686,8/10,1.0,0.8,"Table 1 :
1Estimates of log probability of **data** under various assumptions for K = 1-6K 
No-admixture model 
Admixture model "
232380196,25/28,1.0,0.893,"In this section, we conduct some ablation studies to characterize our hybrid networks. Concretely, we study whether the proposed PSC loss is less sensitive to **data** sampling, the advantage of using PSC loss in feature learning comparing to cross-entropy loss, and the advantage of our curriculum based joint training comparing to the two-stage learning strategy. Table 3. Evaluation of the sensitivity of PSC loss to **data** sampling. Hybrid-PSC with random PSC and Hybrid-PSC with CB-PSC denote in the PSC based hybrid network, we use random **data** sampling and class-balanced **data** sampling for the feature learning branch respectively. Classification accuracy (%) on long-tailed CIFAR-100 is reported.  Table 4. Evaluation of the advantage of supervised contrastive losses over cross-entropy loss for feature learning in long-tailed classification. CE-CE denotes both feature learning and classifier learning adopt cross-entropy loss, i.e., our supervised contrastive loss is replaced by cross-entropy loss. Classification accuracy (%) on long-tailed CIFAR-100 is reported. contribute to the insensitivity of the PSC loss on **data** sampling. Firstly, in PSC loss, the image features and prototypes are both 2 -normalized, which breaks the strong correlations between class frequency and feature norms. Secondly, assuming the affinity score between a sample and its prototype is s yi i = z i · p yi /τ . For a sample x i with label y i ∈ {1, 2, . . . , C}, the gradient of the PSC loss L P SC (z i ) w.r.t s yi i is constant, and the gradient w.r.t the affinity to a prototype from a negative class c ∈ {1, 2, . . . , C}\y i , is exp(s c i )/ y∈{1,2,··· ,C},y =yi exp(s y i ). The denominator excludes the dominating term of s yi i and thus results in a prominent gradient. The constant gradient for positive class and prominent gradients for negative classes can help to alleviate the overfitting in over-sampling and enhance the inter-class separability of the features."
19033986,1/28,3.0,0.036,"In this paper we concentrate on the Dark Matter (DM) interpretation. As we show below, the PAMELA, FERMI and HESS observations still point towards non-conventional DM models, with leptophilic final states, large ""event rates"" in our galaxy and a high mass scale (O(TeV)). The reason for this is traced back to the fact that the new FERMI **data** does not show any sharp feature at low scale. Such a feature must be visible if the DM mass is low. Therefore, DM lighter than about a TeV is now excluded as an interpretation of the PAMELA excess. This conclusion further implies that the positron fraction should plateau or continue rising, but cannot go down at the higher energies being probed by PAMELA. Thus the DM scenario may be excluded in the future by PAMELA."
15294885,21/25,1.0,0.84,"variable [ 13 ,
13Definition 3.1] and by a relation between M(f ) andM (f ) shown in [17, Eq. (5)]. By using the phase φ(f ) ofM (f ), we can rewrite the complementary PSD asM (f ) = |M(f )|e jφ(f ) = k(f ) M(f )M(−f )e jφ(f ) , where 0 ≤ φ(f ) ≤ 2π. In the next lemma, the properties of the impropriety frequency and the phase functions are provided. Lemma 2: The impropriety frequency function k(f ) and the phase function φ(f ) satisfy 0 ≤ k(f ) ≤ 1, k(−f ) = k(f ), and φ(−f ) = φ(f ), ∀f. (17) Proof: Sincem[−k] =m[k] by definition, we haveM (−f ) =M (f ), which implies φ(−f ) = φ(f ). This also leads to k(−f ) = k(f ) by (16). By using the property |M(f )| 2 ≤ M(f )M(−f ) shown in [17, Eq. (5)], we have 0 ≤ k(f ) ≤ 1. ✷ For example, an uncorrelated real-valued PAM **data** sequence results in k(f ) = 1, ∀f , whereas any proper-complex **data** sequence results in k(f ) = 0, ∀f . By using the impropriety frequency function, we can rewrite the MSE (15) in the form of a function ofs(f ) as a function of s(f )."
118926097,1/8,1.0,0.125,"The goodness of the UE simulation provided by MC event generators and corresponding tunes can be tested by comparing predictions with available data. These **data** are generally measurements of the number of charged particles and their transverse momentum sum in different regions of the phase space relative to the direction of the hardest objects in the event. In particular, the hard object, which might be a jet, a charged particle or a Z boson, identifies a direction in the transverse plane. The transverse plane is then divided into four regions, according to their azimuthal angle: a ""toward"" and an ""away"" region sensitive to the hard scattering and its recoiling object, and two ""transverse"" regions, more sensitive to UE contributions. In recent measurements, the two transverse regions are further divided into separate measurements. The transverse region with the highest activity is called ""transMAX"" while the one with the smallest activity is labelled as ""transMIN"". The charged-particle multiplicity and the transverse momentum sum of the charged particles, measured as a function of the transverse momentum of the leading charged particle, are referred to as ""UE observables"" in the following."
1520137,2/35,1.0,0.057,"We study how visual perception of a target bar can be biased by contextual bars in the image, and how a Bayesian model of object inference can account for the data. Human observers are more likely to perceive a target bar when the contextual contrast, i.e., the luminance difference between the contextual bars and background, is weaker rather than stronger. Relative to the situation without the context, they are biased to perceive the target in a context of weak contrast when the target can perceptually group well with the context, as if the context fills in the target. Meanwhile, they are biased not to perceive the target in a context of strong contrast, as if the context suppresses the perception, regardless of whether it could perceptually group well with the would-be target. The Bayesian model illustrates that the context influences the perception by biasing (1) observers' prior belief that a target should be present and (2) observers' internal model of the likely input contrasts from a target bar. Our **data** suggest that brain areas beyond the primary visual cortex along the visual pathway are responsible for inferring object causes for input images."
207778550,10/48,3.0,0.208,"The difference between the measured values y i and the true value µ is given as the sum of various errors. The error due to a finite counting statistics is denoted as ε i and the error due to recognized systematic effects, such as the detector efficiency, as η i . The error due to USU is denoted as δ i . These quantities are all considered as random variables. The errors η i and δ i of different measurements can be correlated, e.g., if the measurements were performed using the same detector or the same sample. It is noteworthy that only the measured values y i are known and the values of all other variables are uncertain and therefore have to be estimated using statistical assumptions. For instance, if the y i represent the measured values corrected for recognized systematic errors, a reasonable and common assumption is that the most likely η i is zero. Even though it is not necessary to do so from a statistical point of view, it is usually assumed that the random variables η i , ε i , and δ i are governed by normal distributions, which leads to a simplified mathematical treatment. Depending on the case, this choice may be motivated either by the central limit theorem, the principle of maximum entropy, or the fact that the normal distribution is the limit of several distributions, such as the Poisson distribution, e.g., see Refs. [3,23]. Report JCGM 100:2008 [79] from the Bureau International des Poids et Mesures is a well-known guide to uncertainty quantification (UQ) for experimental measurements in the field of metrology and, while it is a useful reference for present purposes, its recommendations do not always apply in the case of nuclear **data** evaluation. As mentioned earlier in this paper, the latter often involves employing Bayesian prescriptions that combine available data, which may consist of both experimental and theoretically derived information, while in conventional metrology the **data** are assumed to be entirely observational in nature and sufficiently comprehensive to enable reliable probabilistic treatments of particular measurement processes without having to resort to prior assumptions, e.g., see [3]."
249258070,71/119,2.0,0.597,"Collectively, there were ten categories of police reactive and proactive activities coded from the data, and these categories make up the dependent variables for the current study. Police reactivity to CFS were coded into seven different categories using an approach consistent with previous research utilizing CFS **data** (see Wu & Lum, 2017). Violent crimes included CFS involving reported shootings, robbery, and several types of assaults. Property offenses was a category comprised of burglary, theft, and forgery type offenses. Disorder incidents consisted of an array of disturbances, including general, family, and noise CFS. Suspicious incidents included calls where an alarm was sounded (e.g., vehicle alarm set off) and reports involving a suspicious person or vehicle. The trafficrelated activities category comprised CFS for traffic accidents, driving while intoxicated (DWI), and other traffic-related issues (i.e., road rage). Service-related activities included calls to assist first-responders, including other officers, fire fighters, or emergency medical services. Lastly, the non-crime events category consisted primarily of assisting specialized units with transporting individuals and responding to silent 911 calls."
119421228,1/11,1.0,0.091,"Our theoretical model, described in Sec. II, consists of a diamond coated with a cover layer, which we also sometimes refer to as a ""protective layer"". We use the fluctuation-dissipation theorem to obtain noise spectra at room temperature, and we calculate the effective capacitance and loss tangent. The surface cover materials we consider are: glycerol, propylene carbonate (PC), polymethyl methacrylate (PMMA), polyvinylindene flouride (PVDF), perflouropolyether (PFPE), and dimethyl sulfoxide (DMSO). PMMA, PVDF, PFPE, glycerol, and PC have been commonly used in experimental work [48][49][50][51][52]. PMMA and PVDF are both solids, while all the others are liquid at room temperature. We analyze the noise spectra at frequencies ranging from 1 kHz − 10 MHz, typical in experiments, and compare the results with experimental **data** of Ref. [32]."
207778550,5/48,1.0,0.104,"In addition to the discussion in Sect. II, expositions on nuclear **data** evaluation methodology during preceding decades can be found in a monograph by Smith [3], in review papers by Capote et al. [22] and by Smith and Otuka [23], and elsewhere, e.g., as mentioned in a comprehensive report of the NEA WPEC SG-24 [24]."
216162633,2/6,5.0,0.333,"The asymptotic behavior of the global solutions with E 0 ≤ d is unknown, as well as the characterization of the qualitative behavior for E 0 > d in terms of the initial data. We have to mention two remarkable papers, [18,27] where the blow up of solutions for the Cauchy problem associated to (NLW) is studied, that is with Ω = R N . In those works a nonlinear damping term of the form g(u t ) ≡ δ|u t | λ u t is considered and it is proved that for any (η, ξ) ∈ (0, ∞) × [0, ∞), there are infinitely many compactly supported initial **data** (u 0 , u 1 ) such that if ∇u 0 2 = η and E 0 = ξ, the corresponding solution blows up in finite time. We notice that this result is not true for the initial boundary value problem (NLW) with that nonlinearity g(u t ). Indeed, for ξ < d and η 2 < 2rd α(r−2) the solution is globally defined, see [8], because u 0 ∈ W if and only if α ∇u 0 2 2 < 2r r−2 d. However, the result in [18,27] exhibits a complex dynamics because it is known that there are global solutions for the Cauchy problem associated to (NLW) with initial **data** such that η and ξ are small, see [27] and references therein."
253553223,23/28,1.0,0.821,"Lemma A.7 (Sampling error of empirical covariance operators). Assume that X ∈ L ψ 2 (P; X ), Y ∈ L ψ 2 (P; Y), and that the **data** pairs (X i , Y i ) n i=1 ∈ X × Y are sampled i.i.d. from the joint law L (X, Y ). Then, for all δ ∈ (0, 1 2 ] and n log(1/δ), the bound C YX − C YX S 2 (X ,Y) 24 √ 2e Y L ψ 2 (P;Y) X L ψ 2 (P;X ) log(1/δ) n holds with P ⊗n -probability at least 1 − δ. An analogous bound holds for the sampling error of C XX when Y L ψ 2 (P;Y) is replaced with X L ψ 2 (P;X ) in the right-hand side of the inequality."
17837291,17/52,1.0,0.327,"Since not all a q are zero and the functions e H ζ (q) are linearly independent, it follows that
k is odd. Now the coefficient of α −k+1 becomes q e H ζ (q) (2b q − a q H ζ , d ) = 0.
Again by linear independence of the exponential functions, it follows that a q = 0 = b q for all q, which is a contradiction. 
A ′ d − B ′ d = X ω(α)e H ζ + ω(−α)e H ζ + H ζ ,d α .
Since both A ′ d , B ′ d have coefficients which are polynomial in α, this shows that the class ω(α) has property (b) of the preceding lemma. Suppose A, B both come from Euler **data** Q, P respectively, ie. 
A d = i * 0 Q v d and B d = i * 0 P v d . Suppose also that j * 0 (P d )| q = j * 0 (Q d )| q at α = λ/δ.T q X. (ii) deg α (A d − B d ) ≤ −2.
Then A = B."
118656364,25/51,1.0,0.49,"In this section we study the simplest non-abelian case, corresponding to two M5-branes wrapped on S 2 , or equivalently we study the reduction of the 5d U (2) theory to 4d on an interval with Nahm pole boundary conditions. The flat 4d theory is given by a map into the 2-monopole moduli space M 2 , with the action given in (4.30). For the curved space theory we find a description in terms of a sigma-model into S 1 × R ≥0 supplemented by self-dual two-forms obeying some constraints. We provide a detailed analysis of the geometrical **data** 9 The factor i is due to our conventions in which A θ is purely imaginary. 10 These transformations correspond to gauge group elements g = e iα(θ) with α(0) = 0 and α(π) = 2πn. The quantization of n is required for g to be trivial at the endpoints of the θ interval."
55185767,9/16,1.0,0.562,"We will now show that the subset M K,ψ,x,r of U K,x,r consisting of those y satisfying both inequalities (4.6.5) sup
ℓ,j,i |G ℓ,j,i,K,ψ (x, r)| C ≤ sup ℓ,j |f ℓ,j,K,ψ (x, y)| C ≤ sup ℓ |f ℓ,K,ψ (x, y)| C
has big volume in the sense that (4.6.6) Vol(U K,x,r ) ≤ q d+1 K Vol(M K,ψ,x,r ). Once this is proved, we are done for our original family (f ℓ ) ℓ by replacing d with d + 1 while keeping the **data** of the ϕ, G ℓ,j,i , and h ℓ,j,i ."
235125570,19/29,1.0,0.655,"
1.4)  in 1D with the initial **data** Φ 0 (x) = (cos(x), cos(x)) T , electromagnetic potentials V (x) = 1/(1 + sin(x) 2 ) and A 1 (x) = (1 + sin(x))/(2 + cos(4x)). The same mesh size h = π/8 and time step τ = 10
"
38256305,2/49,1.0,0.041,"Pore pressure diffusion is considered to play a key role in controlling the occurrence of earthquakes associated with Koyna and Warna reservoir [8][9][10][11][12][13][14][15][16][17][18][19]. Hence, the prevalent fault structure and tectonic setting in the region that influence seismicity need further investigation. The detailed inventory preparation of the near-surface fault and fracture pattern is likely to help detect relatively high permeable areas that allow intrusion and infiltration of surface water. Towards this end, satellite **data** were harnessed to detect near-surface fault and fracture zones and to understand the structural pattern via visual lineament analysis. The Sentinel satellite radar images available since 2015 from ESA are assumed to delineate sub-surface structures whose subtle morphological details are enhanced, which were earlier lacking in detail, due to radar backscatter of the surface. Landsat 8 **data** available since 2013 are also seen to show improvement when compared to previous Landsat missions because of enhanced radiometric and thermal resolutions. One of the goals of the present paper was to investigate whether additional structural/tectonic information can be gleaned from these newly-available satellite data."
87345260,7/13,1.0,0.538,"Following the expected behavior in an optimal condition, fi rst I evaluated the index. I considered the best index as the one that recovered most times the same original ranking -fi rst to third areas-, as an ordered ranking. Then, using the selected index, I evaluated the best area , as the one found most often in the fi rst place. I tested six scenarios by modifying j.topol and j.tip values as follows: j.topol values of 0.50 and 0.32, and j.tip values of 1, 0.50 and 0.32. These values are just used to introduce the concept , but they are similar to strong, mild and relaxed tests. A value of 1 to delete a species means that all areas for that species will be deleted, while a value of 0.32 means that one out of three will be deleted. Smaller values as 0.01 are discarded, it would make no difference, as the perturbation to the **data** would be unimportant."
17837291,32/52,1.0,0.615,"Consider the following example: the concave bundle V = O(−6) over P 3,2,1 , Ω = 1 6H . This example will be studied in our subsequent paper by using resolution of singularities. This is an example of ""local mirror symmetry"" studied in physics [33]. The mirror formula there can derived as a special case of our general result. In fact, the Euler **data** which computes the K d in this case is determined by
j * 0 P d = 6d−1 m=1 (−6H + mα).
The corresponding equivariant Euler class, after taking nonequivariant limit with respect to the T action, is:
e G (X 0 /W d ) = d m=1 (H − mα) 2d m=1 (2H − mα) 3d m=1 (3H − mα).
The corresponding hypergeometric series and Picard-Fuchs equation can be immediately written down. It turns out that the hypergeometric series gives the periods of a meromorphic 1-form for a family of elliptic curves [33]."
201251303,2/10,2.0,0.2,Step 1. Generate **data** with = 500
17228594,3/7,1.0,0.429,"From the P * P correlator one can extract the static quark-antiquark potential V (r) by V (r) = − 1 T ln P * P(r) . We prefer to look at the first and the second derivative of this potential for the force between the quark and the antiquark and information about sub-leading terms. To facilitate our comparison with string models we actually compute a scaled second derivative which we call c(r). This quantity is expected to become the Lüscher term (= − (d−2)π 24 ) asymptotically. On the lattice these are given by
F(r) = V (r) −V (r − 1) & c(r) =r 3 2 [V (r + 1) +V (r − 1) − 2V (r)] (3.1) wherer = r + a 2 + O(a 2 ) andr = r + O(a 2 )
are defined as in [9] to reduce lattice artifacts. For large distances, we are going to concentrate on the potential due to the NG string, the so called Arvis potential [16] given by V Arvis 
= σ r 1 − (d−2)π 12σ r 2 1/2
. We will define the L.O. and N.L.O. approximations by retaining 1/r and 1/r 3 terms in the potential respectively. We will compare our lattice **data** on force and c(r) with predictions from leading order, NLO and from the full Arvis potential."
16212589,3/6,3.0,0.5,"These **data** were not corrected for the effects of slow inactivation, which was still present in this preparation. We measured slow inactivation as described above for the wild-type channels by determining peak current responses to strongly depolarizing pulses (e.g., to 0 mV) after several minutes at various holding potentials. Fig.  5 B shows an example of such a slow inactivation curve. Slow inactivation caused diminution of the steady currents ranging from ~0.9 at -80 to ~-,0.4 at -60. Note that correction for such slow inactivation would further steepen the limiting slope. The net effect of such correction would be to add another effective charge to the 12 estimated above."
118661060,4/17,1.0,0.235,"Much more remarkable in the figure is qualitative differences between the ONe core and the Fe cores. Among other things, the Fe cores emit a substantial amount of neutrinos from much earlier on than the ONe core. This is simply due to their higher temperatures as mentioned earlier. The electron-positron pairs are more abundant, producing neutrinos copiously. In fact, the ONe core is so cold until the electron capture on Mg is opened that electrons are strongly degenerate and, as a consequence, positrons are scarce, making the neutrino luminosity from the pair annihilation negligibly small. Unfortunately, the plasmon contribution is also tiny as long as the temperature is very low, since only a small amount of plasmons are thermally populated then. It is hence understandable that the neutrino luminosity becomes substantial in the ONe core only after the electron captures and the subsequent burnings of oxygen and silicon heat it up, which occur less than a second before collapse. Even after the formation of the NSE region (T 5 × 10 9 K) inside the core, the number luminosity of the ONe core is much smaller than those of the Fe cores. As a matter of fact, ∼ 10 53 neutrinos are emitted in the 15 M ⊙ model while in the 8.4 M ⊙ model the number of emitted neutrinos is only ∼ 10 51 . Note, however, that the luminosity increases very quickly in the last few hundred milliseconds in the ONe core and It is still increasing rapidly at the end of our calculation. At this point the central density is 10 11 g/cm 3 and no more **data** are available from the stellar evolution calculation. Unlike the Fe cores, the ONe core is rather slowly contracting even at this point, with the NSE region being expanding gradually. We hence expect the luminosity continues to rise further until neutrinos are trapped inside the core, which will be somewhat delayed, since the neutrinos considered here have smaller energies (∼ 5 MeV) than those produced by electron captures during collapse (∼ 10 MeV)."
246863515,3/44,4.0,0.068,"Note that Equation (5) still allows the distributions of potential long-term outcome Y (a) in the experimental and observational **data** to be different, so that the long-term treatment effect on the experimental population can be different from our target. Moreover, in Section 6 we further relax eq. (5) to allow the distributions of covariates X to be different in the two samples.
A Y S X U s (a) Short-term confounders U s . A Y S X U o (b) Outcome confounders U o . A Y S X U s U o
(c) Short-term confounders and outcome confounders coexist."
18538029,1/9,1.0,0.111,"The Hubble expansion, the Cosmic Microwave Background Radiation(CMBR) and the primordial Big Bang Nucleosynthesis(BBN) are three cornerstones of the standard hot Big Bang cosmological model. Recently these three kinds of observations have been making a strong case for the existence of a nearly uniform component of dark energy with negative pressure: More than twenty experiments of CMBR measurements have now covered three orders of magnitude in multipole number, and are beginning to define the position of the first acoustic peak at a value that is consistent with a flat universe [1]; However, the deuterium abundance measured in four high redshift hydrogen clouds seen in absorption against distant quasars [2] combined with baryon fraction in galaxy clusters from X-ray **data** [3] give a low matter density universe, Ω m ∼ 0. 3-0.4; The discrepancy between the low matter density and a flat universe is observationally resolved by high redshift Ia type supernovae (SNeIa) observations [4], which implies an accelerating universe driven by a dark, exotic form of energy component. For a recent review of observational evidence for the dark energy component, reader is referred to Ref. [5]. It seems that determining the amount and nature of the dark energy is emerging as one of the most important challenges in cosmology."
207778550,32/48,5.0,0.667,"An example of dealing with α-counting experiments illustrated how the clues can be used to identifying noncorrelated USU effects and approaches to estimating them. Another example explored the large scatter in calculated versus experimental criticality values of metal, fast neutron spectrum, highly enriched uranium criticality benchmarks. It was discussed in detail that in all but one case no obvious physics reason for the spread in the **data** nor missing uncertainties of the size of the scatter could be identified. Hence, it was recommended that USU effects could account for the unknown spread in these **data** until it is resolved."
1520137,20/35,1.0,0.571,"We study how visual perception of a target bar can be biased by contextual bars in the image, and how a Bayesian model of object inference can account for the data. Human observers are more likely to perceive a target bar when the contextual contrast, i.e., the luminance difference between the contextual bars and background, is weaker rather than stronger. Relative to the situation without the context, they are biased to perceive the target in a context of weak contrast when the target can perceptually group well with the context, as if the context fills in the target. Meanwhile, they are biased not to perceive the target in a context of strong contrast, as if the context suppresses the perception, regardless of whether it could perceptually group well with the would-be target. The Bayesian model illustrates that the context influences the perception by biasing (1) observers' prior belief that a target should be present and (2) observers' internal model of the likely input contrasts from a target bar. Our **data** suggest that brain areas beyond the primary visual cortex along the visual pathway are responsible for inferring object causes for input images."
17837291,20/52,2.0,0.385,"Remark 6.6. The preceding theorem says that one way to compute A (or Q) is by first finding an explicit Euler **data** B linked to A, and then relate A and B via mirror transformations."
119354744,2/4,2.0,0.5,"Finally, on Fig. 1c we show fits to the recent NA49 **data** [16] on π − production in P bP b collisions at three different energies (per nucleon). The values of (q; GeV. The origin of q > 1 in this case is not yet clear. The inelasticity seems to grow with energy. It is also obvious that for higher energies some new mechanism starts to operate because we cannot obtain in this case agreement with **data** using only the energy conservation constraint. The possibility offered in this matter by some closer inspection of properties of eqs. (3) and (7) are displayed in Fig. 2. Whereas upper panels are self-explanatory, the lower demand some attention. They show how p(y) can be distorted and it could be that one of the mechanisms presented there is showing up in high energy NA49 **data** mentioned above. Three mechanisms are listed there (only q = 1 case is considered). Fig. 2d shows the changes in p(y) introduced by y-dependent p T , especially when p T grows substantially towards y = 0 (in such a way as to keep the averaged p T the same as original 0.4 GeV/c). In this case one observes increase of p(y) at small values of y but it seems to be too wide to explain the NA49 data. Fig. 2e shows result of the special kind of momentum dependent residual interactions discussed in [20]. In our case it would result in with additional constraint imposed now on the modulus of the momenta:
K q = 3 · K (π − )p(y) = 1 Z · exp [−β · µ T cosh y − λ · µ T | sinh y|](11)Y m −Y m dy µ T | sinh y| p(y) = κ · √ W 2 − M 2 N .(12)
Here W = K · √ s and M = i µ 2 T + p 2 Li with p Li = µ T sinh y i (sum is over all produced particles, i = 1, . . . , N , and κ is parameter telling us the fraction of momentum turned into interaction). As it can be seen in Fig. 2e, the effect of this interaction although visible is very weak and probably negligible at present data."
15394341,4/18,1.0,0.222,"We performed a synthetic test to compare objectively the detection performance of FAST against a reference autocorrelation code. Our synthetic **data** consisted of scaled-down earthquake waveforms inserted at known times into noisy seismic **data** (7,8), which provides ground truth."
5227101,1/10,2.0,0.1,"Two focal PSF images for the rotation angles of highest and lowest central signal, respectively, (Fig. 5k) confirm the disappearance of central intensity as predicted by the simulations (see Fig. 2a-e). The quality of the 'doughnut' PSF, resulting from the astigmatism-corrected vortex phase (Fig. 5e), is further increased. Figure 5i and l illustrate the application of the 'six segments' PM for measurement of trefoil aberration. The shown PSFs were measured at the same rotational angle of the PM, but with different amounts of bias trefoil for expansion and compensation of wavefront distortions, respectively, leaving the right-hand side PSF image without central intensity. Intensity **data** for coma sensing are presented in Fig. 5j and example images of involved PSFs are shown in Fig. 5m."
118139389,7/13,4.0,0.538,"the **data** for low mass black holes, but still produces galaxies with a slightly higher stellar mass. For high mass black holes there is again an offset similar to Figure 4 with the galaxies having a higher stellar mass and following a linear relation."
119080499,18/30,1.0,0.6,"We performed a finite size calculation with lattice size of 15 × 15 × 15a 3 0 based on the model Hamiltonian (17). Numbers of states in the band structure of LaH 3 and LaH 2 are counted as a function of energy. The discrete densities of states were smoothed out by replacing delta peaks with Lorentzian distribution curve of width Γ = 0.3 eV. Our results for LaH 2 and LaH 3 are similar to the previous theoretical calculations by Gupta and Burger [36,38]. In Fig. 15, the pronounced two-peak structure is associated with the flat region of hydrogen bands near symmetry points X, L, K (refer to the band structure Fig. 9 and 10) while the small bump at the zero energy in LaH 2 is associated with the La band near symmetry points W and K. Experiments showed that the small bump shrinks as hydrogen concentration increases and eventually disappears when the sample approaches the trihydride. However Gupta and Burger expected a metallic state for LaH 3 from their LDA calculation which ignored electron correlation. In our calculations, the density of states goes to zero at −2.2 eV, indicating an insulating behavior for LaH 3 . However, the calculated widths for LaH 2 and LaH 3 are considerably smaller than the experiment results, a discrepancy similar to that in the LDA . The experimental **data** also shows a shift of the lowest energy peak when the concentration increases. The positions of calculated peaks do not significantly depend on the concentration and does not agree to the experiment. The origin of these discrepancies is not clear. Since our theory uses some parameters extracted from LDA, similar features except for the band gap may be expected from both theories."
249258070,31/119,2.0,0.261,"Collectively, there were ten categories of police reactive and proactive activities coded from the data, and these categories make up the dependent variables for the current study. Police reactivity to CFS were coded into seven different categories using an approach consistent with previous research utilizing CFS **data** (see Wu & Lum, 2017). Violent crimes included CFS involving reported shootings, robbery, and several types of assaults. Property offenses was a category comprised of burglary, theft, and forgery type offenses. Disorder incidents consisted of an array of disturbances, including general, family, and noise CFS. Suspicious incidents included calls where an alarm was sounded (e.g., vehicle alarm set off) and reports involving a suspicious person or vehicle. The trafficrelated activities category comprised CFS for traffic accidents, driving while intoxicated (DWI), and other traffic-related issues (i.e., road rage). Service-related activities included calls to assist first-responders, including other officers, fire fighters, or emergency medical services. Lastly, the non-crime events category consisted primarily of assisting specialized units with transporting individuals and responding to silent 911 calls."
216162633,2/6,6.0,0.333,"For the (GB) * problem, characterizations of blow-up and global solutions with E 0 < d by means of the potential well method were proved in [21,22,16] and in [34], for the one-dimensional and the multidimensional problems, respectively. For E 0 ≥ d, there are only partial results on the dynamics of particular equations of the type (GB) * . Most of them are results for blow up of solutions, see [29,25,16]. Similar analysis, for E 0 ≥ d, have been performed to prove the same qualitative properties for other equations, see [14,28] to cite just some of the most influential papers on the subject, and see [4,19,20,26,30,31,35,36,37] and references therein for some recent works. In [6] a numerical study for the Cauchy problem of the focusing cubic nonlinear Klein-Gordon equation in three dimensions for radial initial **data** shows that for high energies, the qualitative behavior seems to be much more complicated than for E < d, and more research is required to find a threshold between globality and blow up of solutions. In [17] some numerical experiments were performed for the one-dimensional case of (GB) * , with δ = 0, to calculate the mountain pass level and confirm the range of validity of the potential well method. In particular, for E 0 > d, it was observed that the sign of I(u 0 ) is irrelevant to get the blow up of solutions in finite time, but instead, the sign of the inner product P(u 0 , u 1 ) is important to obtain nonexistence of global solution. In [11,12] we have proved that this is true for the undamped nonlinear Klein-Gordon equation and for the problem (P) with δ = 0. Here, we are going to prove that, for any δ > 0 and any positive E 0 , the sign of P(u 0 , u 1 ) is the main ingredient to prove the nonexistence of global solutions for problem (P) and in particular for (GB) * . However, we shall consider δ ≥ 0, in order to see the influence of the damping in the qualitative behavior."
207778550,45/48,1.0,0.938,"table. TABLE IV. Estimatesμ of the cross section ratios and uncertainties at 9 MeV and 10 MeV derived from the **data** listed in Table XI of Appendix VI A. The results derived by different order polynomials (M ) to model the energy dependence of the cross section ratio are compared.9 MeV 
10 MeV 
Dataset 
uN,i(%)μi (uμ i ) uμ i (%) ziσ δ,iσδ,i (%)μi (uμ i ) uμ i (%) ziσ δ,iσδ,i (%) 
01 -Tovesson et al. [85] 
0.84 
0.5792 (49) 
0.85 1.25 0.00371 0.64 
0.5842 (50) 
0.86 1.17 0.0031 
0.53 
02 -n TOF (1) [86] 
1.73 
0.5843 (140) 2.40 0.80 0.0 
0.00 
0.6005 (143) 2.38 1.55 0.0169 
2.82 
03 -n TOF (2) [86] 
2.49 
0.5659 (209) 3.69 -0.34 0.0 
0.00 
0.5938 (216) 3.64 0.72 0.0 
0.00 
04 -n TOF (3) [86] 
3.35 
0.5683 (208) 3.66 -0.23 0.0 
0.00 
0.5782 (212) 3.67 -0.01 0.0 
0.00 
05 -n TOF (4) [86] 
3.74 
0.5565 (241) 4.33 -0.67 0.0 
0.00 
0.5496 (253) 4.60 -1.13 0.0136 
2.47 
06 -Behrens et al. [87] 
0.81 
0.5630 (60) 
1.07 -1.66 0.00805 1.43 
0.5709 (63) 
1.10 -1.18 0.0039 
0.69 
07 -Difilippo et al. [88] 
2.39 
0.5692 (138) 2.42 -0.28 0.0 
0.00 
0.5713 (140) 2.45 -0.50 0.0 
0.00 
08 -Cierjacks et al. [89] 
1.09 
0.5604 (110) 1.96 -1.14 0.00614 1.10 
0.5670 (99) 
1.75 -1.15 0.0056 
0.98 
09 -Coates et al. [90] 
2.92 
0.5775 (176) 3.05 0.25 0.0 
0.00 
0.5720 (177) 3.09 -0.36 0.0 
0.00 
10 -Shcherbakov et al. [91] 
2.45 
0.5618 (142) 2.53 -0.80 0.0 
0.00 
0.5662 (144) 2.54 -0.84 0.0 
0.00 
11 -Lisowski et al. [92] 
0.94 
0.5806 (64) 
1.10 1.18 0.00396 0.68 
0.5824 (68) 
1.17 0.60 0.0 
0.00 
Evaluated values µ (uμ) 
0.5731 (28) 0.49%σ δ = 0.0035 (0.6%) 0.5783 (29) 0.50%σ δ = 0.0069 (1.2%) "
238834501,8/19,1.0,0.421,"Figure 5 .Figure 6 .
56T estimates for stations in the Coquimbo Region with at least 1 year of available **data** at a resolution of at least 15 min. For lasso regressions (average and smoothing), uncertainties are given as 2.5 and 97.5 percentiles of the test errors separately, while for ARD regression, one value of 1.96 times the standard deviation of the predictive distribution is given. Lasso estimates are corrected for the bias of the error distributions. Errorbars on plots correspond to these ranges. 'Time' is the time of maximum T for each type of estimate; ' center' is the distance from a station to the eclipse's center line; ' coast' the distance to the coast. 'N days' is the number of available days on record, and 'DOF' are the degrees of freedom (number of regression coefficients) for each estimate. Vector wind as a function of distance along (x) the path of the eclipse and from the center line (y). Eclipse wind is the observed wind vector minus the reference estimate. The x distance is calculated from observation time assuming an umbral velocity of 2000 km h −1 ; the positive direction points west to east as usual since the eclipse's movement is also in this sense. The field is smoothed with biquadratic radial basis functions. While the x, y locations are non-square, the arrow lengths are isotropic; red arrows show the scale of the wind with u = v = 1 ms −1 .
"
53380409,4/119,1.0,0.034,"The **data** sample was collected by the ATLAS detector during the pp collision running of the LHC at ffiffi ffi s p ¼ 13 TeV in 2015 and 2016. Events were selected for the different channels with various triggers, as described in their respective papers [9-18]. Channels featuring charged or neutral leptons were selected with single or multiple electron and muon triggers with various p T thresholds and isolation requirements or with missing transverse momentum triggers with varying thresholds. A high-p T jet trigger was used in the fully hadronic channels. After requiring that the **data** were collected during stable beam conditions and with a functional detector, the integrated luminosity amounts to 36.1 fb −1 ."
119479526,19/29,1.0,0.655,"Figure 5 :
5The Bayesian model prediction (solid blue line), the results of previous Hf assessments (dashed lines) and the experimental **data** points (given by symbols in the legend) versus temperature.
"
235727288,40/46,1.0,0.87,"Figure 6 :
6Evaluating the choice of q for SMC. Since the scale of the likelihoodπ 1 depends on the number of **data** examples, we expect the numerical stability of q-paths to vary by N . While the minimum q yielding a stable estimator (orange) increases with N , the best performing
"
98546104,10/11,1.0,0.909,"
the enthalpy change of fusion for the i i component. A first estimation of the EGC curve can be done w x from the experimental **data** of the a q L equilib-Ž rium. An iterative procedure performed by means of w x the WINIFIT program 21 based on Oonk's method w x. E Ž . 20 enables one to obtain reasonable DG X EGC w x values. This procedure was applied to the R q L w x and FCC q L equilibria.
"
248496020,1/23,1.0,0.043,"Moreover, the two processes L and R subject to L 0 = R 0 = 0 increase only when X hits its boundary a and b, and  where I(·) is the indicator function. For more about reflected stochastic processes one can refer to [12]. Assume that the process X is observed at regularly sapced time points {t k = kh, k = 0, 1, · · · , n}. Let θ 0 ∈ Θ be the true value of the parameter θ. We are aiming to study the nonlinear least squares estimator (NLSE) for θ 0 based on the sampling **data** {X t k , L t k , R t k , k = 0, 1, · · · , n}."
117119864,79/83,1.0,0.952,"The spectra presented in this paper are but a hint of what lies ahead for photoemission studies of electrons in condensed matter. Much can be done with the present capabilities of the technique and the prospects are very good for the further technical advances that are needed. The varied scenarios whereby the beautiful Fermi liquid paradigm is realized even in strongly correlated systems are being elucidated with unprecedented clarity and detail, and the appreciation for the Fermi liquid gained thereby only serves to heighten the great intrigue of exploring interesting ways in which the paradigm can fail. experimental verification of the ""dense impurity ansatz"" for a small TK cerium material. The slight decrease of the sideband and the larger decrease of the EF intensity with dilution are due to a known decrease of TK arising from volume expansion. 8. TL model spectra from theory of Ref. [126] in panels (b) to (f) compared to Li0.9Mo6O17 ARPES **data** of panel (a) and Fig. 7 (c) to show sensitivity of Tomonaga-Luttinger description of **data** to choice of ratio of velocities of holon peaks and spinon edges. Holon peak dispersion is held constant and matched to experimental peak dispersion for ease of comparison of spinon edge dispersions. The lineshapes of panel (d) and Fig. 7 (d) provide an excellent description of the data."
52910603,25/27,1.0,0.926,"
) and laser-off (trials) as well as the respective indifference point for each condition. (B), Transitivity plots are shown for the blocked (left) and patent (right) fiber types. Transitivity measures represent the consistency of food pellet preferences across six sessions in which animals experienced each of the possible pairs of three pellets (A-C,A-B,B-C) for both fiber-type conditions. Stable and consistent preferences should be in accordance with the relation: IP A:C » IP A:B Â IP B:C which can be visualized in the scatterplots as points falling close to the identity line. The Y-and X-axes show the IP, in log scale, for the pellet pair with the largest preference difference, IP A:C , and the product of the IPs from the other two pairs (IP A:B Â IP B:C ). The within-session pairs of laser-on trials (yellow symbols) and laser-off trials (grey symbols) are indicated by a dark grey line connecting the scatterplot pairs. Histograms comparing laser-on and -off trials are shown in the background for each of the plots. The six sessions shown in (A) are marked with a red outline on the respective scatter points in (B) Data is provided in Supplementary file 3 andFigure 5-source **data** 1. (C) The distances between within-session pairs (the lengths of the grey lines connecting the scatterplot pairs in (B) plotted as the cumulative empirical distribution function for each of the fiber-type conditions (blocked: yellow, patent: brown). For **data** seeFigure 5-source **data** 1. DOI: https://doi.org/10.7554/eLife.38963.011The following source **data** is available for figure 5:Source **data** 1. Text File Containing the Source Data forFigure 5with the Same Layout as in Supplementary file 3. DOI: https://doi.org/10.7554/eLife.38963.012
"
207778550,34/48,3.0,0.708,"In the second step, minimum realistic and conservative bounds are quantified for each sub-process by taking into account whether an uncertainty applies to all measurements (e.g., PFNS uncertainty) or is reduced because it is not fully correlated between measurements (e.g., statistical uncertainties). Also, uncertainties applying to all types or only two out of three types of measurements are quantified. A total minimum realistic and a conservative upper PUBs bound for each measurement type is quantified and shown in Fig. 14. It is noteworthy that the conservative upper PUBs bound encloses at least 66% of the mid-points of the experimental **data** indicating that these bounds are realistic within their respective group, consistent with Clue 3."
146120626,5/8,1.0,0.625,"If probing research leadership is the objective, using gh L is much simpler than using h α because its calculation is based on the easily determined position of an author in the list of co-authors. An important restriction is that the author position needs to contain information about author's role in a collaboration which is not always the case. If applicable in a given field of research, choosing the last author instead of the one with the highest h as the leading author can have several advantages. Some have already been mentioned, for example the presence of high-h co-authors on a paper who are simply on the author list because they have contributed to the funding of the project [10], a practice that is not desirable but common. Another example could be a collaboration involving several groups, all lead by senior people with a high h. Even if the project is lead by one individual, this would not necessarily be the person with the highest h, especially if the groups cover different sub-disciplines. In condensed matter physics, for instance, density functional theory is a sub-field of huge impact and therefore high citation numbers and h-indices [21]. A senior individual working in this field would be likely to out-α the other co-authors. Note that both gh 1 and gh L contain single author papers. If gh L is to be used as an indicator of research leadership, it is not obvious that single author papers should be included. On the other hand, single author papers are quite rare (less than 3% of the total for our **data** set), and we can therefore ignore this issue here."
251500724,8/26,1.0,0.308,"The fingerprint of the transition appears very clearly in Figure 3 as the sudden, dramatic increase in the viscoelastic moduli below a critical temperature. It is worth noting that the sample shows a Newtonian rheological behavior at the loading temperature of 70 °C. This Newtonian behavior at 70 °C was confirmed by steady-state measurements in Couette geometry, which are not reported here for brevity. The corner point of Figure 3a, where the complex viscosity is reported, indicates that the sol-gel transition begins at a temperature of about 36 °C. At a slightly lower temperature (about 32 °C, see Figure 3b), the elastic modulus overcomes the loss modulus, suggesting that the material changes its behavior from liquid-like to solid-like. The **data** in Figure 3 determine a first, rheology-related process indication, as far as the extrusion temperature is concerned. According to the kC's viscoelastic response upon cooling, an excessively high extrusion temperature would result in the production of a purely viscous, low-viscosity, structureless filament, with obvious negative consequences on the structural consistency of the construct. Conversely, an excessively low temperature would shift the kC material into the solid-like region, thus making the generation of a smooth, continuous filament impossible."
246863515,24/44,1.0,0.545,"Under the weaker conditions in Assumptions 10 and 11, Corollary 1 and theorem 7 shows that we need more complex identification strategies for the average long-term treatment effect over the observational **data** distribution. Actually, even in this case, the simpler identification strategies in Sections 4.1 to 4.3 are still useful. Below we show that under an additional assumption, they can identify average long-term treatment effect over the experimental **data** distribution."
87345260,5/13,1.0,0.385,"In conservation biology, there must be a measure of the confi dence and robustness of the results. A sensitivity analysis , deleting at random part of the information, helps to understand the support of the **data** as the persistence of a given area in the ranking. Therefore, jack-knife is the appropriate tool to explore the behavior of the results to perturbations in the **data** set (Holmes 2003 )."
117119864,51/83,1.0,0.614,"The spectra presented in this paper are but a hint of what lies ahead for photoemission studies of electrons in condensed matter. Much can be done with the present capabilities of the technique and the prospects are very good for the further technical advances that are needed. The varied scenarios whereby the beautiful Fermi liquid paradigm is realized even in strongly correlated systems are being elucidated with unprecedented clarity and detail, and the appreciation for the Fermi liquid gained thereby only serves to heighten the great intrigue of exploring interesting ways in which the paradigm can fail. experimental verification of the ""dense impurity ansatz"" for a small TK cerium material. The slight decrease of the sideband and the larger decrease of the EF intensity with dilution are due to a known decrease of TK arising from volume expansion. 8. TL model spectra from theory of Ref. [126] in panels (b) to (f) compared to Li0.9Mo6O17 ARPES **data** of panel (a) and Fig. 7 (c) to show sensitivity of Tomonaga-Luttinger description of **data** to choice of ratio of velocities of holon peaks and spinon edges. Holon peak dispersion is held constant and matched to experimental peak dispersion for ease of comparison of spinon edge dispersions. The lineshapes of panel (d) and Fig. 7 (d) provide an excellent description of the data."
207778550,6/48,3.0,0.125,"While evaluators are limited in what they can do if they consider only reported experimental **data** uncertainties, which often are incomplete, progress can be made if they are able to supplement this information with estimates of required uncertainties and correlations obtained from templates of suggested values generated through the collective experiences of other experimenters and evaluators in the field, e.g., see [59][60][61][62][63]. This may lead to an evaluator choosing either to alter the uncertainties provided in the literature for certain **data** or to outright reject those **data** that, in their opinion, are of such poor quality (e.g., seriously discrepant) that their inclusion in the evaluation process would lead to unacceptable distortions in the evaluated results (e.g., see the case of Staples **data** [64] in 239 Pu(n,f) Prompt Fission Neutron Spectra (PFNS) evaluation [65])."
207778550,31/48,1.0,0.646,"Neutron capture cross-section measurements made using the activation technique are sometimes performed using rather thick samples, thereby requiring corrections for neutron multiple scattering and interfering reactions. The scattering of incident neutrons in these samples reduces the average neutron energy. In some cases, when the sample is a hygroscopic chemical compound that can absorb a substantial amount of water impurity, the effect can be large. This results from scattering on hydrogen and oxygen in the sample that also will significantly reduce the average neutron energy (due to light hydrogen and oxygen atoms assuming considerable recoil energy). Furthermore, due to the increase of capture cross sections at lower neutron energy, this will effectively increase the sample activation and the cross section derived from the **data** if left uncorrected. However, while correction for this effect is possible in principle, it requires knowledge of the exact amount of water in each sample. This is usually very difficult, if not impossible, to determine. Also, for obvious reasons it could be variable during the course of the experiment."
202192488,6/8,1.0,0.75,"Figure 2 .
2b and p values along the Futagawa-Hinagu FZ. (a) Δb = b post − b pre as a function of latitude. The number for each **data** point corresponds to the region number shown in d. Dashed line represents Δb = 0. Yellow and red stars show latitudes of epicenters of mainshock and M6.5-class foreshocks, respectively; open circles and squares indicate M5-class aftershocks and foreshocks, respectively, along the Futagawa-Hinagu FZ. Differences in b in inset of Figure 1b correspond to Δb for Region 6. (b) log P b for Δb in a. Dashed and dotted lines indicate log P b = −1.3 and −2.6, respectively. (c) p values (circles) with their uncertainties (vertical bars) for different lower-cutoff magnitudes as a function of latitude. Open circles show p values obtained based on N ≤ 20 earthquakes. Dashed line represents p = 1. The p value of M ≥ 3 was not plotted for Region 9 because the model-fitting analysis did not converge. (d) Map showing Regions 1-9 (red rectangles). Region 6 corresponds to the region shown in Figures 1a and 1b. FZ = fault zone.
"
52995678,21/29,1.0,0.724,"These results suggest the following unexpected conclusion: Using Lagrangian **data** to construct ROMs might prove beneficial even in approximating Eulerian fields. Figure 6: Mean streamfunction from G-ROM (first row), λ−ROM (second row) α−ROM with α = 1 (third row), α = 10 2 (fourth row), and α = 10 4 (fifth row), for r = 10 (first column), r = 15 (second column), r = 20 (third column), and r = 30 (fourth column). 24"
16212589,3/6,7.0,0.5,"Although our measurements for these Na § channels extend the measurable range and show a higher limiting slope, they do not contradict previous estimates. As indicated above, our **data** also show a slope of six charges in the range of 0.1 to 0.001, where most previous estimates of limiting slope were derived. For skeletal muscle Na + channels, however, this range is not sufficiently negative to reach the true limit."
52141668,6/16,1.0,0.375,"Using the three classifiers, we obtained the best result using Random Forest with 80.29% accuracy and 74.12% for F-measure as can be seen in Table  2. The imbalanced dataset made most of the **data** to be classified into the majority class, which is the   In order to handle the imbalance **data** problem, we also try to apply two resampling techniques, Synthetic Minority Over-sampling Technique (SMOTE) for increasing the minority classes and Random Under-sampling (RUS) for decreasing the majority classes in training data. By applying the two resampling techniques, we manage to increase the ability of the classifiers in detecting deception. However, it also decreases the ability in detecting truth as well. This causes the F-measure score for each classifier to decrease as can be seen in Table  3."
247627894,9/15,1.0,0.6,"By the Neyman-Pearson Lemma and the Likelihood-Ratio Trick, the test statistic with the optimal power for a test of H 0 : p(x|θ 0 ) and H 1 : p mix (x|S c ) can be found by optimizing the binary cross-entropy:
L BXE (φ|S c , θ 0 ) = −E (y,x∼p(x|θ 0 ),p mix (x|Sc) [(y log s φ (x) + (1 − y) log(1 − s φ (x))],(8)
where samples from θ 0 are labeled y = 0 and samples from the mixture of alternatives are labelled y = 1. To optimize the global power from Equation 6 in a likelihood-free way we can now average over S c and θ 0 to define a global loss L(φ)
L(φ) = E Sc,θ 0 [L BXE (φ|S c , θ 0 )](9)
given choice of densities p(θ alt |S c ), p(S c |θ 0 ), p(θ 0 ). The final optimization procedure is simple: for each minibatch, sample θ 0 and a random set of alternatives from θ alt from a random surface S c (θ 0 ), evaluate the neural network on **data** x ∼ p(x|θ i ) and compute the average of the binary cross-entropy losses for each alternative, where samples from θ 0 are labeled y = 0 and any samples from any θ alt are labelled y = 1. With such a procedure and average loss definition, the parameters φ can be optimized via standard stochastic gradient descent. The algorithm is summarized in Algorithm 1."
199124939,1/8,5.0,0.125,"The remarkable similarity in behavior to PDA suggests that in AXT aggregates, optical stimulation of S1 (which has some 2 1 Agsuperposition state contribution on early timescales) separates and delocalizes the bound triplet-pairs over multiple chains to a greater degree than occurs naturally within the aggregates. Using a kinetic model complementary to that shown for PDA (SI, Figure S8) we find the amount of 1 [T…T] optically liberated from 2 1 Agin AXT aggregates to be significantly less, 3 ± 1 %, in line with the weak interchromophore coupling. We do remark however this to be a qualitative upper bound estimate as care must be taken in modelling the photophysics of carotenoids with transient absorption **data** due to alternate branching pathways introduced by numerous 'dark' or charge transfer states which would serve to reduce this number further. Although AXT is chemically different to PDA, the likeness in photophysical response suggests that the possibility for extraction of the 1 [T…T] state from a 2 1 Agsuperposition, via an optical pulse, is widespread. The range of energetic orderings, numerous 'dark' states and potential for conformational changes on photoexcitation means further investigation is required to show these observations hold for all carotenoid systems. Figure 6: Schematic of the spatial separation of triplet-pairs from the 2 1 Agstate in pi-conjugated molecular systems. The bound triplet-pair excitons (dashed boxes on blue background) separate along the conjugated molecular backbone into a spatially free triplet-pair state where there is maximum spin entanglement but minimum spatial entanglement. After a given time, the spins recombine to give rise to the observation of an overall lifetime enhancement in 2 1 Ag -."
146120626,4/8,2.0,0.5,"A roughly linear time dependence of h on average, however, does not imply that this also holds on the level of an individual researcher. In fact, this is not the case. Fig. 1(b) illustrates some strong deviations from the lin- ear behaviour for a few chosen examples from our **data** set. Researcher V shows almost ideal linear behaviour, apart from a slight delay in the start of the career. The other individuals show all types of different characteristics such an increased slope at later times (W and, rather extreme, X), or more complicated curve shapes (Y and Z). These different shapes and possible reasons for them have been discussed previously [17]. It could be interesting to investigate this further by assigning researchers to different shape categories but this goes beyond the scope of the present work. Here, it is only important that a large variety of curve shapes is found and that their tendency to average out to a linear curve does not imply that linear behaviour holds on the level of individual researchers. With respect to the calculation of h α , a consequence of this is that from knowing h today, it is impossible to make reliable statements about what h has been at some point in the past. In order to quantify the deviation from a linear time dependence, we perform a linear fit of h(t) for individuals of the sub-group with a career length of at least 25 years, using the constraint that h in the first year of the career h(0) matches the actually observed value (mostly 0, but sometimes 1 or even 2). The resulting slope and the sum of the squared residuals are shown as histograms in Figure 2(a) and (b), respectively. We see that h typically increases at a rate of 0-2 per year and that, while a linear fit works reasonably well in most cases, there are many individuals for which it does not."
216162633,6/6,1.0,1.0,"6. Examples 6.1. Generalized Boussinesq equation. We consider the equation (GB) * . We remember that hypothesis (H0) holds with c = min{m, α1 α2 }. Also, we assumed that the source terms f and the corresponding potential operator F do not have any particular form but they satisfy (H1). The solution in the sense of Definition 2.1 holds, see [29,25,33], then nonexistence of global solutions is due to blow-up. By Theorem 3.1, if the initial **data** are such that u 0 2 * + α 2 u 0 2 2 > 0, (u 0 , u 1 ) * + α 2 (u 0 , u 1 ) 2 > 0, and the initial energy
E 0 = 1 2 u 1 2 * + α 2 u 1 2 2 + α 3 ∇u 0 2 2 + m 2 u 0 2 * + α 1 u 0 2 2 − F (u 0 ),
is such that E 0 ∈ I δ = (α δ , β δ ), given in Theorem 3.1, then the corresponding solution is not global and blows up in finite time. Moreover, by Corollary 1, for every positive initial energy E 0 , there exists initial **data** such that imply the nonexistence of global solutions in the norm of H."
16515985,8/18,1.0,0.444,"The effect of particulate additives on the viscoelastic properties was studied by adding monodisperse polystyrene + 30% polybutylmethacrylate spheres (9.55 ± 0.44 µm diameter, 0.5 % volume fraction ) to the lamellar phase sample. The storage modulus is consistently higher in the presence of particles ( Fig. 11) but approaches that of a particle-free sample at the longest times (∼ 10 min). A similar trend is observed for G ′′ (Fig. 12), although the **data** has more scatter. As mentioned in section III, the particles tend to coagulate at very long times, and the anchoring of the defect network is lost; this too is in keeping with the ultimate decrease of the moduli to those of the particle-free sample. The variation in the dynamic modulii between samples was greater in the presence of particles than without, roughly 20%, though part of it could be attributed to the difficulty in maintaining a constant particle concentration in all samples, but all the trends were reproducible. Our video microscopy studies have shown that sheared lyotropic lamellar phases display a striking defect network structure made up of oily streaks. These defects anneal away slowly under shear treatment, by a route which appears to be the thinning and disappearance of lines. The addition of a small concentration of suspended particulate impurities greatly retards the decay of this defect network. Previous studies of particulate additives in a lyotropic gel system showed a large increase in the rigidity modulus [1]. It was conjectured there that surfactant molecules adsorb on the particles, resulting in direct interparticle bridges and stress transmission. The oily streak network we observe is a far more likely candidate for such a stress-transmitting structure."
28774068,19/23,1.0,0.826,"Fig. 8 46 Fig. 9
8469Distributions of vorticity in the time-averaged flow field. Left wall-parallel plane; right wall-normal plane directly upstream of the propeller. T c = 27.3, h/R = 1.Domain boundary of occurrence of ground vortices induced by the propeller. The **data** of turbofans (green curve) were reported in
"
119303658,6/22,1.0,0.273,"In Figure 3d we demonstrate how well both fits are able to reproduce the observed function ǫ(t). To this end, we use the fitting residual (Williams et al. 2009;Lugaz 2010)
σ 2 = 1/N N i=1 (ǫ(t i ) − ǫ f it (t i )) 2(11)
with N being the number of **data** points along the ICME track and ǫ f it the calculated elongation value for the time step t i using the given set of fitting results V , φ and t 0 . For the least-squares minimization using the downhill-simplex-method, a slightly modified σ 2 is used, where we replaced both ǫ terms with their norms |ǫ(t i )| and |ǫ f it (t i )|, which lead to a more robust minimization than Equation (11). From Figure 3d, where we plotted
ǫ(t i ) − ǫ f it (t i )
for every ǫ(t) measurement, it is seen that both functions are able to almost equally well describe the observed elongation variation of the ICME leading edge. The residual σ 2 is a factor of 2 higher for FPF than for HMF, and both residual functions follow each other very closely, with the strongest deviations to the observed elongations at the end of the track. Nevertheless, based on the fits one cannot clearly distinguish between the two methods based on the consideration that a goodness-of-fit is better for one method as compared to the other. A real-time space weather predictor would thus not be able to distinguish if one of the predictions should be preferred over the other because he/she is able to fit the ǫ(t) function with almost the same accuracy with both methods."
119125001,2/32,2.0,0.062,"Many researchers in dispersive partial differential equations have recently examined whether blow-up behaviour, such as the norm-inflation described above, occurs for generic or only exceptional sets of rough initial data. To quantify this, one is quickly lead to random initial data. Indeed, one natural form of rough initial **data** is pu 0`f ω 0 , u 1`f ω 1 q, where the functions pu 0 , u 1 q P 9 H 1ˆL2 are regular and deterministic, while the functions pf ω 0 , f ω 1 q P H sˆH s´1 are rough and random. An analogue of Theorem 1.1 in this case would imply the stability of the scattering mechanism under a perturbation by noise. The literature on random dispersive partial differential equations is vast. We refer the interested reader to the survey [6], and mention the related works [2,4,8,9,11,12,14,15,16,38,39,41,42,44]. In the following discussion, we focus on the Wiener randomization [3,38] of a function f P H s pR d q. Let ϕ P C 8 pR d q be a smooth and symmetric function satisfying ϕ| r´3{8,3{8s d "" 1, ϕ| R d zr´5{8,5{8s d "" 0, and ř kPZ d ϕpξ´kq "" 1 for all ξ P R d . We then define the associated operator P k by y P k f pξq :"" ϕpξ´kq p f pξq ."
244214739,14/15,1.0,0.933,"Figure 5 .
5Comparison along the 50-10 ka interval of **data** linked to the ice advection or dynamics at the sites under examination. Reconstructed dissolved oxygen concentrations (do) at the surface were added for each core to check the effect of ice cover on the ventilation of the sea-surface ocean during HEs. The North Atlantic stratotypical reference (i.e., NGRIP δ 18 O [46]) is also plotted; the stratigraphical positions of the main cold events as documented in previous Figures are also represented by vertical bands. For color used in this figure, the reader is referred to the Web version of this article.
"
119337430,1/25,1.0,0.04,"In order to simplify the notation and also the analysis of the problem, we introduce the operator
A(B) = µ 1 I + µ 2 B + µ 3 B −1 ,(1.5)
and we write
µ 1 ∆v + µ 2 ∇ · (D(v)B + BD(v)) + µ 3 ∇ · (D(v)B −1 + B −1 D(v)) = D(v)A(B) + A(B)D(v).
This allows us to rewrite the viscoelastic Stokes problem (1.1) in the form   for a given tensor A, and investigate the existence, uniqueness and regularity of solutions of problem (1.7) in the case that A is an essentially bounded and uniformly positive definite symmetric tensor. The existence and uniqueness of solutions of the weak form of (1.7) together with the regularity v ∈ H 1 0 (Ω), p ∈ L 2 (Ω) follows from a classical application of Lax-Milgram Theorem, once the ellipticity is guaranteed (see Theorem 4.1). The regularity v ∈ H 2 (Ω), p ∈ H 1 (Ω) follows by applying a result of Machá [25, Theorem 4.2 with k = 0] for a slightly more general system, once all the conditions are verified, and along with that we obtain more explicit estimates in terms of all the **data** of the problem (see Theorem 4.2). For higher-order regularity, the case k ≥ 1 in [25,Theorem 4.2] is not optimal, and neither is any other result that we are aware of (the main reason is that the conditions on A are imposed independently of the space dimension and the Sobolev embeddings for particular dimensions are not explored; see Remark 4.8). We, therefore, work out the details of the proof for higher-order regularity and obtain the desired result under less stringent regularity assumptions on A, along with the associated explicit estimates (see Theorem 4.3)."
29602952,9/19,2.0,0.474,"We have generated 1000 **data** sets from the nonignorable selection model. From Crow's data, we have obtained the distribution of the ten family sizes 1, 2, . . . , 10. The frequencies of the family sizes are 9, 24, 16, 13, 9, 2, 4, 1, 1, 1. Thus, using the table method, we draw 100 family sizes for each of the 1000 simulated **data** sets. Now, noting that p(a k ,r k |p, π, θ)=p(a k |r k , π)p(r k |p, π, θ),"
207778550,30/48,1.0,0.625,"An example of this effect is seen in the measurements of 10 B(n,α) 7 Li performed on two separate occasions at the same laboratory using the same multi-grid fission chamber, by Zhang et al. in 2002 [109] and by Zhang et al. in 2011 [110]. For the earlier measurements, it was not known that particle leaking had occurred. Thus, the measured cross sections were significantly lower than other measurements, as is seen in Fig. 11. Initially these **data** could have been considered to have had an USU component. For the later measurements it was understood that particle leaking can occur in such experiments, and a more complete kinematic analysis was carried out to remove its effect. Those **data** agree with recent measurements, such as those of Giorginis and Khryachkov [108] who understood the particle leaking problem, as is shown in Fig. 11."
119125001,25/32,1.0,0.781,"Step 2: Splitting the energy increment. From Lemma 5.4, we have that
r E R t 0 ,x 0 rws ď E |x´x 0 |ď16R rwspt 0 q`C ż K R t 0 ,x 0 |F ||w| 2 |B t w|dxdt`C ż K R t 0 ,x 0 |F | 3 |B t w|dxdt .(69)
The main term is the second summand in (69). We use a Littlewood-Paley type decomposition of the linear evolution and write
ż K R t 0 ,x 0 |F ||w| 2 |B t w|dxdt ď ÿ N ěR ż K R t 0 ,x 0 |F N ||w| 2 |B t w|dxdt`ÿ N ďR{2 ż K R t 0 ,x 0 |F N ||w| 2 |B t w|dxdt .
Step 3: High frequencies. The high frequencies can be controlled using the single-scale estimate from Proposition 6.1. Indeed, we have that
ÿ N ěR ż K R t 0 ,x 0 |F N ||w| 2 |B t w|dxdt À η ÿ N ěR N 3 4´s`8 δ r E R t 0 ,x 0 rws 1 2´r E R t 0 ,x 0 rws`r F R t 0 ,x 0 rws¯1 2 (70) À ηR 3 4´s`8 δ r E R t 0 ,x 0 rws 1 2´r E R t 0 ,x 0 rws`r F R t 0 ,x 0 rws¯1 2 .
Step 4: Low frequencies. For τ n "" N n P N N 0 and y j "" N j P N Z 4 , we write In the last line, we have used that
K R t 0 ,x 0 "" ď pn,jqPN 0ˆZ 4 K N τn,y j ĎK R t 0 ,x 0 K N τn,y j .
We first control the contributions on the time intervals rτ n , τ n`N s. To this end, we define w pN,n,jq as the K N τn,y j -locally forced solution with **data** w pN,n,jq pτ n q "" wpτ n q and B t w pN,n,jq pτ n q "" B t wpτ n q ."
53380409,106/119,1.0,0.891,"The combination of the individual channels proceeds with a simultaneous analysis of the signal discriminants across all of the channels. For each signal model being tested, only the channels sensitive to that hypothesis are included in the combination. The statistical treatment of the **data** is based on the ROOFIT [61], ROOSTATS [62], and HISTFACTORY [63] **data** modeling and handling toolkits. Results are calculated in two different signal parametrization paradigms, corresponding to one-dimensional upper limits on the cross section times branching fraction (σ × B) and two-dimensional limits on coupling strengths. The statistical treatment of each case is described here."
207778550,28/48,1.0,0.583,"This section includes four examples that illustrate situations where it would be either impossible, or at best inconclusive or impractical, to try to identify and quantify USU, due simply to circumstances or a lack of adequate information. Often, such impossibility is linked to experimental **data** that have been improperly corrected (or not corrected at all) for a particular physical effect."
53380409,64/119,1.0,0.538,"
new heavy resonances decaying into different pairings of W, Z, or Higgs bosons, as well as directly into leptons, are presented using a **data** sample corresponding to 36.1 fb −1 of pp collisions at ffiffi ffi s p ¼ 13 TeV collected during 2015 and 2016 with the ATLAS detector at the CERN Large Hadron
"
119634392,1/9,2.0,0.111,"Another commonly studied equation is the generalized Navier-Stokes equation, given by ∂ t u + (u · ∇)u = νLu − ∇p,
u(0, x) = u 0 (x), div (u) = 0
where L is a Fourier multiplier with symbol m(ξ) = −|ξ| γ for γ > 0. Choosing γ = 2 returns the standard Navier-Stokes equation. In [17], Wu proved (among other results) the existence of unique local solutions for this equation provided the **data** is in the Besov space B s p,q (R n ) with s = 1 + n/p − γ and 1 < γ ≤ 2. If the norm of the initial **data** is sufficiently small, these local solutions can be extended to global solutions."
119479526,3/29,2.0,0.103,"Likelihood is the denominator of Eq. (1) and represents the probability of the **data** given the model. The marginal Likelihood has the desirable qualities of rewarding models that match the **data** well and penalizing models that are overly complex (i.e. have too many degrees of freedom or parameters). The marginal Likelihood is given by,
P (D|M ) = Ω Θ P (D|Θ, M) P (Θ|M ) dΘ,(2)
where Ω Θ represents the complete parameter space. Note that by integrating over the parameters in the model we are able to compare Bayesian Evidences for two distinct models in a meaningful way, even if they contain different numbers of parameters. This ratio of the Bayesian Evidence for two models is called the Bayes Factor and is given by
R = P (M A |D) P (M B |D) = P (D|M A ) P (M A ) P (D|M B ) P (M B ) ,(3)
where M A and M B are the two models under consideration [15]. Kass and Raftery [21] provide a commonly used guide to interpret the Bayes factor, in which a value in the range 3 − 20 indicates a positive strength of evidence for the preference of one model versus another and a factor in the range 20 − 150 represents strong evidence to prefer one model over another. Unfortunately because of the integral over parameter space Eq. (2) is notoriously difficult to evaluate. Only with recent advances in sampling techniques has the model Evidence become an appealing option for model selection. We briefly discuss some selected sampling approaches in the following section."
17837291,41/52,1.0,0.788,"Definition 5 . 3 .
53Two Euler **data** A, B are linked if for every balloon pq in X and everyd = δ[pq] ≻ 0, (A d − B d )| qis regular at α = λ δ where λ is the weight on the tangent line T q (pq)."
218971725,1/10,4.0,0.1,"In this work, we exploit an optimal multi-wave sampling approach for design-based estimators. In survey literature, the well-known Neyman allocation (Neyman, 1934) is the optimal sampling strategy; it minimises the variance of population total for the variable of interest. The regression parameter can be written as the total of its influence functions (Breslow et al., 2009a), so Neyman allocation can then be adopted for minimising the variance of regression parameter. The influence functions also depend on phase-2 **data** so that a multi-wave sampling can be useful."
17671315,10/32,1.0,0.312,"The modulus of the curve of Fig. 4-b) does not exceed 0.2. On the other hand, amplitudes of peaks of experimentally measured curves of Fig. 3 are of the order of 10 4 . This means that the **data** correspond to Aδ (x − x 0 ) in (7), where the number A = const. > 0 is unknown. In our experience a similar situation takes place for all experimental data, not necessarily for the current one. It is clear therefore that in order to make the **data** suitable for the model (6)-(9), they must be multiplied by a certain number, which we call ""calibration factor"" and denote CF. A proper choice of CF is a non-trivial task. To have unbiased studies, it is important that CF must be the same for all targets."
119415579,23/41,2.0,0.561,"A clear increase in the incidence rate of mCP stars with increasing mass was identified in Paper I (mCP stars account for ≈ 3 per cent of MS stars with M ≈ 1.5 M and ≈ 10 per cent of MS stars with 3.0 < M/M < 3.8). The Monte Carlo simulation involving the Zorec & Royer (2012) **data** did not reveal an increase in Bc with decreasing M , which might have otherwise explained the increased rarity of lower mass mCP stars. We conclude that, regardless of whether Bc exists, this particular property of mCP stars is likely a product of additional factors such as the increase in subsurface convection zone depth with decreasing mass."
246863515,10/44,1.0,0.227,"In this section, we provide three different estimators for the average long-term treat effect, corresponding to the three different identification strategies in Section 4 respectively. This involves combining two samples, so we assume that as n → ∞, n E /n O → λ where 0 < λ < ∞. This is a common assumption in the **data** combination literature [e.g., Angrist andKrueger, 1992, Graham et al., 2016]."
117119864,20/83,1.0,0.241,"Interest in the LL was greatly intensified by Anderson's proposal [127] that the ARPES lineshapes and certain other properties of the superconducting cuprates signaled in a general way the possibility of LL behavior in quasi 2-d systems, and by pioneering PES studies [128][129][130] of quasi-1-d metals that found power law onsets to EF rather than a Fermi edge. A complication for interpreting the PES **data** is that below a transition temperature TCDW the quasi-1-d metals studied display static charge density wave (CDW) formation which gaps the quasi-1-d FS and produces an insulator. Especially in 1-d, strong CDW fluctuations involving electron-phonon interactions above TCDW can cause the PES lineshape to have NFL behavior that mimics that of the LL, in particular that there can be a pseudogap [131][132][133] such that the weight at EF is greatly suppressed. This situation has motivated ARPES activity [17] on low-dimensional non-cuprate materials with the intent of elucidating CDW behavior, of searching for a paradigm having the distinctive features of the LL lineshape, and of seeing connections to cuprate spectra. The finding of the FL paradigm TiTe2 discussed in section 3.2 above was part of this general program, which has now progressed also to success with the other two goals, as described next."
249258070,111/119,4.0,0.933,"The breakdown of self-initiated activities across these three units were 64.2% for patrol, 17.5% for special crime units, and 18.3% for the differential response team. Unfortunately, the HPD CFS **data** do not provide detailed information about the nature of self-initiated activities, such as offense or incident type for patrol officers. Activities performed by specialized crime investigation and DRT officers might be deduced approximately using the division information. But to further breakdown the activity to specific units produces small frequencies of specific offenses, which would have been insufficient to conduct meaningful analysis. For example, there were only 338 self-initiated activities recorded under gang division, and 165 self-initiated activities recorded under robbery division during the study period. The **data** do include some information on dispositional outcomes of police self-initiated activities. The most frequent type of action for self-initiated activities is ""collecting information"" for each of the three types of police units. The disposition of ""collecting information"" is used in instances where the incident is resolved without needing to generate an offense report or referral for further investigation. Some examples are if the officer resolves the issue by mediation, the complaint is a civil issue rather than criminal, or if it is to provide further information about an offense report that already exists. Other self-initiated actions include making an offense or supplementary report, making an arrest, issuing tickets, etc."
118927342,9/10,2.0,0.9,"
B(B − → Λpπ + π − ) 3.7 +1.2 −0.5 ± 0.1 ± 0.9 5.9 ± 1.110 6 B(B − → ΛpK + K − ) 3.0 +1.1 −0.5 ± 0.1 ± 0.7 --10 6 B(B 0 → ppπ + π − ) 3.0 +0.5 −0.3 ± 0.3 ± 0.7 3.0 ± 0.310 6 B(B 0 → ppπ ± K ∓ ) 6.6 ± 0.5 ± 0.0 ± 2.3 6.6 ± 0.5 included the **data** to constrain the non-factorizable effects, which results in δN ef"
238834501,3/19,2.0,0.158,"We calculate a distribution over prediction errors by subtracting the observed value at t • for all non-eclipse days from the corresponding estimates:Ŷ   The lasso, which we apply to both regression problems, is a regularised regression whose loss function contains a ℓ 1 penalty on the vector of regression coefficients β . It is this form of the penalty that leads to the coefficients for less influential predictors to be set to zero, and thereby to subset selection 54 . The penalty is scaled by a hyperparameter, denoted α here, which controls how many predictors are culled: the larger the value of α , the more coefficients become zero, and the lower the resulting model complexity. We choose its value by approximately minimising a root mean square test error (RMSE) computed by averaging PE 2 over t • and d • . The regression estimate itself is not overly sensitive to the precise value of α , and so we simply choose a single value that is approximately optimal for a number of meteorological stations (see Table 3).
d • t = X T d • tβ d • t • Y dt • = X dt •β d • t • .
We compare the lasso estimates with estimates based on simple averaging and smoothing. For the averaging, the **data** matrix's date dimension is first ordered according to the mean square difference over times t • between each day and eclipse day. Then, the N closest days, in this Euclidean distance sense, are averaged. A lower value of N corresponds to a larger value of α (stronger regularisation and fewer non-zero coefficients). As with α , we select a value for N which approximately minimizes the average test error over all stations."
17837291,22/52,1.0,0.423,"The Euler **data** Q
Theorem 7.1. (i) deg α A d ≤ −2.
(ii) If for each d the class b T (V d ) has homogeneous degree the same as the degree of M 0,0 (d, X), then in the nonequivariant limit we have
X e −H·t/α A d = α −3 (2 − d · t)K d X HG[A](t) − e −H·t/α Ω = α −3 (2Φ − t i ∂Φ ∂t i ).
Proof: Earlier we have proved that
A d = i * 0 Q v d = ev ! ρ * b T (V d ) α(α − c 1 (L)) ,
where L = L d is the line bundle on M 0,1 (d, X) whose fiber at a point (f, C; x) is the tangent line at x."
119415579,4/41,1.0,0.098,"We obtained 95 Stokes V observations of 37 stars from Aug. 2, 2015 to Aug. 10, 2016 using ESPaDOnS. Twentythree Stokes V observations of 3 stars were obtained using NARVAL from Aug. 20, 2016to Feb. 20, 2017. All of the observations obtained using ESPaDOnS and NARVAL were reduced with the Libre-ESpRIT software package, which is an updated version of the ESpRIT reduction package that was applied to the MuSiCoS **data** (Donati et al. 1997)."
119419652,3/18,1.0,0.167,"Jet and dijet cross-section measurements allow for a good test of perturbative QCD (pQCD) in p-p collisions. The ATLAS collaboration performed such doubly differential measurements using a total integrated luminosity of up to 3.2 fb −1 at a centre-of-mass energy of √ s = 13 TeV [4]. Reconstructed jets were formed using the anti−k T algorithm with a distance parameter R = 0.4 [5], and using EM-scale calorimeter clusters as the inputs. The inclusive-jet results cover a wide kinematic range in jet transverse momentum (p T ) from 100 GeV to 3.5 TeV and for several separate ranges of jet rapidity up to |y| = 3.0. In the case of the dijet cross-section measurements, the results are presented as a function of dijet invariant mass (m jj ) in the range 300 GeV < m jj < 9 TeV, and for values of the quantity y * = 1 2 |y 1 − y 2 | -half the absolute rapidity difference between the two leading jets 1 -up to y * = 3.0. The choice of bin sizes (for p T , y, m jj and y * ) was motivated by the respective detector resolution. The measured results from **data** were compared to next-and in some cases next-to-next-to-leading order (NLO and NNLO, respectively) pQCD predictions, and in all cases were found to be in good agreement with the SM predictions. The inclusive-jet and dijet results are summarized in Figure 1, where the usual convention is adopted of applying multiplicative offset factors to the results in various |y| (y * ) bins in order to allow for a better visual comparison of the shapes. Doubly and triply differential jet cross-section measurements were also performed by the CMS collaboration [6,7,8].  Figure 1: Results of the measured (a) jet and (b) dijet differential cross-section results at √ s = 13 TeV by the ATLAS collaboration [4]."
54041858,6/45,3.0,0.133,"To understand the intensity and progression of peak and valley events, an energy analysis was performed. For each event the average energy was determined by taking the average of the energies of the two outer sensors. Figure 4 shows the AE event energy versus time for the entire test that were added in sequence to produce Figure 3. Each **data** point represents a single AE event from a discrete source. Significant AE activity occurs at the beginning of the test, primarily in the first few cycles. For most of the test, little AE occurs until the very end of the test. The energy ranges from 0.1 to 50 V 2 /s until near the end of the test where energies less than 0.01 V 2 /s were recorded. Since the valley events were only observed at the end of the test (201,990 s), the peak and valley events were plotted separately after 200,000 s in Figure 4b,c, respectively. There were 2156 valley events compared to 1367 events over this period. Valley events were also much more prevalent prior to about 206,000 s. It is also obvious that the valley events tended to be of higher energy than the peak events though both peak and valley showed increasing energy events as time progressed. The peak events contained low and high energies (Figure 4b), including nearly all the very low energy events at the end of the test."
55185767,7/16,2.0,0.438,"Finally we treat the case that Y is VF. By Theorem 5.3.1 and by the already treated cases that Y = RF n and Y = VG, it is enough to treat the case that W K,x is a single open ball in K for each K and each x ∈ X K . Moreover, on these balls, we may assume that the functions like h as in (3.2.1) that appear in the build-up of f have the Jacobian property, and that all other build-up **data** of f factors through the projection W → X. But then calculating the integral is easy, and goes as follows. By the Jacobian property for h, the set h K (x, W K,x ) is an open ball, say, of valuative radius n K ∈ Z. If n K ≤ 0, then the integral of ψ(h K (x, y)) over y ∈ W K,x equals zero for each ψ in D K . If n K > 0, then ψ(h K (x, y)) is constant on W K,x , say, equal to ξ K,ψ,x , and hence the integral of ψ(h K (x, y)) over y ∈ W K,x equals the volume of the ball W K,x , say q m K K , times ξ K,ψ,x . By Lemma 4.6.2 and by the already treated cases that Y = RF n , we may suppose that h factors through the projection W → X, so that q m K K ξ K,ψ,x lies in C exp (X) by definition of C exp -functions and we are done. (1) For every definable set X, C(X) contains the characteristic function of every definable set A ⊂ X. (2) The collection is stable under integration, namely, for any X and Y and any f ∈ C(X × Y ) such that for each K and each x ∈ X K , the function y → f K (x, y) is integrable over Y K , the function
(K, x) → y∈Y K f K (x, y)|dy|
lies in C(X) (with our usual product measure on Y K )."
245800515,8/16,1.0,0.5,"Proof. The first inequality follows from
t 0 (g β,t (τ )) −1 Ĝ (τ ) 2 dτ ≤ Γ(1 − β) AU 0 − f (0) 2 κ0 0 ((t − τ ) −β + τ −β ) −1 dτ ≤ 1 β + 1 Γ(1 − β) AU 0 − f (0) 2 κ 1+β 0 , where we used κ0 0 ((t − τ ) −β + τ −β ) −1 dτ ≤ κ0 0 τ β dτ ≤ 1 1 + β κ β+1 0 .
The second inequality is obtained by direct computation and the estimate 1 ) and the L ∞ norm no better than O(t β−1 κ 0 ). Hence, denoting by κ = max j κ j , we require at least that κ 0 = O(κ 4−2β 1+β ) in order to obtain optimal assymptotic convergence order O(κ 2−β ) in the L 2 norm. In the L ∞ norm, we require at least κ 0 = O(κ 2−β ) to obtain optimal convergence for t n > c for any fixed constant c > 0. Further, κ 0 = O(κ 2−β β ) is required if optimal convergence is to be expected uniformly for t n > κ 0 ; this is compatible with the result from [39]; see (2.11).
κ 0 t β − (t − κ 0 ) β ≤ β max τ ∈[t−κ0,t] τ β−1 = β(t − κ 0 ) β−1 .
If the **data** f is smooth for t > 0 then so is the solution u with a possible singularity at t = 0; see (4.2)."
119313773,7/21,1.0,0.333,"The critical length of the **data** N b above which breakdown of linear response can be detected in a statistically significant way depends on the perturbation size ε. In particular, χ 2 is an increasing function of ε for sufficiently large values of ε, cf. (11). This dependency can be intuitively understood since the response to small perturbations must be distinguished from the variations in the unperturbed system due to the sampling error. This implies that to be able to identify a deviation from linear response at a specified perturbation size ε with a significance level p the perturbation size needs to be sufficiently large. This is illustrated in Figure 8 where we show the Monte-Carlo estimateq of the expectation value of the breakdown parameter as a function of the perturbation size which is parametrized by the perturbation interval dε. For each value of dε the perturbed system is sampled at ε i = 2i−M +1 2 dε with i = 0, . . . , M − 1 for fixed **data** length N = 10 6 with M = 20. For perturbation sizes dε < 8 × 10 −7 the observations are consistent with linear response theory and only for dε > 8 × 10 −7 can the actual breakdown be detected in a statistically significant way. For comparison we have again included in Figure 8 a plot showing the breakdown parameter as a function of dε for the doubling map where linear response assures Eq → 0 for ε → 0. Here linear response is consistent with the observations for the whole range of perturbation sizes considered. Figure 9 illustrates that the smaller the applied perturbation the larger the **data** length has to be to detect breakdown. Shown is the critical **data** length N b above which breakdown can be detected for a given perturbation size. The critical **data** length N b was determined to be the value of N such that q = q α for α = 0.05. A linear fit suggests N b ∼ ε −γ , where γ was estimated in Figure 9 to be 0.91."
110141515,14/21,1.0,0.667,"This paper evaluates the characteristics of the π-CLCL type immittance converter using simulation and experiments. The study ensures immittance conversion of the circuit at resonant frequency. The experimental **data** reinforces the results obtained from simulations. The output current remains fairly constant as load is increased up to three times the characteristic impedance. Therefore, it can be said that the output current is independent of the load. Thus, the π-CLCL configuration of the immittance converter can be used for constant current and dynamic load applications. The study confirms that the π-CLCL configuration is more efficient than the π-CLC and T-LCL configurations. Furthermore, the efficiency is not much affected by the changes in frequency. Maximum efficiency and the load at which maximum efficiency occurs both depend on the ratio of the two quality factors. The condition Q 1 =1.5Q 2 gives better efficiency than Q 1 =Q 2 . If quality factors can be improved, the efficiency of the converter will increase substantially. Therefore, a highly efficient converter can be built using the proposed π-CLCL topology of the immittance converter if inductors having higher quality factors are used. Efficiency Efficiency @ +10% resonant frequency Efficiency @ -10% resonant frequency Efficiency @ resonant frequency
"
119217429,1/13,1.0,0.077,"In this paper, we present 1. ′′ 7 resolution image of SO(v=0 3(2)-2(1)) line emission from the ALMA Cycle 0 observations. SO molecular line emissions are known to trace relatively dense and active clouds in a region (Bachiller et al. 2001;Aladro et al. 2013;Hacer et al. 2013;Codella et al. 2014). We investigated the spatial distributions and velocity structures with high-resolution observations based on ALMA archive **data** of G0.25 obtained with an extended configuration. The mosaic of the molecular line emission across this cloud were obtained at 90 GHz. We speculate that G0.25 cloud may represent the precursor to a superstar cluster."
38256305,49/49,1.0,1.0,"Figure 15 .
15Summary of the gained results, combining estimated shear wave velocity Vs 30 **data** with the weighted overlay results and lineament analysis/lineament density calculation.
"
119125001,6/32,1.0,0.188,"x pRˆR 4 q ă 8 . (iii) u almost surely scatters to a solution of the linear wave equation. Thus, there exist random scattering states pu0 , u1 q P 9
H 1 pR 4 qˆL 2 pR 4 q s.t. lim tÑ˘8 }puptq´W ptqpu0`f ω 0 , u1`f ω 1 q, B t uptq´B t W ptqpu0`f ω 0 , u1`f ω 1 qq} 9 H 1ˆL2 "" 0 .
While Theorem 1.3 is only proven for the microlocal randomization, the majority of our argument directly applies to the Wiener randomization. The main novelty in this paper lies in the application of a wave packet decomposition. To illustrate this idea, fix some k P Z d with }k} 8 "" N , and assume that p f k pξq "" N´sϕpξ´kq. Then, f k will essentially be unaffected by both the Wiener and microlocal randomizations, and hence forms an important example. From the method of non-stationary phase, it follows for all times t P r0, N s that the evolution expp˘it|∇|qf k is concentrated in the ball |x˘tk{}k} 2 | À 1, and has amplitude "" N´s. In space-time, we can therefore view the evolution as a tube, see Figure 2a. For larger times, the dispersion of the evolution becomes significant, and the physical localization deteriorates. The wave packet perspective also explains the effect of the frequency randomization on the evolution. In Figure 2b, we display a bush (cf. [7]), which is a collection of wave packets intersecting at a single point. If all wave packets in the bush have comparable amplitudes and the **data** is deterministic, one expects that the L 8 t L 8"
119332876,7/24,1.0,0.292,Source 179 Figure 4. X-ray spectra of the Galactic stars (data points) together with power law models (black stepped lines) and multitemperature optically-thin thermal models (grey stepped lines). All spectra are shown in the observer frame. Both models and **data** have been divided by the product of effective area and Galactic transmission as a function of energy.
222090354,2/21,2.0,0.095,"Definition 2.11. For ρ 0 ∈ Z E,p and ρ ∈ AC([0, ∞); P p (X)) a curve a maximal slope with ρ(0) = ρ 0 , we denote by ω ′ρ (ρ 0 ) the relaxed ω-limit set with initial **data** ρ 0 subordinate to ρ, defined by
ω ′ρ (ρ 0 ) := {ρ ∈ P(X) : ∃t n → ∞ s.t. ρ(t n ) →ρ in P(X) as n → ∞}.
With these notations, our first main result is the relation between ω ρ (ρ 0 ) (resp. ω ′ρ (ρ 0 )) and E (resp. E w )."
2275365,10/18,1.0,0.556,"However, for application of DS Belief-Functions for representation of uncertainty in expert system knowledge bases there exist several severe obstacles. The main one is the missing frequentist interpretation of the DS-Belief function and hence neither a comparison of the deduction results with experimental **data** nor any quantitative nor even qualitative conclusions can be drawn from results of deduction in Dempster-Shafer-theory based expert systems [17]."
247596533,4/14,1.0,0.286,"The current investigations present the numerical investigations of an infection-based fractional order nonlinear prey-predator system using the stochastic procedures of the scaled conjugate gradient and the artificial neuron networks. The stochastic computational procedure SCGNNs is applied to solve three different cases using different fractional-order values. The **data** proportions applied 75%, 10, and 15% for training, validation, and testing to solve the infection FONPPS. Ten numbers of neurons have been used to solve the nonlinear biologicalbased differential model. The numerical simulations of the infectious disease FONPPS are accomplished using the SCGNNs, while the competitive performances have been presented using the Adams-Bashforth-Moulton approach. The numerical results of the nonlinear fractional-order biological system are calculated using the computational SCGNNs to reduce the mean square error. To ratify the exactness, reliability, capability, and aptitude of the proposed SCGNNs, the numerical measures are plotted using the regression, MSE, STs, correlation, and Ehs. The identical performances designate the precision and accuracy of the proposed stochastic scheme and the AE values found in suitable ranges based on the nonlinear fractional-order biological system. The AE values, and the plots of other performances represent the dependability and consistency of the proposed approach. In future studies, the stochastic SCGNNs are pragmatic to achieve the results of the lonngren-wave systems and fractional order nonlinear systems [37][38][39][40][41][42][43][44][45].    "
119128665,3/7,1.0,0.429,"We start with a general simple ""meta""-lemma, which implies, together with a specific computation of the bound on the number of singularities, all our specific results below. Consider a class of functions F on R n . We assume that F is closed with respect to taking partial derivatives, restrictions to affine subspaces of R n , and with respect to sufficiently rich perturbations. There are many classes of functions that comply with this condition, for example, we may speak about the class of real polynomials of n variables and degree d, and the classes considered below in this section. Assume that for each f 1 , , . . . , f n ∈ F the number of non-degenerate solutions of the system
f 1 = f 2 = · · · = f n = 0,
is bounded by the constant C(D(f 1 , . . . , f n )), where D(f 1 , . . . , f n ) is a collection of ""combinatorial"" **data** of f i , like degrees, which we call a ""Diagram"" of f 1 , . . . , f n . We assume that the diagram is stable with respect to the deformations we use. In each of the examples below we define the appropriate diagram specifically."
249258070,11/119,3.0,0.092,"The CFS **data** also captured proactive activities conducted by three distinct types of police units: patrol, specialized crime investigation, and the DRTs. Other than responding to calls, patrol officers also perform patrol duties, investigate suspicious activities, and form relationships with community members. Officers in specialized crime investigation divisions, such as robbery, vice, auto theft, gang, major assaults and family violence, and homicide divisions etc., mainly conduct investigations concerning specific type of offenses. Officers in DRT units do not respond to calls for service, instead, they proactively work with the community, focusing on problem solving and improving quality of life issues. These activities are all recorded as self-initiated activities within the CAD system."
208637417,14/23,1.0,0.609,"• More carefully study the spectral theory of the distance kernel operator to understand the decay of the eigenvalues and the histograms of √ λ i φ i , Φ k , and E X,k = | d X (x, y)− k i=1 λ i φ i (x)φ i (y)|. In particular, our experimental **data** shows these distributions are much more concentrated than the worst-case estimates coming from Markov's inequality."
119313773,16/21,1.0,0.762,"Figure 9 :
9The critical **data** length N b above which the breakdown of linear response is detectable as a function of the perturbation size ε, parametrized by dε. The slope of the linear fit is −0.91.
"
2275365,3/18,1.0,0.167,"...... Suppose you are a Bayesian and you must express your ""belief"" that the killer will We make use of the Smets' claim that ""the de re and de dicto interpretations lead to the same results"" ( [34], p. 333), that is Bel(A|B) = Bel(¬B ∨ A). Hence being may possess his private view on a subject, it is only after we formalize the feeling of subjectiveness and hence ground it in the **data** that we can rely on any computer's ""opinion"". We hope we have found one such formalization in this paper. The notion of labeling developed here substitutes one aspect of subjective human behaviour -if one has found one plausible explanation, one is too lazy to look for another one. So the process of labeling may express our personal attitudes, prejudices, sympathies etc. The interpretation drops deliberately the strive for maximal objectiveness aimed at by traditional statistical analysis. Hence we think this may be a promising path for further research going beyond the DS-Theory formalism."
119479526,6/29,1.0,0.207,"In a method introduced by Lahav et al. [27], and expanded by Ma et al. [13], the reported errors for each dataset are weighted using hyperparameters in the Bayesian analysis. In the thermodynamics literature, **data** variances are often available, though covariances are rarely reported. Consequently, we do not present methods for handling **data** covariances in this work and we assume independence of the **data** points."
14912634,7/23,1.0,0.304,"Hub-height wind speed may not represent the flow across the entire rotor disk. Recent work by Wagner et al (2009) and Antoniou et al (2007) suggest calculating a speed representative of the disk area for use in power curves. Following Sumner and Masson (2006), 10 min average SODAR data, at nine measurement heights from 40 to 120 m, were used to calculate a rotor-averaged or equivalent wind speed (U equiv SODAR , m s −1 ) across heights representing the rotor disk:
U equiv SODAR = 2 A t H+r H−r U(z)(r 2 − H 2 + 2Hz − z 2 ) 1/2 dz(8)
where U(z) is mean wind speed (m s −1 ) at height z (m), r is radius of the rotor disk (m), H is hub-height (m), z is measurement height (m), and dz is 10 m. This integral over the rotor disk altitudes was evaluated via summation using the SODAR **data** at discrete 10 m vertical intervals. Only vertical variability is measured and included in this interval as horizontal homogeneity is assumed. Further, following Wagner et al (2009), equation (8) was modified to recognize that the instantaneous wind speed is a composite of both mean (U) and turbulent (σ U ) components. To incorporate any turbulent energy encountered by the rotor, a modified 'true-flux' equivalent wind speed (U equivTI SODAR , m s −1 ) was calculated:
U equivTI SODAR = 2 A t H+r H−r U I (z)(r 2 − H 2 + 2Hz − z 2 ) 1/2 dz(9)
where wind speed at each height U I (z) has now been 'corrected' to include any additional energy from turbulence:
U I (z) = 3 U 3 (z)(1 + 3I 2 U ).(10)
Note that equation (9) assumes that the wind turbine is able to extract energy from turbulent motions in the air stream."
245800515,9/16,1.0,0.562,"We will compare the convergence of the L1 scheme and CQ with our estimators. The meshes will be of the form t j = T (j/N ) k with k ≥ 1; the mesh is uniform if k = 1 and graded towards 0 if k > 1. The exact initial **data** will be used, i.e., U 0 = u 0 ."
207778550,9/48,1.0,0.188,"As mentioned before, there has been a long history of criticism of nuclear **data** evaluated uncertainties, in particular of the Neutron Standards' uncertainties as being underestimated. The main problem in the case of evaluations based on primarily experimental **data** (as is the case of Neutron Standards) remains in the realm of properly estimating correlations between the input experimental data. The proposed unrecognized uncertainty-estimation method [66,68] addresses this problem, and it is equiv-alent to the determination of hidden uncertainties. The method has been applied to cross-section evaluations in the Neutron Cross Section Standards project [50][51][52], and it has been also used to evaluate the uncertainties of the latest release of the Russian evaluated nuclear **data** library BROND-3 [76,77]."
117119864,12/83,3.0,0.145,"Resonant ARPES reveals that parts of the FS show easily observable 4f weight at 120K. As presented in Ref. [18] this 4f weight is found in k-space in the vicinity of the low mass parts of the FS, the small hole pockets around Z and the small electron pocket around Γ. The angle integrated 4f spectrum at and above TK obtained directly [23] or by k-summing the resonant ARPES spectra [83] has the gener al appearance of the impurity-model spectra of Figs. 2 and 3. Although this finding may be plausible in view of the general theoretical correlation of smaller mass, larger energy scale and larger spectral weight, there is presently no spectral theory of t he Anderson lattice capable of providing insight into ARPES **data** at this level of detail. The so called ""LDA + DMFT"" theory discussed in section 5.2 below is promising for further analysis of such **data** for systems where the Fermi surface is well described by the LDA."
52995678,2/29,2.0,0.069,"We also note that the new Lagrangian ROM that we propose is different from the ROM used in [90]. The two ROMs are built on the same fundamental principle: Lagrangian **data** can be used to generate ROMs for the computation of Lagrangian fields. We note, however, that the strategy used in [90] to incorporate Lagrangian information in the ROMs is different from the strategy that we use in this paper. Indeed, the FTLE field is used in [90] only to determine the number of ROM basis functions. In contrast, we explicitly use the FTLE field in the actual construction of the ROM basis."
52275756,24/51,1.0,0.471,"In this study, hyperspectral remote sensing images were used as auxiliary variables and the spatial variability of soil nutrients was predicted using SLR, SVM, RF, BPNN and BPNNOK models. Compared with the SLR, SVM and RF models, the BPNN model with double hidden layers was more stable and had better fitting accuracy for soil TN (RMSE = 0.409 mg kg −1 , R 2 = 44.24%), soil AP (RMSE = 40.808 mg kg −1 , R 2 = 42.91%) and soil AK (RMSE = 67.464 mg kg −1 , R 2 = 48.53%). Furthermore, the BPNNOK model had the best fitting accuracy for soil TN (RMSE = 0.292 mg kg −1 and R 2 = 68.51%), soil AP (RMSE = 29.62 mg kg −1 and R 2 = 69.30%) and soil AK (RMSE = 49.67 mg kg −1 and R 2 = 70.55%) compared to the other prediction models. The spatial mapping results of the BPNNOK model were closer to the real ranges of soil TN, soil AP and soil AK contents. Areas with high contents of soil TN, soil AP and soil AK in the study area were in the north-central, central and southwest and central and southeast regions, respectively. These findings indicate that the BPNNOK model demonstrated a better fitting ability for the nonlinear relationships between hyperspectral variables and soil nutrients. In addition, the most important bands (464-517 nm) for soil TN (b10, b14, b20 and b21), soil AP (b3, b19 and b22) and soil AK (b4, b11, b12 and b25) exhibited the best response and sensitivity. The application of hyperspectral VNIR bands (450-950 nm) allowed for rapid, efficient mapping and monitoring of soil nutrients at the regional scale. As shown in this study, the application of hyperspectral remote sensing image **data** and the BPNNOK model present potential ways to monitor soil nutrients and manage fertilization at the regional scale.
"
17837291,34/52,2.0,0.654,"Here e S 1 (·) denotes the S 1 -equivariant Euler class. As in the cases we have studied earlier, the left hand side of the above formula indicates that when V = L is a line bundle, we should compare the Euler **data** Q d = ϕ ! π * e(V d ), to the Euler **data** given by
P d = c 1 (L),d m=0 (c 1 (L) − mα).
What is left is to develope uniqueness and mirror transformations, which we are unable to achieve at this moment, though they can be easily axiomized. Now let us look at K-theory formula, which can be proved by using equivariant localization in K-theory. First following the same idea, we get the explicit formula as follows:"
235829209,33/47,1.0,0.702,"Figure 2 :
2Prediction performance of 7 th -order polynomials surrogate model for the Hénon map in the x and p plane at a given tune ν = 0.205. Each dot represents one initial condition in the testing cluster. The red dashed line is the desired/expected value. Note the original **data** has been scaled/normalized to a range of [0, 1].
"
16542351,1/8,1.0,0.125,"H eavy quark physi cs o ers a uni que opportuni ty to expl ore the avour structure of the Standard M odel .T he thi rd fam i l y ofquarksi sthe l essknow n. Itsup-type quark, the top,hasnotyetbeen observed,al though we are getti ng m ore and m ore stri ngent l i m i ts on i ts m ass from di rect search and radi ati ve correcti on anal ysi s. O n the other hand, the correspondi ng dow n-type quark, the b, i s bei ng studi ed w i th i ncreasi ng accuracy. Both grow i ng stati sti cs and the operati on ofdedi cated vertex detector for btaggi ng athi gh energy m achi nesal ready al l ow sa good determ i nati on ofbcoupl i ngs. Present **data** cl earl y poi ntto the b asthe I 3 = {1/2 com ponent ofa weak l eft-handed i sodoubl et [ 1] ."
119128665,7/7,1.0,1.0,"where p ij is a polynomial of degree d ij . The diagram D(A) of A is the collective **data** D(A) = (n, k, j 1 , . . . , j k , (d ij ) i=1,...,k, j=1,...,j i )."
53380409,63/119,1.0,0.529,"A combination of results from searches for heavy resonance production in various bosonic and leptonic final states is presented. The **data** were collected with the ATLAS detector at the LHC in pp collisions at ffiffi ffi s p ¼ 13 TeV and correspond to an integrated luminosity of 36.1 fb −1 . While previous combination efforts included only the decays of heavy resonances into VV and VH, the combination presented here also includes decays into lepton-antilepton final states. Compared to the individual analyses, the combined results strengthen the constraints on physics beyond the Standard Model and allow the constraints to be expressed in terms of the couplings to quarks, leptons, or bosons. The relative sensitivities of the different approaches are compared, including bosonic and leptonic final states or different production mechanisms such as quark-antiquark annihilation/gluon-gluon fusion vs vectorboson fusion."
246863515,11/44,3.0,0.25,"Then the average long-term treatment effect is identifiable:
τ = a∈{0,1} (−1) 1−a E h E (a, X) | G = O + E P (G = E) P (G = O | X) P (G = O) P (G = E | X) I [A = a] P (A = a | X, G = E) h (S 3 , S 2 , A, X) −h E (A, X) | G = E + E P (G = E | A = a) P (G = O | X) P (G = O | A = a) P (G = E | X) I [A = a] P (A = a | X, G = E) q (S 2 , S 1 , A, X) (Y − h (S 3 , S 2 , A, X)) | G = O
Theorem 7 shows that even under weaker Assumptions 10 and 11, outcome and selection bridge functions can be still used to identify the average long-term treatment effect. This again has the doubly robust property in that it only requires one of the bridge functions to be correct rather than both. Compared to Theorem 3, Theorem 7 additionally incorporates the ratios P (G = O | X) /P (G = E | X) to adjust for the covariate distribution discrepancy in the two types of **data** (Assumption 10). It also uses the propensity score P (A = a | X, G = E) to account for the dependence between treatment A and covariates X in the experimental **data** (Assumption 11)."
52275756,9/51,2.0,0.176,"Given a dataset with N samples,
{(x 1 , y 1 ), . . . , (x i , y i )}, i = 1, . . . , N (where x i ∈ R n is the input vector, y i ∈ R 1
is a target output and N is the number of **data** points), the standard form of SVM [34] is as follows:
min ω,b,ξ,ξ * 1 2 ω T ω + C N ∑ i=1 ξ i + C N ∑ i=1 ξ * i s.t. ω T φ(x i ) + b − y i ≤ ε + ξ i y i − ω T φ(x i ) − b ≤ ε + ξ * i ξ i , ξ * i ≥ 0, i = 1, . . . , N(2)
where φ(x i ) maps the input space to the feature space, ω and b are optimized coefficients during the training phase, ε is the insensitive loss function, C is the regularized constant and ξ i and ξ * i are two positive slack variables. The dual form is
min α,α * 1 2 (α − α * ) T Q(α − α * ) + ε N ∑ i=1 (α i + α * i ) + N ∑ i=1 y i (α i − α * i ) s.t. N ∑ i=1 (α i − α * i ) = 0, α i ≥ 0, α * i ≤ C, i = 1, . . . , N(3)
where α i and α * i are non-negative Lagrangian multipliers; therefore, the regression function can be given as
N ∑ i=1 (α * i − α i )K(x i , x) + b,(4)
where K(x i , x) is the kernel function. In this study, the radial basis function (RBF) was used as the kernel function. The libsvm package [35] was imported into the MATLAB 2013b software to construct the SVM model and a multi-dimensional fitting relationship between soil nutrients content and the input PCs was established [36][37][38][39]. The SVM model parameters set as follows: the type of SVM was epsilon support vector regression (SVR); the meshgrid function was used to find optimum parameters, which including the parameter C of epsilon-SVR and gamma in kernel function; the epsilon in loss function of epsilon-SVR was 0.01 [35]."
17837291,34/52,1.0,0.654,"One of our key ingredient, the functorial localization formula plays an important role in relating the **data** on M d (X) and those on W d . It turns out that similar formula holds in K-theory. It holds even when X has no group action. This indicates that our method may be extended to compute K-theory multiplicative type characteristic classes on M d (X) (and ultimately on M 0,0 (d, X)), in terms of certain q-hypergeometric series, even for projective manifolds without group action."
17837291,46/52,1.0,0.885,"Theorem 6. 5 .
5Suppose that A, B have property (i) of Theorem 5.4, and that A, B are linked. Suppose that A is an Euler **data** with deg α A d ≤ −2 for all d ≺ 0, and that there exists power series f ∈ R[[K ∨ ]], g = (g 1 , .., g m ), g j ∈ R[[K ∨ ]], all without constant term,such thate f /α HG[B](t) = Ω − Ω H · (t + g) α + O(α −2 ) (6.5)when expanded in powers of α −1 . ThenHG[A](t + g) = e f /α HG[B](t). Proof: By Theorem 6.4, f, g define two mirror transformations µ f , ν g , with HG[B](t) = e f /α HG[B](t) HG[Ã](t) = HG[A](t + g) (6.6) whereB = µ f (B),Ã = ν g (A). Now bothB,Ã have property (i) of Theorem 5.4. (See remark after Theorem 6.4.) Since deg α A d ≤ −2, HG[Ã](t) has the same asymtotic form as HG[B](t) in eqn. (6.5) mod O(α −2 ). It follows that e H·t/α HG[Ã −B](t) ≡ O(α −2 ), or equivalently deg α (Ã d −B d ) ≤ −2. ThusÃ,B satisfy condition (ii) of Theorem 5.4.
"
238834501,13/19,1.0,0.684,"In terms of the **data** matrix X with rows corresponding to days and columns to times, the 'average' type regression estimate for d • is given by where β is an estimate for the (column) vector of regression coefficients β and Ŷ is the regression estimate. This expression corresponds to an unweighted average over N (selected) days if β contains N elements with value 1/N and otherwise zeros. Viewed as a regression, an intercept is included by adding a column of ones to X T , and the estimate β d • t • for β is obtained by minimising a loss function. We denote with the subscripts the fact that β is estimated by using the observation at d • but only the times t • (excluding the eclipse) as a target. The 'smoothing' approach consists in using the columns of X corresponding to t • as the regression's targets and all remaining columns X dt • as predictors:"
119419652,7/18,1.0,0.389,"Analyses involving top quarks -the heaviest of all fundamental particles in the SM -allow for rigorous tests of recent NNLO and NNLL pQCD predictions. An unprecedented number of top quarks have been produced at the centres of the main LHC experiments in recent years; previously statistically limited searches for rare SM processes featuring top quarks are now within reach based on the √ s = 13 TeV Run 2 **data** collected so far. For some less rare and previously measured processes, such as the differential tt cross-section, statistical limitations are now being relegated to the extremes of kinematic regions of phase space, where deviations from the SM may eventually be exhibited. One set of rare processes of particular interest involves the associated production of tt pairs with W /Z or H bosons. Moreover, with the top quark as the heaviest of all SM particles, techniques to identify top quarks with highly collimated decay products could prove vital in searches for new physics: any enhancement in the rates of boosted top quarks could provide hints of as-ofyet undiscovered BSM particles. Finally, SM processes featuring top quarks often represent the dominant source of background to many searches for BSM physics; a better understanding of the kinematics of top quark events in general will lead to improved sensitivity in such searches."
15394341,9/18,1.0,0.5,"Figure S2 :
S2Twenty-second catalog earthquake waveforms, ordered by event time in 1 week of continuous **data** from CCOB.EHN (bandpass, 4 to 10 Hz). (A) FAST detected 21 (blue) out of 24 catalog events within the region of interest in Figure 2. (B) False negatives: FAST did not detect 3 (black) out of 24 catalog events in this data. Autocorrelation detected all 24 catalog events.
"
209991952,12/23,1.0,0.522,"butions of the nuisance parameters encoding the systematic uncertainties. The latter are Gaussian distributions for all sources, including statistical uncertainties arising from the limited number of preselected or opposite-sign **data** events in the estimation of the reducible background, or the limited number of simulated events. Correlations of a given nuisance parameter between the backgrounds and the signal are taken into account when relevant. Table 6 presents 95% CL upper limits on the number of BSM events, S 95 , that may contribute to the SRs. Normalising these by the integrated luminosity L of the **data** sample, they can be interpreted as upper limits on the visible BSM cross-section (σ vis ), defined as σ vis = σ prod × A × = S 95 /L, where σ prod is the production cross-section of an arbitrary BSM signal process, and A and are the corresponding fiducial acceptance and reconstruction efficiencies for the relevant SR. These limits are computed with asymptotic approximations of the probability distributions of the test statistic under the different hypotheses [84]. They were confirmed to be within 10% of an alternative computation based on pseudo-experiments. The probability of the observations being compatible with the SM-only hypothesis is quantified by the p-values displayed in table 6; the smallest, for"
53380409,29/119,1.0,0.244,"The event selection discussed in Sec. VI relies on the reconstruction of electrons, muons, jets, and missing transverse momentum (with magnitude E miss T ). Although the requirements vary for the different channels, the general algorithms are introduced below. The small differences between the efficiencies measured in **data** and MC simulation are corrected for by applying scale factors to the MC simulation so that it matches the data."
38256305,13/49,1.0,0.265,"False color composite Landsat 8 images, including thermal bands (thermal bands: Band 7, wavelength: 2.11-2.29 µm; Band 10, wavelength: 10.6-11.19 µm) combined with texture enhancements, provide clear visibility of linear features, such as those seen in Figure 10. The spectral emissivity properties of larger fault zones in the thermal bands are often influenced by soil moisture. Surface water often finds easy passage through fault zones and, therefore, increases soil moisture. This, in turn, influences the emissivity pattern of thermal bands. The reservoir area is characterized by estimated V s 30 **data** ranging between 600 and 760 m/s. The dam site area is obviously not affected by higher susceptibility to soil amplification, as in the case of lower V s 30 velocities (<400 m/s) within broader valleys and depressions."
236522469,7/33,1.0,0.212,"Reference It ranges from 0 to 1, where: 0-no correct ratings 1-perfect classification [54] Where, ACCoverall: relative compliance observed between classifiers; RACCoverall: hypothetical probability compliance using the observed **data** to calculate the probabilities of each classifier to identify each category randomly; TP: true positive; TN: true negative; FP: false positive; FN: false negative; PPV: positive predictive value; TPR: true positive rate; NPV: negative predictive value; TNR: true negative rate.
Mathews Correlation Coefficient MCC = TP × TN − FP × FN √(
The adjusted F-score is an index that groups all elements of the original confusion matrix and gives more weight to correctly classified patterns in correctly classified classes [52]. The Matthews Correlation Coefficient has a range from −1 to 1, and when it has a value of −1, it indicates a completely wrong binary classifier, while 1 indicates a completely correct binary classifier. This index allows you to evaluate the performance of your classification model [53], whereas similarity is a metric ranging from 0 (wrong classification) to 1 (perfect classification) calculated from the averages of the classifications of the classes of interest [54]."
246863515,24/44,2.0,0.545,"Corollary 2. Suppose Assumptions 1, 4 to 7, 10 and 11 hold and Y (a) ⊥ G | S(a), U, X. Then Equation (11) in Theorem 1, Equation (15) in Theorem 2 and Equation (16) in Theorem 3 all identify the average long-term treatment effect over the experimental **data** distribution, i.e.,
τ E = E [Y (1) − Y (0) | G = E] ,
In Corollary 2, we still assume the weaker conditions in Assumptions 10 and 11. But we additionally require that the experimental and observational **data** share a common conditional distribution of the potential long-term outcome. This additional assumption ensures that the bridge functions defined in terms of the observational **data** distribution can also be used to identify the average long-term treatment effect over the experimental **data** distribution."
5784667,23/27,2.0,0.852,"HVS1 **data** do have a limited power of discrimination in a forensic (or anthropological) context and thus many mtD-NA haplogroups are poorly defined by the control region. Since only the HVS1 region was analyzed, sharing the same HVS1 profile does not necessarily imply that two mtDNAs absolutely belong to the same haplogroup (38)."
15591492,15/34,1.0,0.441,"Total RNA was extracted using the RNeasy kit (Qiagen, Valencia, CA). Specific primers and fluorogenic probes were used to amplify and quantitate gene expression [31]. The gene-specific signals were normalized to the RPL19 housekeeping gene. All TaqMan qRT-PCR reagents were purchased from Applied Biosystems (Foster City, CA). A minimum of a triplicate set of **data** was analyzed for each condition. Data are presented as the mean 6 SEM."
58646123,8/26,2.0,0.308,"We subsequently explored whether or not the **data** provides insights into either of the mechanisms driving this effect between soils and health. We ran 48 additional regressions with interactions between each soil nutrient factor and night time lighting, or the three different geographical distance variables. Detailed results are available upon request. Such interactions with either the micronutrient factor, or with the nitrogen and organic matter factor are rarely significant. In few cases that they are, it suggests that the relation between soils and health is smaller in isolated areas, lending credence to an income effect as the driver of the effect on health. Yet, interactions with the calcium and magnesium factor point to the exact opposite. In fact, various interactions turn out significant (across the different health outcomes, for night time lighting as well as the geographical controls). Given these opposite findings a verdict on a dominating mechanism remains inconclusive and requires additional research, for instance, by using experimental approaches stratified across settings with different subsistence levels."
52810234,19/27,1.0,0.704,"The 69% with ""any respiratory symptom"" included 23.3% with no ""lower respiratory symptoms."" A far smaller percentage of all workers (17.3%) complained of what may be considered the most important respiratory symptom, dyspnea, which was not quantified by any standard scale. Such a reliance on symptoms is subject to recall biases both for symptoms present before 9/11 and for the onset, worsening, and persistence of symptoms after 9/11. Because physical examination and chest radiographs were unrevealing , the only objective results were from Environmental Health Perspectives • VOLUME 115 | NUMBER 2 | February 2007 pulmonary function tests. These were confined to spirometry, which does not provide insight into all aspects of respiratory impairment. The **data** presented by  are limited. Mean values for subsets (classified by WTC exposure, previous smoking history, etc.) are not given. Despite the frequency of cough (42.8%), wheeze (15.1%), and chest tightness (15.4%) and the common diagnoses of asthma/reactive airways dysfunction, only 7.6% of all responders showed airway obstruction, defined as a ratio of forced expiratory volume in 1 sec (FEV 1 ) to forced vital capacity (FVC) less than the 5th percentile of the reference population. Unlike virtually all spirometric surveys of a large population (reviewed by ,  found little difference in impairment by smoking status. Most spirometric impairments were classified as restrictive, uncharacteristic of the symptoms and clinical diagnoses. This frequency of low FVC (22.7%) raises several issues: a) the effects of other clinical factors not reported on, such as obesity; b) technical considerations in subject performance or technician monitoring of the FVC maneuver, despite the investigators attention to these; and c) the appropriateness of the reference-predicted values."
253240026,22/31,1.0,0.71,"In our RNA-seq analyses, we found several enriched GO categories related to ABA, water deprivation, and salt stress. Among the ABA-related genes that have a higher transcript level in Ti-treated plants than in the controls, we can highlight NAP, one of the transcription factors involved in activating ABAbiosynthesis genes, components of the ABA sensor system like PYL4, PYL6, and PP2C52 and SNRK2s kinases that is upregulated in roots but not in shoots. However, ABAsynthesis genes do not seem to be affected by Ti treatment ( Figure 6A). Our transcriptomic **data** also showed that Titreated plants have elevated transcript levels of genes encoding ROS homeostasis proteins such as PEROXIDASE 64 (PER64), CATALASE 3 (CAT3), ALTERNATIVE OXIDASE 1A Effect of Titanium (Ti) on expression of nutrient acquisition related genes. Expression level (mean log2F.C.) of genes encoding transporters of (A) macro and (B) microelements, and other related molecules (C). Gray squares indicate that the gene was filtered-out in the filtering step of the edgeR pipeline due to low CPMs. All heatmaps share the same color scale."
250670380,6/21,1.0,0.286,"Accordingly to the classification of deserts by U.S Geological Survey (U.S.G.S), Thar is a monsoon desert region. As monsoons cross India in late June/July, it loses moisture on the eastern slopes of the Aravali range. Hence the rains are scanty, though there are bouts of heavy rains, but the rainy days are few. Average rainfall within study area varies from 127.93 mm (approx.) per year, to 280.14 mm (approx.) per year. Further, analysis of rainfall **data** from in past century indicates a drought once in every 2-3 years along with flood situation in monsoon due to heavy rainfall in short spells. (Refer fig. 3)"
249932676,17/33,1.0,0.515,"Before testing the hypothesized relationships, we conducted confirmatory factor analysis (CFA) using AMOS 24 to assess the quality of our survey measures. The results of the CFA are presented in Table 1. It shows that the hypothesized four-factor measurement model (i.e., organizational innovation climate, employee innovative behavior, psychological ownership, and task interdependence) provided good fit to the **data** (χ 2 /df = 3.29, p < 0.001; CFI = 0.94; TLI = 0.91; RMSEA = 0.08), which yielded better fit than all alternative three-factor, two-factor, and onefactor models. These findings demonstrated the discriminant validity of the measures of our focal constructs."
238581401,1/34,1.0,0.029,"Disulfiram (Antabuse), a drug used for almost 70 years to treat alcohol abuse, is an emerging candidate for repurposing in cancer therapy. Antitumor activity of disulfiram (DSF) is supported by numerous preclinical studies, case reports, and small clinical trials [1][2][3], yet clinical **data** from larger randomized trials are still lacking. Despite several promising case reports about durable remissions of advanced-stage cancer patients after DSF therapy [1,4,5], results from clinical trials are still limited and less favorable [6], a trend that is shared with other repurposed drugs [7]. The results from the few clinical trials available so far suggest that DSF´s anticancer effect may be limited to a subset of cancer patients [8,9], thereby raising a need for the identification of biomarkers that would help guide the patient selection in the future. A broader assessment of DSF in clinical oncology had been hindered mainly by the unknown identity of the active anticancer metabolite and its mechanism of action in cancer cells, including the key molecular target. Consequently, there is currently no reliable way to predict who among cancer patients is likely to benefit from the DSF treatment. In an effort to improve this situation, we have recently discovered that DSF is metabolized in the human body to bis-diethyldithiocarbamatecopper complex (CuET), that CuET represents the long-sought-after active compound that kills cancer cells, and that mechanistically, such toxicity to cancer cells reflects CuET-mediated impairment of NPL4, an essential cofactor of p97 segregase broadly involved in the degradation of cellular proteins [10,11]. We have also noticed that the CuET complex levels assessed after administration of the same dose of DSF vary significantly among patients [10]. We hypothesize that the observed variable clinical responses to DSF treatment might be attributable, at least in part, to the divergent extent of CuET formation, a process that is likely influenced by genetic and environmental factors, the latter including copper intake and the overall diet. Furthermore, the effectiveness of DSF treatment may be affected also by factors such as concomitant exposure to other drugs or pharmaceutically active compounds, a scenario particularly likely for advancedstage cancer patients. With the primary mechanism of anticancer activity of DSF known, the identification of such factors that impact cellular responses to DSF/ CuET is key to facilitate the repurposing of DSF in clinical oncology."
217125084,44/57,1.0,0.772,"Mitogen-activated protein kinase kinase (MAPKK) gene information from the model plant A. thaliana and rice were downloaded from The Arabidopsis Information Resources database (TAIR: http://www.arabidopsis.org/) [59] and the TIGR rice Genome Annotation Resources database (http://rice.plantbiology.msu.edu/) [60], respectively. To identify MAPKK genes of unknown species, BLASTP searches was conducted using orthologous protein sequences A. thaliana and O. sativa MAPKKs as the query search [61] in the publicly available PLAZA database (http://bioinformatics.psb.ugent.be/plaza/) [62] and phytozome database (http://www.phytozome.net/) [63]. The coding and genomic sequences of MAPKK genes were collected in 51 plant species (Table 1). The gene was only considered as probable MAPKK gene if it harbored the serine/threonine−/dual specificity protein kinase domain, including the active site motif D(L/I/V)K and the phosphorylation site S/T-X 5 -S/T within the activation loop, which was subsequently confirmed by scanning in InterPro software for the presence of MKK's conserved domain [64]. All **data** were checked for redundancy and no any alternative splice variants were considered."
250239249,3/35,1.0,0.086,"All **data** were taken from the Protein Data Bank [29,30]. The analysis was limited to protein X-ray crystal structures. Multi-model refinements and structures containing only Cα atoms were discharged. Only structures refined at a resolution better than 2.5 Å were retained, and this resulted in a list of about 121,000 entries of the Protein Data Bank. These were randomly divided into 14 subsets, each containing 7000 entries. Each entry was contained in one subset only (no overlap). All the analyses were then performed on each of the 14 subsets."
11007627,17/31,1.0,0.548,"In preliminary shaking flasks experiments the recombinant expression of PepP from Lactobacillus plantarum NC8 as well as Lactococcus lactis ssp. lactis DSM 20481 was studied. The cultivation temperatures varied between 4°C and 37°C. At higher temperatures (30°C and 37°C), no active recombinant protein was detected in the soluble fractions after cell disruption, but a strong overexpression band was observed following SDS-PAGE analysis of the insoluble protein fraction (inclusion bodies; **data** not shown). Cultivation at lower temperatures (4°C and 20°C) resulted in a recombinant protein overexpression band in both the soluble and insoluble protein fractions after cell disruption (data not shown). Similar results were obtained using E. coli C41 (DE3) and E. coli C43 (DE3) as hosts for expression. The expression systems containing partial soluble PepP were tested for enzymatic function using a novel, specific PepP activity assay based on the release of L-leucine from the substrate LPP (Figure 1)."
18862159,7/20,2.0,0.35,"As the natural behaviour **data** were not normally distributed (Anderson-Darling test), each pair of datasets, before and after the chemical cue was added, were compared using the Wilcoxon matched pairs test (T-test). Differences in behaviour state frequencies, before and after the cue was added, were compared using Pearson's χ 2 test. Statistical analyses were carried out using Statistica software (Version 7, Statsoft Inc., 2004). In addition, power-law regressions were fitted to the log-log plots of behaviour state duration vs frequency and their regression coefficients were calculated using Microsoft Excel 2003 SP3."
15394341,18/18,1.0,1.0,"Table S1 :
S1Autocorrelation input parameters. These were used for detection in synthetic **data** (except the event detection CC threshold), and in one week of CCOB.EHN data.Autocorrelation Parameter 
Value 
Time series window length 
200 samples (10 s) 
Time series window lag 
2 samples (0.1 s) 
Similarity search: near-repeat exclusion parameter 
50 samples (5 s) 
Scale factor β for MAD, for initial threshold 
5 
Event detection CC threshold 
0.818 
Near-duplicate pair and event elimination time window 
21 s 
FAST and catalog comparison time window 
19 s "
140114079,7/20,2.0,0.35,"To show the generalizability of our findings beyond Nigeria, the Online Appendix presents models of fraud perception and protesting for all African countries using **data** on respondents in Afrobarometer rounds 1, 3, and 4 (Table A7). We show that the positive effect of fraud perception on individual mobilization holds for all countries surveyed in Afrobarometer."
246337567,3/70,2.0,0.043,LD interlinking describes the task of determining whether a named resource (an entity identified by a URI) can be linked to another named resource in order to indicate that they both describe the same thing or that they are related in some capacity (Ferrara et al. 2011). The purpose of LD interlinks is to provide additional information about an entity in order to improve **data** discovery (Kim and Hausenblas 2015).
2924940,13/30,1.0,0.433,"Our direct comparisons of relative transcript and protein abundance levels uncover a dynamic and complex picture of stage-specific gene and protein expression in P. falciparum. Particularly revealing are the insights into differentially expressed isoforms of some proteins that offer a glimpse of an almost bewildering complexity that may lie beneath corresponding RNA profiles and a deceptively simple appearance of overall protein abundance. Many of these isoform changes are fundamentally undetectable in transcript-level analyses and are invisible even to common protein-level investigations such as Western blotting and conventional high-throughput proteomics based on mass spectrometry. Our **data** reveal significant and distinct isoform changes for several proteins (for example, eIF4A, eIF5A, and HSP70-2) as the malaria parasites progress through the late stage of their intra-erythrocytic life cycle. The high reproducibility and temporal specificity of our observations strongly suggest that these changes are more than inconsequential fluctuations and that they represent biologically significant modifications. It is likely that many of these isoforms lead to different biological functionality of the cognate protein."
25460449,3/31,1.0,0.097,"As a second method, we evaluated the leakage of muscle- specific creatine kinase (CK) into the blood serum (McArdle et al., 1994). Consistent with results from the Evans Blue tracer assay, serum levels of CK in phenotypic SSPN-Tg mice was similar to non-Tg littermate controls (Fig. 3B). The levels of serum CK from mdx mice is shown for comparison (Fig. 3B). Taken together, these **data** demonstrate that sarcolemma integrity is not altered upon SSPN overexpression, which is distinct from dystrophin-and sarcoglycan-deficient muscular dystrophies. This finding makes SSPN-Tg mice similar to laminin-defective mice, which also do not display significant membrane damage ."
229179745,61/73,2.0,0.836,"Results from the present work are novel also because they allow for causal inferences. Indeed, different from archival research using large survey **data** [56,57], in our studies, the primary predictor variables, namely participants' socio-economic status and mind-set, were manipulated rather than assessed. Importantly, although participants were objectively in the same income range, subjective socio-economic status was successfully varied by providing them a fictitious feedback about their relative standing in the social and economic ladder. This manipulation allows us to conclude that potentially confounding variables, which typically covary with individuals' actual socio-economic status (e.g., political orientation, tax attitudes, system justification) can not account for our findings."
220474872,29/49,1.0,0.592,"It was thought that Gal1 secreted from large tumours induces T cell apoptosis and switches to Th2 cytokine production. However, recent **data** suggests that Gal1 conditions the tumour endothelial cells by inducing expressions of PD-L1 and Gal9 and suppresses anti-tumour immunity by causing T cell exclusion, which in fact takes place relatively earlier during tumour growth [207]. Thus, any lack of successes in anti-PD-1 treatment, whereby patients eventually develop progressive disease, may be due to Gal1-induced immune-evasion and addressed by introducing anti-Gal1 antibody in combination with anti-PD-1 treatment. Indeed, Gal1 blockade enhanced the effect of anti-PD-1 therapy in preclinical models of head and neck cancers [207]. Therefore, this strategy of blocking Gal1 in combination with anti-cancer immunotherapies should be investigated further and introduced in clinical studies."
53478339,1/24,3.0,0.042,"We present a multi-proxy skeletal δ 18 O, Sr/Ca, and spectral luminescence record of seasonal to interannual variability in riverine influence obtained from nearshore GBR coral microatolls. Annual mean SST and mean summer and winter values were calculated from a mid-Holocene (4665 cal. yr BP) record spanning 31 years, and a modern record spanning 17 years. The combined **data** provide high-resolution information on past climate variability within the north-eastern Australian region and suggest that an altered mid-Holocene hydrological cycle influenced coastal oceanographic conditions of the GBR."
157086359,22/27,1.0,0.815,"The **data** for this analysis were taken from a larger survey of aquaponic growers, including those not involved in education, such as hobbyist and commercial farmers. As a result, our recruitment was focused on aquaponics organizations rather than schools, potentially leading to us missing some educators. Because of this recruitment method, we captured educators who were already engaged in an aquaponics network, perhaps leading to respondents having more confidence in their knowledge than educators who are not engaged with similar networks. Despite this limitation, we have captured a broad range of respondents representing a variety of academic institutions and geographic locations."
220474872,27/49,3.0,0.551,"The potential of combining checkpoint inhibitors with OV therapy is currently being tested in a phase II clinical study (NCT02798406) where the adenovirus DNX-2401 with pembrolizumab are used in the treatment of glioblastoma (GBM) and gliosarcoma. Interim results of this phase II revealed that there was no dose-limiting toxicity or unexpected safety issues with 47% of the patients experienced clinical benefit and two patients with >94% tumour regression [188]. Following the evaluation of **data** obtained from the phase I trial (NCT00805376), avoidance of tumour immune suppression through checkpoint inhibitors has been strongly implicated as a way to help augment clinical benefit [179]. T-cell exhaustion was partially overcome by DNX-2401 in NCT00805376 as its administration induced a decrease in transmembrane immunoglobulin mucin-3 (TIM-3) (discussed in Section 3.2.1.1) [179]."
254129277,22/27,1.0,0.815,"BM-O and JO participated in the design of the study, patient evaluation, interpretation, and revision of data. DA-G and HC-M organized the data, also contributed to its interpretation as well as the writing of the first draft of the manuscript. GP-M contributed to **data** processing, statistical analysis, and **data** interpretation and wrote sections of the manuscript. All authors contributed to the article and approved the submitted version."
238581401,20/34,1.0,0.588,"Interestingly, the potential of MTs as detoxifying proteins has been known for decades and this function has also been linked to possible resistance to some chemotherapeutics [31][32][33]. The detoxifying ability of MTs has been reported even for some nonmetal-based drugs. Chemotherapeutics that are sensed and bound by MTs are neutralized before reaching their intended target(s) and thereby become clinically ineffective. Thus, MT expression represents potential predictive biomarkers of resistance to specific treatments [34,35]. In light of these facts and our **data** presented here, CBD usage might be a relevant factor to be kept in mind for cancer patients not only undergoing the trials with DSF-repurposing therapy but also treated with some standard-of-care chemotherapy drugs."
221825477,11/17,1.0,0.647,"Expanding funding mechanisms is essential to support diverse team science efforts [29]. Under represented groups have been reported to receive less support for research funding [13]. Also, the research funds that are provided are more likely to be limited to research topics that pertain to health disparities, disease prevention and intervention, socioeconomic factors, healthcare, lifestyle, psychosocial, adolescence, and risk management [26]. Specifically, funding was less likely to be awarded if these terms were used in comparison with studies that focused on topics linked to neuron, corneal, cell, and iron [26]. Several funding sources were discussed at the conference. Crowdfunding mechanisms are emerging funding opportunities that bring flexibility to support innovative scientific efforts while promoting a platform that bridges investigators and donors; the American Brain Foundation (ABF) Crowdfunding Grant Mechanism (https ://www. ameri canbr ainfo undat ion.org/proje cts/) is the first neuroscience crowdfunding platform and has received nearly 150 donations and funded 8 projects (as of 2019). The National Institute of Health (NIH) Strategies to Innovate Emergency Care Research Network (SIREN) [30] represents a new network established in 2017 with 11 regional centers and 50 satellite research sites, aimed to develop acute neurological emergency research across the disciplines of Neurology, Neurosurgery, Emergency Medicine and Critical Care, and leverage this infrastructure to support multicenter randomized clinical trials (RCT). This multidisciplinary approach to clinical trials in the study of status epilepticus [31,32] and stroke [33][34][35]  Further, efforts in harmonizing **data** collection in future trials of acute brain injuries, including curated multiple parametric and imaging **data** repositories, are urgently needed to facilitate subsequent **data** analyses and cohort discovery across populations."
25460449,6/31,4.0,0.194,"Relatively low levels of SSPN protein expression (10-fold over endogenous levels) resulted in severe toxicity. This is Fig. 10. Schematic diagram showing a pathogenetic mechanism for SSPN-mediated disruption of the DGC. The **data** in the current report support a model of SSPNinduced dysfunction of the DGC. We propose that SSPN (green) overexpression at the sarcolemma (1) causes clustering of the SGs (yellow) into insoluble aggregates (2). Perturbation of the SG-SSPN subcomplex within the DGC impairs its ability to properly anchor ␣-DG (red) at the sarcolemma (3). The destabilization of ␣-DG attachment to the sarcolemma leads to perturbation of basement membrane (blue) assembly (4). It is feasible that disruption of the basement membrane leads to aberrant cellular signaling (5), which may be responsible for the increased levels of apoptosis in phenotypic SSPN-Tg mice. By this pathogenetic mechanism, SSPN disrupts protein interactions within and across the membrane bilayer leading to a severe phenotype that is reminiscent of congenital MD."
2238019,9/18,2.0,0.5,"FACS is a type of flow cytometry, a method for sorting a suspension of biological cells based on specific light scattering and fluorescent characteristics of each cell. We prepared GFP-fused mouse MOR constructs ( Figure 3A) and confirmed that uORFs mediated mouse MOR expression. As shown in Figure 3B, removal of all three uAUGs in cells transfected with pmMUEG #3 resulted in increased fluorescence levels up to 3.5-fold compared to pmMUEG #1 (wild-type). In cells transfected with pmMUEG #2 (only mutated at uAUG#3), the fluorescence level increased up to 1.7-fold compared to pmMUEG #1. These **data** suggest that uORFs of the mouse MOR can be repressed downstream of MOR expression with the most effective repression activity containing uAUG#3. Additionally, these three uAUGs could synergistically repress the downstream of MOR gene expression."
1441955,18/51,2.0,0.353,"In our study, xanthohumol in a concentration of 25 and after 2 and 8 h incubation of LNCaP cancer cells alone did not affect the expression of proapoptotic proteins Bid and Bax. TRAIL alone and TRAIL used with xanthohumol induced activation of Bid and led to a significant increase of the expression of Bax after 2 and 8 h incubation (the results are shown in Figure 6D,E), respectively. In this paper we have also demonstrated that TRAIL in combination with xanthohumol decreased the expression of anti-apoptotic protein Bcl-xL after 8 h incubation ( Figure 6F). The results are expressed as means˘SD obtained from three independent experiments. The β-actin was used as a control to show equal loading of proteins. The **data** were normalized to the control and β-actin level. A statistical significance of the differences between the treatment and control results is marked with * p < 0.05, ** p < 0.01, *** p < 0.001."
237649489,2/49,1.0,0.041,"The two main types of **data** used in this study are spatial **data** and statistical yearbook data. Spatial Data. (1) Administrative boundary vector **data** of Three Gorges Reservoir area (SHP format). (2) Soil dataset provided by Harmonized World Soil Database (HWSD) and Cold Arid Regions, Available online: http://www.westdc.westgis.ac.cn (accessed on 17 April 2019), which contains the spatial coordinates and properties of the soil (GRID format)."
249258070,57/119,3.0,0.479,"The current results should be considered relative to a few study limitations. The CFS **data** did not specify the nature of proactive activities that patrol, DRT officers, or investigators were engaged in. Furthermore, although the coding of the ten call categories analyzed were informed by prior research (Wu & Lum, 2017), idiosyncrasies associated with the study departments' method of cataloging and recording call information did not always allow for direct comparisons to prior research on COVID-19's impact on police services. Similarly, measuring proactivity solely through self-initiated activities from CFS **data** is not a flawless indicator. Officers may engage in proactive work that is not captured in these **data** (Lum, Koper, et al., 2020). However, this method has been established as a reasonable way to distinguish proactivity from reactivity (Lum, Koper, et al., 2020;Wu & Lum, 2017;Zhang and Zhao, 2021)."
54551628,15/17,1.0,0.882,"Krakauer and colleagues bolstered their experimental fi ndings by comparing them with a statistical model of movements of the arm and wrist. The model uses a Bayesian approach, where previous experience and new **data** are combined to form a new parameter estimate. A key aspect of this approach is that greater uncertainty about the parameter leads to faster learning. Crucially, in their model, the investigators assumed that the majority of movements with the arm also include moving the wrist, but not vice versa. This led to a situation where the uncertainty in the parameter estimate-the imposed rotationdepended not only on the current limb context but also on the history of training in previous limb contexts. The model was able to reproduce most of the effects they saw in the experiments, such as the fi nding that learning would transfer from arm to wrist, but not vice versa, and blocking of generalization."
249258070,3/119,1.0,0.025,"Law enforcement executives have noted the impact of the pandemic on police activities. Based on survey **data** collected from 989 US and Canadian executive officers who are members of the IACP, Lum et al. (2020a) noted several reported changes in police operations during the initial months of the pandemic. By March 23, 2020, 43% of responding agencies stopped or significantly changed their responses to CFS, 57% reported a decline in CFS, 61% implemented policies to reduce proactive stops, and 73% limited community policing activities. As of May 10, 2020, a second survey wave indicated that 53% of the responding agencies continued to have policies that limited proactivity, and 64% were still limiting community-oriented policing activities-both slight decreases from wave 1 (Lum, Maupin, & Stoltz, 2020b)."
134689280,6/20,3.0,0.3,"First, what appears to be a truncation is not necessarily truncated. The features could be intentionally constructed, i.e. actually preserved and originally finished that way. This is arguably the most important distinction: do we see the representation of the finished article thanks to decent preservation or is the shape of this feature a representation of something broken? If the latter, there are still many options. Was the feature destroyed and, if so, when, by whom, and how? Did it deteriorate over time and, if so, by gradual dilapidation of the original feature after disuse or due to other site formation processes? Was it damaged by modification, reuse, or reappropriation in the ancient or recent past? Did it suffer from decay of perishable building materials or decay due to the perishing of originally incorporated natural elements (such as trees and plants)? Was (part of) the feature removed by either animals or humans after disuse or abandonment? Without a symbology for line ends, when conditions of archaeological recording allow for it, the end user will once again rely on rules of thumb to carry out the conjectures. Fortunately this can be done in critical and archaeologically knowledgeable and sensible ways (see Vis, under review, 2014b). Once the metadata of the project as well as spatial **data** and analogous information from historic and cultural proximity have been exhausted, one can still apply visual and morphological contrast when constructing complementary data, document the applied rules of thumb, and mark up **data** for easy separation of these conjectures from retentions of originally acquired spatial data."
27068907,11/25,1.0,0.44,"The terminal software mainly consists of the main program, the RF transceiver subprogram, the touch screen communication subprogram and others. Under terminal control, the master and slave control mode is selected and the **data** is transmitted via the MODBUS protocol. The touch screen triggers the SCM for a certain period of time, sends the acquisition command to the acquisition unit, and stores the received **data** in the set address for **data** processing. In the meantime, it analyzes the **data** returned from the nodes. In the case of failure, the touch screen serves as a protection and sends the alarm information to the monitoring center. See Figure 4 for the specific workflow of the terminal.  "
245864793,17/42,1.0,0.405,"The control architecture of the CardioVR-ReTone was developed to allow the deployment of several exoskeleton control modes: assistive, partially assistive, and resistive. An industrial open controller manufactured by Siemens, CPU 1515SP PC2, was considered the main control unit. The PLC had a CANOpen communication module, used to connect with every actuator EPOS4 controller and several IOs modules used to connect the signals from the exoskeleton joysticks and mechanical safety limit switches mounted on the exoskeleton's structure. The mechanical safety limit switches were used to activate the safe torque off functionality of the actuators in cases of malfunction, considering the positioning process. The open controller operating system hosted the VR engine and Simulink. Simulink ran the kinematic algorithms and sent the positioning, speed, and torque **data** to every joint actuator controller. Feedback was received from every actuator controller and sent back to the Simulink Kinematics algorithm. Surface EMGs placed on human arms sent information about muscle activity using Bluetooth to the open controller, which fed the **data** to Simulink for processing and torque adjustment. The architecture of the CardioVR-ReTone operating platform was based on a distributed control approach; the ROS (Robot Operating System) platform is currently considered the first option for implementing control-related functions."
55648852,20/40,7.0,0.5,"The Italian OBS **data** available at the time of this report are only those recorded by the broadband OBSs. These **data** are saved in SAC files and therefore easy to handle. The procedure was the same as for the rest of data, obtaining .mat files as the final format.  "
146010542,11/17,1.0,0.647,"""Man in the middle"" (MITM/MIM) is a kind of malware which relies on SSL/TSL protocol weakness, being correspondent in communication between two network users (Čekerevac et al., 2017;Mallik et al., 2019). In such a case, downloading of important **data** occurs while users can rarely detect it. 7."
233705419,1/53,2.0,0.019,"Processes 2021, 9, x FOR PEER REVIEW 2 of 13 In this paper, the focus is on the formalization of expert knowledge and its use for test design and integration into different machine learning (ML) algorithms. The formalization of expert knowledge from different domains (e.g., expertise regarding plant mechanical processes and knowledge about thermo-chemical and other physical mechanisms of chemical processes) is particularly challenging and reveals the uniqueness of the presented approach. Due to the lack of data, deep neuronal networks cannot always be trained, and other ML algorithms must be combined to quantify and expand expert knowledge from different domains. Figure 1 shows the continuous learning procedure and the interaction between the different parts of an AI-based hybrid model, i.e., **data** measurement, expert knowledge, and ML algorithms with special metrics for automated evaluation (autoML) [5]. With this procedure, expert knowledge can be continuously quantified, and the understanding of the plant and chemical processes can be expanded. Along with a cloud-based IT solution from Engineering Data Intelligence (EDI Ltd., Pfinztal-Berghausen, Germany) called the ""EDI hive Internet of things (IoT) Framework"" (short EDI hive), the process of capturing expert knowledge and the integration of process engineering fundamentals, AI-based modeling, and the subsequent application of the AIbased hybrid model in power plant operation can be achieved. In this paper, the advantages of AI-based hybrid models in industrial applications are discussed, the experimental results are presented together with the modeling results, and the continuous software-based support of this process through EDI hive is demonstrated."
232380196,11/28,1.0,0.393,"A backbone network, e.g., ResNet [10], is shared between these two learning branches to learn the image representation r 2 R DE for each image x. A projection head f e (·) maps the image representation r into a vector representation z 2 R DS which is more suitable for contrastive loss. We implement this projection head f e (·) as a nonlinear multiple-layer perceptron (MLP) with one hidden layer. Such projection module is proven important in improving the representation quality of the layer before it [5].`2 normalization is applied to z in order that inner product can be used as distance measurements. To avoid abuse of notations, unless otherwise stated we use z as the normalized representation of x for contrastive loss computation. A supervised contrastive loss L SCL is applied on top of the normalized representations for feature learning. The classifier learning branch is simpler which applies a single linear layer f c (·) to the image representation r to predict the class-wise log-its s 2 R DC , which are used to compute the cross-entropy loss L CE . Due to different natures of the two loss functions, the feature learning and classifier learning branches have different **data** sampling strategies. The feature learning branch takes as input anchor point x i together with positive samples {x + i } = {x j |y i = y j , i 6 = j} from the same class and negative samples {x i } = {x j |y j 6 = y i } from other classes. The input batch of the feature learning branch is denoted as
B SC = {x i , {x + i }, {x i }}.
The classifier learning branch directly takes image and label pairs as input B CE = {{x i , y i }}. The final loss function for the hybrid network is:
L hybrid = ↵ · L SCL (B SC ) + (1 ↵) · L CE (B CE ), (1)
where ↵ is a weighting coefficient inversely proportional to the epoch number, as shown in Fig. 2. "
55648852,20/40,2.0,0.5,"We analyzed separately both earthquake and shots data. For the shot database we started with the Cube stations. These stations save the **data** in binary daily files that should be then converted to an appropriate format. The GFZ Potsdam institute provides useful algorithms to convert Cube format to MiniSeed format. During the recorded period, many of the 80 stations deployed lost the GPS signal sometimes, leading to more than one daily file. We found out that the registered **data** had some gaps, meaning that for certain periods of time the station did not register. The most probable cause of this is the loss of GPS signal. Fortunately, MiniSeed files are written with an absolute scale of time, thus, allowing us to identified and exclude these gaps. Another step we performed was to convert MiniSeed into SAC in order to choose the more suitable type file to work with. On the other hand SAC files have the time information not in absolute values but in a single time vector from 1 to the end of samples. Therefore, gaps could not be well identified using this type of files. For this reason we decided to work directly with the MiniSeed data. To reduce computation time we transformed the **data** into .mat format taking only the pieces of signal we needed. Indeed, we cut the signal 30 s before that **data** and 60 s after each shot. When we find a gap in the registration, the shots occurred in that period were not taken into account for that station."
27068907,14/25,1.0,0.56,"Appropriate test environment should be chosen to ensure the accuracy of the results. Normally, the test is carried out in an open space outdoors. The tester should select a certain number of nodes, which have the same straight-line distance from the terminal, and carries out the test at the straight-line distance of 20m, 40m, 60m, 80m, 100m, 120m, 150m, and 200m. The nodes at each distance should be set into a group. Each group of nodes should be tested continuously for 3h and measured three times. The measured results should be averaged and the loss of **data** packets should be summed up. Table 4 displays the final results. "
12790199,9/21,4.0,0.429,"Translation processes convert interaction history **data** to another form, such as a conversion from visual 2-D **data** to 3-D coordinates for the arm to target, or from continuous color information to a discrete color category."
236447646,6/11,1.0,0.545,"We also used the ASMD Python API to select the proper datasets for our experiments. To avoid over-fitting during the evaluation stage, we did not use the Maestro [24] and MusicNet [22] datasets because the AMT models were trained on them. Instead, we used the ""Saarland Music Dataset"" [25] for evaluating piano A2SA. It consists of 50 piano audio recordings along with the associated MIDI performances, recorded with high-quality piano equipped with MIDI transducers. As regards to multi-instrument music, we relied on another well known dataset: the ""Bach10"" [23] dataset, which includes 10 different Bach chorales synthesized with virtual chamber instruments. Even though Bach10 dataset provides non-aligned scores, we used our artificially misaligned **data** to obtain results comparable with the other datasets."
52141668,5/16,1.0,0.312,"As the attempt of automatically detecting deception in people, we try to explore deception cues within the choices of words when lying to others. In this experiment, we use Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2007) and Whissell's Dictionary of Affect in Language (DAL) (Whissell, 2009) in order to determine the psychological scores for each sentence. Using LIWC, we extract 72 features which comprise of word class scores and also scores for non-word elements of the sentence such as punctuations and parenthesis. From IDC, we use 9:1 of all **data** as learning **data** and the rest of them as testing data. For the learning experiment, we use 8:1 of all learning **data** as training **data** and developing **data** as can be seen in Figure 1. We use three classifiers, Random Forest, linear Support Vector Machine (SVM), and Neural Networks."
207778550,3/48,1.0,0.062,"When the degree of correlation exceeds a critical value, depending on the data, it may cause the well-known effect that the resulting mean lies outside the **data** interval and its uncertainty decreases, see for example [4]. In the field of nuclear **data** this phenomenon is also known as Peelle's Pertinent Puzzle (PPP) [5]. This effect is mainly related to an incorrectly constructed covariance matrix from measurement observables, which may suggest that the used statistical model is not adequate to describe the **data** [6][7][8][9]. As a practical matter, in nuclear **data** evaluation physically unreasonable evaluated results may be generated by the least-squares method in certain extreme cases (strong correlations and discrepant input data) if no compensation for PPP is applied."
